diff --ruN a/stablehlo/docs/spec.md b/stablehlo/docs/spec.md
--- stablehlo/docs/spec.md
+++ stablehlo/docs/spec.md
@@ -4733,10 +4733,12 @@
 Receives data from a channel with `channel_id` and produces `results`.
 
 If `is_host_transfer` is `true`, then the operation transfers data from the
-host. Otherwise, it transfers data from another device. What this means is
-implementation-defined. This flag duplicates the information provided in
+host. Otherwise, it transfers data from another device based on the values of
+`source_target_pairs`. This flag duplicates the information provided in
 `channel_type`, so in the future we are planning to only keep one of them
-([#666](https://github.com/openxla/stablehlo/issues/666)).
+([#666](https://github.com/openxla/stablehlo/issues/666)). If `is_host_transfer`
+= `false` and `source_target_pairs` is `None` or empty, it is considered
+undefined behavior.
 
 `results` consist of payload values which come first and a token which comes
 last. In the future, we are planning to split the payload and the token into two
@@ -4745,12 +4747,13 @@
 
 #### Inputs
 
-| Label | Name               | Type                                            | Constraints |
-|-------|--------------------|-------------------------------------------------|-------------|
-| (I1)  | `token`            | `token`                                         | (C4)        |
-| (I2)  | `channel_id`       | constant of type `si64`                         |             |
-| (I3)  | `channel_type`     | enum of `DEVICE_TO_DEVICE` and `HOST_TO_DEVICE` | (C1)        |
-| (I4)  | `is_host_transfer` | constant of type `i1`                           | (C1)        |
+| Label | Name                  | Type                                            | Constraints   |
+|-------|-----------------------|-------------------------------------------------|---------------|
+| (I1)  | `token`               | `token`                                         |               |
+| (I2)  | `channel_id`          | constant of type `si64`                         |               |
+| (I3)  | `channel_type`        | enum of `DEVICE_TO_DEVICE` and `DEVICE_TO_HOST` | (C5)          |
+| (I4)  | `is_host_transfer`    | constant of type `i1`                           | (C5-C6)       |
+| (I5)  | `source_target_pairs` | 2-dimensional tensor constant of type `si64`    | (C1-C4), (C6)       |
 
 #### Outputs
 
@@ -4760,19 +4763,23 @@
 
 #### Constraints
 
-* (C1) `channel_type` is defined as:
-  * `HOST_TO_DEVICE` if `is_host_transfer = true`,
+* (C1) `dim(source_target_pairs, 1) = 2`.
+* (C2) `is_unique(source_target_pairs[:, 0])`.
+* (C3) `is_unique(source_target_pairs[:, 1])`.
+* (C4) `0 <= source_target_pairs < N`, where `N` is defined as:
+  * `num_replicas` if `cross_replica` is used.
+  * `num_partitions` if `cross_partition` is used.
+* (C5) `channel_type` is defined as:
+  * `DEVICE_TO_HOST` if `is_host_transfer = true`,
   * `DEVICE_TO_DEVICE` otherwise.
-* (C2) `0 < size(results)`.
-* (C3) `is_empty(result[:-1])` or `is_tensor(type(results[:-1]))`.
-* (C4) `is_token(type(results[-1]))`.
 
 #### Examples
 
 ```mlir
 %results0, %results1 = "stablehlo.recv"(%token) {
-  channel_handle = #stablehlo.channel_handle<handle = 1, type = 3>,
-  is_host_transfer = true
+  channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+  is_host_transfer = false,
+  source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
 } : (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token)
 ```
 
@@ -5855,23 +5862,28 @@
 
 #### Semantics
 
-Sends `inputs` to a channel `channel_id` and produces a `result` token.
+Sends `inputs` to a channel `channel_id`. Inputs are then sent to other devices
+in the order specified by `source_target_pairs`. The operation produces a
+`result` token.
 
 If `is_host_transfer` is `true`, then the operation transfers data to the
-host. Otherwise, it transfers data to another device. What this means is
-implementation-defined. This flag duplicates the information provided in
+host. Otherwise, it transfers data to another device based on the values of
+`source_target_pairs`. This flag duplicates the information provided in
 `channel_type`, so in the future we are planning to only keep one of them
-([#666](https://github.com/openxla/stablehlo/issues/666)).
-
-#### Inputs
-
-| Label | Name               | Type                                            | Constraints |
-|-------|--------------------|-------------------------------------------------|-------------|
-| (I1)  | `inputs`           | variadic number of tensors or quantized tensors |             |
-| (I2)  | `token`            | `token`                                         |             |
-| (I3)  | `channel_id`       | constant of type `si64`                         |             |
-| (I4)  | `channel_type`     | enum of `DEVICE_TO_DEVICE` and `DEVICE_TO_HOST` | (C1)        |
-| (I5)  | `is_host_transfer` | constant of type `i1`                           | (C1)        |
+([#666](https://github.com/openxla/stablehlo/issues/666)). If `is_host_transfer`
+= `false` and `source_target_pairs` is `None` or empty, it is considered
+undefined behavior.
+
+#### Inputs
+
+| Label | Name                  | Type                                            | Constraints   |
+|-------|-----------------------|-------------------------------------------------|---------------|
+| (I1)  | `inputs`              | variadic number of tensors or quantized tensors |               |
+| (I2)  | `token`               | `token`                                         |               |
+| (I3)  | `channel_id`          | constant of type `si64`                         |               |
+| (I4)  | `channel_type`        | enum of `DEVICE_TO_DEVICE` and `DEVICE_TO_HOST` | (C5)          |
+| (I5)  | `is_host_transfer`    | constant of type `i1`                           | (C5-C6)       |
+| (I6)  | `source_target_pairs` | 2-dimensional tensor constant of type `si64`    | (C1-C4), (C6) |
 
 #### Outputs
 
@@ -5881,7 +5893,13 @@
 
 #### Constraints
 
-* (C1) `channel_type` is defined as:
+* (C1) `dim(source_target_pairs, 1) = 2`.
+* (C2) `is_unique(source_target_pairs[:, 0])`.
+* (C3) `is_unique(source_target_pairs[:, 1])`.
+* (C4) `0 <= source_target_pairs < N`, where `N` is defined as:
+  * `num_replicas` if `cross_replica` is used.
+  * `num_partitions` if `cross_partition` is used.
+* (C5) `channel_type` is defined as:
   * `DEVICE_TO_HOST` if `is_host_transfer = true`,
   * `DEVICE_TO_DEVICE` otherwise.
 
@@ -5889,8 +5907,9 @@
 
 ```mlir
 %result = "stablehlo.send"(%operand, %token) {
-  channel_handle = #stablehlo.channel_handle<handle = 1, type = 2>,
-  is_host_transfer = true
+  channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+  is_host_transfer = false,
+  source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
 } : (tensor<2x2xi64>, !stablehlo.token) -> !stablehlo.token
 ```
 
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -1229,8 +1229,9 @@
     Example:
     ```mlir
     %result = "stablehlo.send"(%operand, %token) {
-      channel_handle = #stablehlo.channel_handle<handle = 1, type = 2>,
-      is_host_transfer = true
+      channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+      is_host_transfer = false,
+      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
     } : (tensor<2x2xi64>, !stablehlo.token) -> !stablehlo.token
     ```
   }];
@@ -1239,10 +1240,16 @@
     Variadic<HLO_TensorOrPerAxisQuantizedTensor>:$inputs, /*send_i1*/
     HLO_Token:$token, /*send_i2*/
     StableHLO_ChannelHandle:$channel_handle, /*send_i3_i4*/
-    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_host_transfer /*send_i5*/
+    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_host_transfer, /*send_i5*/
+    OptionalAttr<I64ElementsAttr>:$source_target_pairs /*send_i6*/
   );
 
   let results = (outs HLO_Token);
+  let builders = [
+    OpBuilder<(ins
+      "::mlir::Type":$result_type, "::mlir::Value":$operand,
+      "::mlir::DenseIntElementsAttr":$source_target_pairs,
+      "::mlir::stablehlo::ChannelHandleAttr":$channel_handle)>];
 }
 
 def StableHLO_RecvOp : StableHLO_Op<"recv", [
@@ -1259,8 +1266,9 @@
     Example:
     ```mlir
     %results:2 = "stablehlo.recv"(%token) {
-      channel_handle = #stablehlo.channel_handle<handle = 1, type = 3>,
-      is_host_transfer = true
+      channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+      is_host_transfer = false,
+      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
     } : (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token)
     ```
   }];
@@ -1268,8 +1276,14 @@
   let arguments = (ins
     HLO_Token:$token, /*recv_i1*/
     StableHLO_ChannelHandle:$channel_handle, /*recv_i2_i3*/
-    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_host_transfer /*recv_i4*/
-  );
+    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_host_transfer, /*recv_i4*/
+    OptionalAttr<I64ElementsAttr>:$source_target_pairs /*recv_i5*/
+  );
+  let builders = [
+    OpBuilder<(ins
+      "::mlir::Type":$result_type, "::mlir::Value":$operand,
+      "::mlir::DenseIntElementsAttr":$source_target_pairs,
+      "::mlir::stablehlo::ChannelHandleAttr":$channel_handle)>];
 
   let results = (outs Variadic<HLO_StaticShapeTensorOrPerAxisQuantizedTensorOrToken>);
   let hasVerifier = 1;
diff --ruN a/stablehlo/stablehlo/dialect/Version.h b/stablehlo/stablehlo/dialect/Version.h
--- stablehlo/stablehlo/dialect/Version.h
+++ stablehlo/stablehlo/dialect/Version.h
@@ -38,7 +38,7 @@
   static FailureOr<Version> fromString(llvm::StringRef versionRef);
 
   /// Return a Version representing the current VHLO dialect version.
-  static Version getCurrentVersion() { return Version(1, 10, 10); }
+  static Version getCurrentVersion() { return Version(1, 12, 0); }
 
   /// Return a Version representing the minimum supported VHLO dialect version.
   static Version getMinimumVersion() { return Version(0, 9, 0); }
diff --ruN a/stablehlo/stablehlo/dialect/VhloAttrs.td b/stablehlo/stablehlo/dialect/VhloAttrs.td
--- stablehlo/stablehlo/dialect/VhloAttrs.td
+++ stablehlo/stablehlo/dialect/VhloAttrs.td
@@ -45,14 +45,6 @@
 def VHLO_ArrayAttrV1 : VHLO_AttrDef<"ArrayV1", "0.9.0", "current"> {
   let mnemonic = "array_v1";
   let parameters = (ins ArrayRefParameter<"mlir::Attribute">:$value);
-  let genVerifyDecl = 1;
-  let extraClassDefinition = [{
-    LogicalResult ArrayV1Attr::verify(
-        llvm::function_ref<mlir::InFlightDiagnostic ()> errFn, ArrayRef<mlir::Attribute> value) {
-      if (!allFromVhlo(value)) return errFn() << "expected array of VHLO attriutes";
-      return success();
-    }
-  }];
   let assemblyFormat = "`<` custom<AttributeArray>($value) `>`";
 }
 
@@ -75,9 +67,9 @@
     LogicalResult DictionaryV1Attr::verify(
         llvm::function_ref<mlir::InFlightDiagnostic ()> errFn,
         ArrayRef<std::pair<mlir::Attribute, mlir::Attribute>> value) {
-      for (auto & entry : value)
-        if (!isFromVhlo(entry.first) || !isFromVhlo(entry.second))
-          errFn() << "expected VHLO attribute";
+      for (auto & entry : value) {
+        if (!isFromVhlo(entry.first)) errFn() << "expected VHLO key attribute";
+      }
       return success();
     }
   }];
diff --ruN a/stablehlo/stablehlo/dialect/VhloDialect.td b/stablehlo/stablehlo/dialect/VhloDialect.td
--- stablehlo/stablehlo/dialect/VhloDialect.td
+++ stablehlo/stablehlo/dialect/VhloDialect.td
@@ -49,6 +49,8 @@
       1.8.0: Introduce `f4E2M1FN`, `f6E2M3FN`, `f6E3M2FN` and `f8E8M0FNU` types.
       1.9.0: Add `ResultAccuracy` attribute to `exp` op.
       1.10.0: Add `ResultAccuracy` attribute to `cbrt`, `cosine`, `exponential`, `exponential_minus_one`, `log`, `log_plus_one`, `logistic`, `rsqrt`, `sine`, `sqrt`, `tan` and `tanh` ops.
+      1.11.0: Allow (de)serializing VHLO programs mixed with potentially unstable dialects.
+      1.12.0: Add `source_target_pairs` attribute to `send` and `recv` ops.
   }];
 
   let useDefaultAttributePrinterParser = 0;
diff --ruN a/stablehlo/stablehlo/dialect/VhloOps.td b/stablehlo/stablehlo/dialect/VhloOps.td
--- stablehlo/stablehlo/dialect/VhloOps.td
+++ stablehlo/stablehlo/dialect/VhloOps.td
@@ -896,13 +896,26 @@
   let results = (outs VHLO_AnyType:$result);
 }
 
-def VHLO_RecvOpV1 : VHLO_Op<"recv_v1", "0.9.0", "current"> {
+def VHLO_RecvOpV1 : VHLO_Op<"recv_v1", "0.9.0", "1.11.0"> {
   let arguments = (ins
     VHLO_AnyType:$token,
     VHLO_AnyAttr:$channel_id,
     VHLO_AnyAttr:$channel_type,
     VHLO_AnyAttr:$is_host_transfer
   );
+  let regions = (region VHLO_AnyRegion:$computation);
+  let results = (outs Variadic<VHLO_AnyType>:$results);
+}
+
+def VHLO_RecvOpV2 : VHLO_Op<"recv_v2", "1.12.0", "current"> {
+  let arguments = (ins
+    VHLO_AnyType:$token,
+    VHLO_AnyAttr:$channel_id,
+    VHLO_AnyAttr:$channel_type,
+    VHLO_AnyAttr:$is_host_transfer,
+    VHLO_AnyAttr:$source_target_pairs
+  );
+  let regions = (region VHLO_AnyRegion:$computation);
   let results = (outs Variadic<VHLO_AnyType>:$results);
 }
 
@@ -1085,7 +1098,7 @@
   let results = (outs VHLO_AnyType:$result);
 }
 
-def VHLO_SendOpV1 : VHLO_Op<"send_v1", "0.9.0", "current"> {
+def VHLO_SendOpV1 : VHLO_Op<"send_v1", "0.9.0", "1.11.0"> {
   let arguments = (ins
     Variadic<VHLO_AnyType>:$inputs,
     VHLO_AnyType:$token,
@@ -1093,6 +1106,20 @@
     VHLO_AnyAttr:$channel_type,
     VHLO_AnyAttr:$is_host_transfer
   );
+  let regions = (region VHLO_AnyRegion:$computation);
+  let results = (outs VHLO_AnyType:$result);
+}
+
+def VHLO_SendOpV2 : VHLO_Op<"send_v2", "1.12.0", "current"> {
+  let arguments = (ins
+    Variadic<VHLO_AnyType>:$inputs,
+    VHLO_AnyType:$token,
+    VHLO_AnyAttr:$channel_id,
+    VHLO_AnyAttr:$channel_type,
+    VHLO_AnyAttr:$is_host_transfer,
+    VHLO_AnyAttr:$source_target_pairs
+  );
+  let regions = (region VHLO_AnyRegion:$computation);
   let results = (outs VHLO_AnyType:$result);
 }
 
diff --ruN a/stablehlo/stablehlo/dialect/VhloTypes.h b/stablehlo/stablehlo/dialect/VhloTypes.h
--- stablehlo/stablehlo/dialect/VhloTypes.h
+++ stablehlo/stablehlo/dialect/VhloTypes.h
@@ -27,20 +27,23 @@
 namespace mlir {
 namespace vhlo {
 
-class VhloTypeConverterBase : public TypeConverter {
- public:
-  VhloTypeConverterBase() : TypeConverter(){};
-
-  virtual ~VhloTypeConverterBase() = default;
-
-  virtual Attribute convertEncoding(Attribute attr) const = 0;
-};
 
 // This class is used to manage conversions between VHLO and Builtin
 // dialects.
-class VhloTypeConverter : public VhloTypeConverterBase {
+class VhloTypeConverter : public TypeConverter {
  public:
-  VhloTypeConverter() : VhloTypeConverterBase() {}
+  VhloTypeConverter() : TypeConverter(), allowOtherDialects(false) {}
+  VhloTypeConverter(bool allowOtherDialects)
+      : TypeConverter(), allowOtherDialects(allowOtherDialects) {}
+
+  virtual ~VhloTypeConverter() = default;
+
+  virtual Attribute convertEncoding(Attribute attr) const = 0;
+
+  Attribute convertUnknownAttribute(Attribute attr) const {
+    if (allowOtherDialects) return attr;
+    return {};
+  }
 
   // A subclass can call this method to add conversions from VHLO -> Builtin
   // types. Note that conversions are applied in reverse order, with the most
@@ -58,6 +61,9 @@
 
   // Mark unrealized casts as legal. Useful for dialect mixing.
   void addUnrealizedMaterializations();
+
+ private:
+  bool allowOtherDialects;
 };
 
 // Autogenerated VHLO type printers and parsers.
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
@@ -1,4 +1,4 @@
-// RUN: stablehlo-opt --stablehlo-aggressive-folder --split-input-file --verify-diagnostics %s | FileCheck %s
+// RUN: stablehlo-opt --stablehlo-aggressive-folder=fold-op-element-limit=100 --split-input-file --verify-diagnostics %s | FileCheck %s
 
 ////////
 // AddOp
@@ -42,6 +42,21 @@
 // -----
 
 ////////
+// ClampOp
+
+// CHECK-LABEL: func.func @clamp_fold
+func.func @clamp_fold(%arg0: tensor<3xi32>) -> tensor<3xi32> {
+  %min = stablehlo.constant dense<[1, 5, 10]> : tensor<3xi32>
+  %max = stablehlo.constant dense<[10, 25, 12]> : tensor<3xi32>
+  %operand = stablehlo.constant dense<[0, 30, 11]> : tensor<3xi32>
+  // CHECK: stablehlo.constant dense<[1, 25, 11]> : tensor<3xi32>
+  %0 = "stablehlo.clamp"(%min, %operand, %max) : (tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> tensor<3xi32>
+  func.return %0: tensor<3xi32>
+}
+
+// -----
+
+////////
 // CompareOp
 
 // CHECK-LABEL: func.func @compare_folds
@@ -98,6 +113,26 @@
   // CHECK-DAG:  [[R3:%.+]] = stablehlo.constant dense<{{\[\[0, 1, 2, 11, 12\], \[3, 4, 5, 13, 14\]\]}}> : tensor<2x5xi32>
   // CHECK-NEXT: return [[R0]], [[R1]], [[R2]], [[R3]]
   return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xi32>, tensor<3x3xi32>, tensor<2x5xi32>
+}
+
+// -----
+
+////////
+// DivOp
+
+// CHECK-LABEL: @div_fold_cst
+func.func @div_fold_cst() -> (tensor<i32>, tensor<ui32>, tensor<f32>) {
+  %cst = stablehlo.constant dense<2> : tensor<i32>
+  %cst_1 = stablehlo.constant dense<2> : tensor<ui32>
+  %cst_2 = stablehlo.constant dense<2.0> : tensor<f32>
+  // CHECK: stablehlo.constant dense<1> : tensor<i32>
+  // CHECK: stablehlo.constant dense<1> : tensor<ui32>
+  // CHECK: stablehlo.divide{{.*}} : tensor<f32>
+  // DISABLED-CHECK: stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
+  %0 = stablehlo.divide %cst, %cst : tensor<i32>
+  %1 = stablehlo.divide %cst_1, %cst_1 : tensor<ui32>
+  %2 = stablehlo.divide %cst_2, %cst_2 : tensor<f32>
+  return %0, %1, %2 : tensor<i32>, tensor<ui32>, tensor<f32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -1,4 +1,4 @@
-// RUN: stablehlo-opt --stablehlo-aggressive-simplification --allow-unregistered-dialect --split-input-file %s | FileCheck %s
+// RUN: stablehlo-opt --stablehlo-aggressive-simplification=fold-op-element-limit=100 --allow-unregistered-dialect --split-input-file %s | FileCheck %s
 
 /////////
 // AddOp
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
@@ -521,16 +521,16 @@
 // -----
 
 // CHECK-LABEL: func @eval_slice_zerorank
-func.func @eval_slice_zerorank() -> tensor<f32> {
-  // CHECK: [[RESULT:%.*]] = stablehlo.constant dense<3.300000e+01> : tensor<f32>
-  // CHECK: return [[RESULT]]
-  %0 = stablehlo.constant dense<33.0> : tensor<f32>
+func.func @eval_slice_zerorank() -> tensor<i32> {
+  // CHECK: [[RESULT:%.*]] = stablehlo.constant dense<33> : tensor<i32>
+  // CHECK: return [[RESULT]]
+  %0 = stablehlo.constant dense<33> : tensor<i32>
   %1 = "stablehlo.slice"(%0) {
     start_indices = array<i64>,
     limit_indices = array<i64>,
     strides = array<i64>
-  } : (tensor<f32>) -> tensor<f32>
-  func.return %1 : tensor<f32>
+  } : (tensor<i32>) -> tensor<i32>
+  func.return %1 : tensor<i32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_12_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_12_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_12_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.1_12_0.mlir
@@ -0,0 +1,2972 @@
+// RUN: stablehlo-opt --mlir-print-op-generic %s.bc | FileCheck %s
+// RUN: stablehlo-translate --deserialize %s.bc | stablehlo-translate --serialize --target=1.12.0 | stablehlo-opt --mlir-print-op-generic | FileCheck %s
+// RUN: stablehlo-translate --deserialize %s.bc | stablehlo-opt > %t.0
+// RUN: stablehlo-opt --strip-debuginfo %s > %t.1
+// RUN: diff %t.0 %t.1
+// RUN: stablehlo-translate --serialize --target=1.12.0 --strip-debuginfo %s > %t.2
+// RUN: diff %s.bc %t.2
+// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo -emit-bytecode -debug-only=vhlo-bytecode %s 2>&1 | FileCheck --check-prefix=CHECK-WARN %s
+// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo -emit-bytecode %s | stablehlo-opt -debug-only=vhlo-bytecode 2>&1 | FileCheck --check-prefix=CHECK-WARN %s
+
+// CHECK-WARN-NOT: Not Implemented
+
+// ============ ATTRIBUTES ============
+
+// CHECK-LABEL: "attr_comparison_direction_eq"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_eq(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 EQ>
+    comparison_direction = #stablehlo<comparison_direction EQ>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_ne"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_ne(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 NE>
+    comparison_direction = #stablehlo<comparison_direction NE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_ge"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_ge(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 GE>
+    comparison_direction = #stablehlo<comparison_direction GE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_gt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_gt(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 GT>
+    comparison_direction = #stablehlo<comparison_direction GT>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_le"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_le(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 LE>
+    comparison_direction = #stablehlo<comparison_direction LE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_direction_lt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_direction_lt(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    // CHECK: comparison_direction = #vhlo<comparison_direction_v1 LT>
+    comparison_direction = #stablehlo<comparison_direction LT>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_notype"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_notype(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>
+    // CHECK: compare_type = #vhlo<comparison_type_v1 NOTYPE>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_float"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_float(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 FLOAT>,
+    compare_type = #stablehlo<comparison_type FLOAT>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_totalorder"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_totalorder(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 TOTALORDER>,
+    compare_type = #stablehlo<comparison_type TOTALORDER>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_signed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_signed(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 SIGNED>,
+    compare_type = #stablehlo<comparison_type SIGNED>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "attr_comparison_type_unsigned"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_comparison_type_unsigned(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    // CHECK: compare_type = #vhlo<comparison_type_v1 UNSIGNED>,
+    compare_type = #stablehlo<comparison_type UNSIGNED>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// ConvDimensionNumbers aka #stablehlo.conv is covered below.
+
+// CHECK-LABEL: "attr_custom_call_api_version_unspecified"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_unspecified(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>
+    api_version = 0 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_original"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_original(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>
+    api_version = 1 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_status_returning"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_status_returning(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING>
+    api_version = 2 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_status_returning_unified"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_custom_call_api_version_status_returning_unified(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    // CHECK: api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING_UNIFIED>
+    api_version = 3 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_dict"
+// CHECK: #vhlo.dict_v1<{#vhlo.string_v1<"attr1"> = #vhlo.integer_v1<1 : i32>, #vhlo.string_v1<"attr2"> = #vhlo.integer_v1<2 : i32>}
+func.func @attr_dict() attributes {stablehlo.attr = {attr1 = 1 : i32, attr2 = 2 : i32}} {
+  return
+}
+
+// CHECK-LABEL: "attr_custom_call_api_version_typed_ffi"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+// CHECK: api_version = #vhlo<api_version_v1 API_VERSION_TYPED_FFI>
+// CHECK-SAME: backend_config = #vhlo.dict_v1<{#vhlo.string_v1<"bar"> = #vhlo.integer_v1<42 : i32>}>
+func.func @attr_custom_call_api_version_typed_ffi(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    backend_config= {bar = 42 : i32},
+    api_version = 4 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+
+// CHECK-LABEL: "attr_custom_call_api_version_typed_ffi_no_backend_config"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+// CHECK: api_version = #vhlo<api_version_v1 API_VERSION_TYPED_FFI>
+// CHECK-SAME: backend_config = #vhlo.dict_v1<{}>
+func.func @attr_custom_call_api_version_typed_ffi_no_backend_config(%arg0: tensor<f32>) -> tensor<f32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    api_version = 4 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// DotDimensionNumbers aka #stablehlo.dot is covered below.
+
+// CHECK-LABEL: "attr_fft_type_fft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_fft(%arg0: tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 FFT>
+    fft_type = #stablehlo<fft_type FFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>>
+  func.return %0 : tensor<16xcomplex<f32>>
+}
+
+// CHECK-LABEL: "attr_fft_type_ifft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_ifft(%arg0: tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 IFFT>
+    fft_type = #stablehlo<fft_type IFFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>>
+  func.return %0 : tensor<16xcomplex<f32>>
+}
+
+// CHECK-LABEL: "attr_fft_type_rfft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_rfft(%arg0: tensor<16xf32>) -> tensor<9xcomplex<f32>> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 RFFT>
+    fft_type = #stablehlo<fft_type RFFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xf32>) -> tensor<9xcomplex<f32>>
+  func.return %0 : tensor<9xcomplex<f32>>
+}
+
+// CHECK-LABEL: "attr_fft_type_irfft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_fft_type_irfft(%arg0: tensor<9xcomplex<f32>>) -> tensor<16xf32> {
+  %0 = "stablehlo.fft"(%arg0) {
+    // CHECK: fft_type = #vhlo<fft_type_v1 IRFFT>
+    fft_type = #stablehlo<fft_type IRFFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<9xcomplex<f32>>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "exponential_HIGHEST"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}
+func.func @exponential_HIGHEST(%arg0: tensor<8x16xf32>) -> tensor<8x16xf32> {
+  %0 = "stablehlo.exponential"(%arg0) {
+    // CHECK: result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 HIGHEST>>
+    result_accuracy = #stablehlo.result_accuracy<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #stablehlo.result_accuracy_mode<HIGHEST>>
+  } : (tensor<8x16xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: "exponential_TOLERANCE"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}
+func.func @exponential_TOLERANCE(%arg0: tensor<8x16xf32>) -> tensor<8x16xf32> {
+  %0 = "stablehlo.exponential"(%arg0) {
+    // CHECK: result_accuracy = #vhlo.result_accuracy_v1<atol = 1.000000e-05, rtol = 0.000000e+00, ulps = 1, mode = #vhlo<result_accuracy_mode_v1 TOLERANCE>>
+    result_accuracy = #stablehlo.result_accuracy<atol = 1.000000e-5, rtol = 0.000000e+00, ulps = 1, mode = #stablehlo.result_accuracy_mode<TOLERANCE>>
+  } : (tensor<8x16xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: "exponential_DEFAULT"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}
+func.func @exponential_DEFAULT(%arg0: tensor<8x16xf32>) -> tensor<8x16xf32> {
+  %0 = "stablehlo.exponential"(%arg0) {
+    // CHECK: result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>
+    result_accuracy = #stablehlo.result_accuracy<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #stablehlo.result_accuracy_mode<DEFAULT>>
+  } : (tensor<8x16xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// GatherDimensionNumbers aka #stablehlo.gather is covered below.
+
+// CHECK-LABEL: "attr_precision_config_default"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_precision_config_default(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    // CHECK: precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "attr_precision_config_high"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_precision_config_high(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    // CHECK: precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGH>, #vhlo<precision_v1 HIGH>]>
+    precision_config = [#stablehlo<precision HIGH>, #stablehlo<precision HIGH>]
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "attr_precision_config_highest"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_precision_config_highest(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    // CHECK: precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "attr_rng_algorithm_default"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_rng_algorithm_default(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    // CHECK: rng_algorithm = #vhlo<rng_algorithm_v1 DEFAULT>
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_algorithm_three_fry"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_rng_algorithm_three_fry(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    // CHECK: rng_algorithm = #vhlo<rng_algorithm_v1 THREE_FRY>
+    rng_algorithm = #stablehlo<rng_algorithm THREE_FRY>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_algorithm_philox"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_rng_algorithm_philox(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    // CHECK: rng_algorithm = #vhlo<rng_algorithm_v1 PHILOX>
+    rng_algorithm = #stablehlo<rng_algorithm PHILOX>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_distribution_uniform"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @attr_rng_distribution_uniform(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<0xindex>) -> tensor<f32> {
+  %0 = "stablehlo.rng"(%arg0, %arg1, %arg2) {
+    // CHECK: rng_distribution = #vhlo<rng_distribution_v1 UNIFORM>
+    rng_distribution = #stablehlo<rng_distribution UNIFORM>
+  } : (tensor<f32>, tensor<f32>, tensor<0xindex>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "attr_rng_distribution_normal"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @attr_rng_distribution_normal(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<0xindex>) -> tensor<f32> {
+  %0 = "stablehlo.rng"(%arg0, %arg1, %arg2) {
+    // CHECK: rng_distribution = #vhlo<rng_distribution_v1 NORMAL>
+    rng_distribution = #stablehlo<rng_distribution NORMAL>
+  } : (tensor<f32>, tensor<f32>, tensor<0xindex>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// ScatterDimensionNumbers aka #stablehlo.scatter is covered below.
+
+// CHECK-LABEL: "attr_transpose_no_transpose"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_transpose_no_transpose(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    // transpose_a = #vhlo<transpose_v1 NO_TRANSPOSE>,
+    transpose_a = #stablehlo<transpose NO_TRANSPOSE>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "attr_transpose_transpose"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_transpose_transpose(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    // transpose_a = #vhlo<transpose_v1 TRANSPOSE>,
+    transpose_a = #stablehlo<transpose TRANSPOSE>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "attr_transpose_adjoint"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @attr_transpose_adjoint(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    // transpose_a = #vhlo<transpose_v1 ADJOINT>,
+    transpose_a = #stablehlo<transpose ADJOINT>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// TypeExtensionsAttr aka #stablehlo.type_extensions is covered below.
+
+// CHECK-LABEL: "attr_type_extensions_bounds"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @attr_type_extensions_bounds(%arg0: tensor<?x?xf32, #stablehlo.type_extensions<bounds = [16, ?]>>) -> tensor<?x?xf32, #stablehlo.type_extensions<bounds = [16, ?]>> {
+  // CHECK: "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<?x?x!vhlo.f32_v1, #vhlo.type_extensions_v1<bounds = [16, ?]>>) -> ()
+  func.return %arg0 : tensor<?x?xf32, #stablehlo.type_extensions<bounds = [16, ?]>>
+}
+
+
+// ============ DEFAULTS ============
+
+// CHECK-LABEL: "default_all_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_gather(%arg0: tensor<16x8xf32>) -> tensor<16x16xf32> {
+  //               CHECK: "vhlo.all_gather_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   all_gather_dim = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<false>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.all_gather"(%arg0) {
+    all_gather_dim = 1 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<16x8xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "default_all_gather_variadic"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_gather_variadic(%arg0: tensor<16x8xf32>, %arg1: tensor<16x8xf32>) -> (tensor<16x16xf32>, tensor<16x16xf32>) {
+  %0:2 = "stablehlo.all_gather"(%arg0, %arg1) {
+    all_gather_dim = 1 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<16x8xf32>, tensor<16x8xf32>) -> (tensor<16x16xf32>, tensor<16x16xf32>)
+  func.return %0#0, %0#1 : tensor<16x16xf32>, tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "default_all_reduce"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_reduce(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.all_reduce_v2"(%[[ARG0]])
+  //          CHECK-SAME: <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<false>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+
+  %0 = "stablehlo.all_reduce"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_all_to_all"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_all_to_all(%arg0: tensor<4x16xf32>) -> tensor<16x4xf32> {
+  //               CHECK: "vhlo.all_to_all_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  //          CHECK-SAME:   concat_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>>,
+  //          CHECK-SAME:   split_count = #vhlo.integer_v1<4 : i64>
+  //          CHECK-SAME:   split_dimension = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<4x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x4x!vhlo.f32_v1>
+  %0 = "stablehlo.all_to_all"(%arg0) {
+    split_dimension = 1 : i64,
+    concat_dimension = 0 : i64,
+    split_count = 4 : i64,
+    replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>
+  } : (tensor<4x16xf32>) -> tensor<16x4xf32>
+  func.return %0 : tensor<16x4xf32>
+}
+
+// CHECK-LABEL: "default_all_to_all_variadic"
+func.func @default_all_to_all_variadic(%arg0: tensor<4x16xf32>, %arg1: tensor<5x16xf32>) -> (tensor<16x4xf32>, tensor<20x4xf32>) {
+  %0:2 = "stablehlo.all_to_all"(%arg0, %arg1) {
+    split_dimension = 1 : i64,
+    concat_dimension = 0 : i64,
+    split_count = 4 : i64,
+    replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>
+  } : (tensor<4x16xf32>, tensor<5x16xf32>) -> (tensor<16x4xf32>, tensor<20x4xf32>)
+  func.return %0#0, %0#1 : tensor<16x4xf32>, tensor<20x4xf32>
+}
+
+// CHECK-LABEL: "default_cholesky"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_cholesky(%arg0: tensor<1x16x16xf32>) -> tensor<1x16x16xf32> {
+  //      CHECK: "vhlo.cholesky_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   lower = #vhlo.bool_v1<false>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.cholesky"(%arg0) : (tensor<1x16x16xf32>) -> tensor<1x16x16xf32>
+  func.return %0 : tensor<1x16x16xf32>
+}
+
+// CHECK-LABEL: "default_collective_permute"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_collective_permute(%arg0: tensor<16x8xf32>) -> tensor<16x8xf32> {
+  //               CHECK: "vhlo.collective_permute_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x8x!vhlo.f32_v1>
+  %0 = "stablehlo.collective_permute"(%arg0) {
+    source_target_pairs = dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>
+  } : (tensor<16x8xf32>) -> tensor<16x8xf32>
+  func.return %0 : tensor<16x8xf32>
+}
+
+// CHECK-LABEL: "default_collective_broadcast"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_collective_broadcast(%arg0: tensor<16x8xf32>) -> tensor<16x8xf32> {
+  //               CHECK: "vhlo.collective_broadcast_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0, 1]]> : tensor<1x2xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x8x!vhlo.f32_v1>
+  %0 = "stablehlo.collective_broadcast"(%arg0) {
+    replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>
+  } : (tensor<16x8xf32>) -> tensor<16x8xf32>
+  func.return %0 : tensor<16x8xf32>
+}
+
+// CHECK-LABEL: "default_compare"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_compare(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  //      CHECK: "vhlo.compare_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   compare_type = #vhlo<comparison_type_v1 NOTYPE>,
+  // CHECK-SAME:   comparison_direction = #vhlo<comparison_direction_v1 EQ>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "default_composite"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_composite(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.composite_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{}>
+  //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
+  //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
+  //          CHECK-SAME:   version = #vhlo.integer_v1<0 : i64>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.composite"(%arg0) {
+    name = "stablehlo.composite_target",
+    decomposition = @composite_target
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_convolution"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_convolution(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>) -> tensor<1x6x6x16xf32> {
+  //      CHECK: "vhlo.convolution_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<0> : tensor<2x2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x6x6x16x!vhlo.f32_v1>
+  %0 = "stablehlo.convolution"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>) -> tensor<1x6x6x16xf32>
+  func.return %0 : tensor<1x6x6x16xf32>
+}
+
+// CHECK-LABEL: "default_custom_call"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_custom_call(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.custom_call_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>,
+  // CHECK-SAME:   backend_config = #vhlo.string_v1<"">,
+  // CHECK-SAME:   call_target_name = #vhlo.string_v1<"foo">,
+  // CHECK-SAME:   called_computations = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   has_side_effect = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   operand_layouts = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   output_operand_aliases = #vhlo.array_v1<[]>
+  // CHECK-SAME:   result_layouts = #vhlo.array_v1<[]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo"
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_dot_general"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_dot_general(%arg0: tensor<8x8x16xf32>, %arg1: tensor<8x16x8xf32>) -> tensor<8x8x8xf32> {
+  //      CHECK: "vhlo.dot_general_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   accumulation_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+  // CHECK-SAME:   rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<8x16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot_general"(%arg0, %arg1) {
+    dot_dimension_numbers = #stablehlo.dot<
+      lhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_batching_dimensions = [0],
+      rhs_contracting_dimensions = [1]
+    >
+  } : (tensor<8x8x16xf32>, tensor<8x16x8xf32>) -> tensor<8x8x8xf32>
+  func.return %0 : tensor<8x8x8xf32>
+}
+
+// CHECK-LABEL: "dot_general_algorithm"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @dot_general_algorithm(%arg0: tensor<8x8x16xf32>, %arg1: tensor<8x16x8xf32>) -> tensor<8x8x8xf32> {
+//      CHECK: "vhlo.dot_general_v2"(%[[ARG0]], %[[ARG1]]) <{
+// CHECK-SAME:   accumulation_type = #vhlo.type_v1<!vhlo.f32_v1>,
+// CHECK-SAME:   allow_imprecise_accumulation = #vhlo.bool_v1<false>,
+// CHECK-SAME:   lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+// CHECK-SAME:   lhs_component_count = #vhlo.integer_v1<1 : i64>,
+// CHECK-SAME:   lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+// CHECK-SAME:   lhs_precision_type = #vhlo.type_v1<!vhlo.tf31_v1>,
+// CHECK-SAME:   num_primitive_operations = #vhlo.integer_v1<1 : i64>,
+// CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+// CHECK-SAME:   rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+// CHECK-SAME:   rhs_component_count = #vhlo.integer_v1<1 : i64>,
+// CHECK-SAME:   rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>,
+// CHECK-SAME:   rhs_precision_type = #vhlo.type_v1<!vhlo.tf31_v1>
+// CHECK-SAME: }> : (!vhlo.tensor_v1<8x8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<8x16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot_general"(%arg0, %arg1) {
+    dot_dimension_numbers = #stablehlo.dot<
+      lhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_batching_dimensions = [0],
+      rhs_contracting_dimensions = [1]
+    >,
+    algorithm = #stablehlo.dot_algorithm<
+      lhs_precision_type = tf32,
+      rhs_precision_type = tf32,
+      accumulation_type = f32,
+      lhs_component_count = 1,
+      rhs_component_count = 1,
+      num_primitive_operations = 1,
+      allow_imprecise_accumulation = false
+    >
+  } : (tensor<8x8x16xf32>, tensor<8x16x8xf32>) -> tensor<8x8x8xf32>
+  func.return %0 : tensor<8x8x8xf32>
+}
+
+// CHECK-LABEL: "default_dynamic_broadcast_in_dim"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_dynamic_broadcast_in_dim(%arg0: tensor<?x?xf32>, %arg1: tensor<2xindex>) -> tensor<?x?xf32> {
+  //      CHECK: "vhlo.dynamic_broadcast_in_dim_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   known_expanding_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   known_nonexpanding_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x?x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_broadcast_in_dim"(%arg0, %arg1) {
+    broadcast_dimensions = array<i64: 0, 1>
+  } : (tensor<?x?xf32>, tensor<2xindex>) -> tensor<?x?xf32>
+  func.return %0 : tensor<?x?xf32>
+}
+
+// CHECK-LABEL: "default_dynamic_conv"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_dynamic_conv(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>, %arg2: tensor<2x2xi64>) -> tensor<1x?x?x16xf32> {
+  //      CHECK: "vhlo.dynamic_conv_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<2x2x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>, tensor<2x2xi64>) -> tensor<1x?x?x16xf32>
+  func.return %0 : tensor<1x?x?x16xf32>
+}
+
+// CHECK-LABEL: "default_dynamic_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_dynamic_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>, %arg2 : tensor<3xi32>) -> tensor<1x5x8xf32> {
+  //      CHECK: "vhlo.dynamic_gather_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>, !vhlo.tensor_v1<3x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x8xf32>
+  func.return %0 : tensor<1x5x8xf32>
+}
+
+func.func @default_func(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK:      "vhlo.func_v1"() <{
+  // CHECK-SAME:   arg_attrs = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>>>,
+  // CHECK-SAME:   res_attrs = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   sym_name = #vhlo.string_v1<"default_func">,
+  // CHECK-SAME:   sym_visibility = #vhlo.string_v1<"">
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG0:.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : () -> ()
+  func.return %arg0 : tensor<f32>
+}
+
+// CHECK-LABEL: "default_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x1xf32> {
+  //      CHECK: "vhlo.gather_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x1x!vhlo.f32_v1>
+  %0 = "stablehlo.gather"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >,
+    slice_sizes = array<i64: 1, 1, 1>
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
+  func.return %0 : tensor<1x5x1xf32>
+}
+
+// CHECK-LABEL: "default_infeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_infeed(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //               CHECK: "vhlo.infeed_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   infeed_config = #vhlo.string_v1<"">,
+  // CHECK-SAME{LITERAL}:   layout = #vhlo.array_v1<[]>
+  //          CHECK-SAME: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.infeed"(%arg0) : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "default_outfeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_outfeed(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.outfeed_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   outfeed_config = #vhlo.string_v1<"">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.outfeed"(%arg0, %arg1) : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_recv_with_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_recv_with_source_target_pairs(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //      CHECK: "vhlo.recv_v2"(%[[ARG0]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.recv"(%arg0) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+    source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "default_send"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_send(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.send_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.send"(%arg0, %arg1) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+    source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "default_reduce_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_reduce_scatter(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //               CHECK: "vhlo.reduce_scatter_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   scatter_dimension = #vhlo.integer_v1<0 : i64>
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<false>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_scatter"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension = 0 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "default_reduce_window"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @default_reduce_window(%arg0: tensor<2x17x31x7xf32>, %arg1: tensor<f32>) -> tensor<2x16x30x7xf32> {
+  //               CHECK: "vhlo.reduce_window_v1"(%[[ARG0]], %[[ARG1]])  <{
+  //          CHECK-SAME:   base_dilations = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>,
+  // CHECK-SAME{LITERAL}:   padding = #vhlo.tensor_v1<dense<0> : tensor<4x2xi64>>,
+  //          CHECK-SAME:   window_dilations = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.maximum_v1"(%[[ARG2]], %[[ARG3]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<2x17x31x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x16x30x7x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_window"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>
+  } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
+  func.return %0 : tensor<2x16x30x7xf32>
+}
+
+// CHECK-LABEL: "default_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_scatter(%arg0: tensor<200x100x300xf32>, %arg1: tensor<10x2xi32>, %arg2: tensor<10x300xf32>) -> tensor<200x100x300xf32> {
+  //      CHECK: "vhlo.scatter_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   unique_indices = #vhlo.bool_v1<false>,
+  // CHECK-SAME:   update_window_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>
+  %0 = "stablehlo.scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [0, 1],
+      scatter_dims_to_operand_dims = [0, 1],
+      index_vector_dim = 1
+    >
+  } : (tensor<200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) -> tensor<200x100x300xf32>
+  func.return %0 : tensor<200x100x300xf32>
+}
+
+// CHECK-LABEL: "default_select_and_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @default_select_and_scatter(%arg0: tensor<10x24x24x64xf32>, %arg1: tensor<10x23x23x64xf32>, %arg2: tensor<f32>) -> tensor<10x24x24x64xf32> {
+  //      CHECK: "vhlo.select_and_scatter_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<0> : tensor<4x2xi64>>,
+  // CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG31:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG41:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL11:.*]] = "vhlo.compare_v1"(%[[ARG31]], %[[ARG41]]) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GE>}>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL11]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }, {
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG32:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG42:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL12:.*]] = "vhlo.add_v1"(%[[ARG32]], %[[ARG42]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL12]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>, !vhlo.tensor_v1<10x23x23x64x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>
+  %0 = "stablehlo.select_and_scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg3, %arg4) {compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GE>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }, {
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>
+  } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
+  func.return %0 : tensor<10x24x24x64xf32>
+}
+
+// CHECK-LABEL: "default_sort"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @default_sort(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.sort_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<-1 : i64>
+  // CHECK-SAME:   is_stable = #vhlo.bool_v1<false>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.compare_v1"(%[[ARG1]], %[[ARG2]]) <{compare_type = #vhlo<comparison_type_v1 FLOAT>, comparison_direction = #vhlo<comparison_direction_v1 GT>}>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.sort"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg1, %arg2) {compare_type = #stablehlo<comparison_type FLOAT>, comparison_direction = #stablehlo<comparison_direction GT>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }) : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// ============ OPS ============
+
+// CHECK-LABEL: "op_abs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_abs(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.abs_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.abs"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_add"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_add(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_after_all"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_after_all(%arg0: !stablehlo.token) -> !stablehlo.token {
+  // CHECK: "vhlo.after_all_v1"(%[[ARG0]]) : (!vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.after_all"(%arg0) : (!stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_all_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_all_gather(%arg0: tensor<16x8xf32>) -> tensor<16x16xf32> {
+  //               CHECK: "vhlo.all_gather_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   all_gather_dim = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<true>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.all_gather"(%arg0) {
+    all_gather_dim = 1 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<16x8xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_all_reduce"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_all_reduce(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.all_reduce_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<true>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.all_reduce"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_all_reduce_with_promotable_types"
+func.func @op_all_reduce_with_promotable_types(%operand: tensor<f32>) -> tensor<f64> {
+  //  CHECK: "vhlo.all_reduce_v2"(%[[ARG0:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f64_v1>
+  %result = "stablehlo.all_reduce"(%operand) ({
+    ^bb0(%arg0: tensor<f64>, %arg1: tensor<f64>):
+      %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+      "stablehlo.return"(%0) : (tensor<f64>) -> ()
+  }) {
+    replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<f32>) -> tensor<f64>
+
+  func.return %result : tensor<f64>
+}
+
+// CHECK-LABEL: "default_all_reduce_variadic"
+func.func @default_all_reduce_variadic(%arg0: tensor<f32>, %arg1: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  %0:2 = "stablehlo.all_reduce"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> (tensor<f32>)
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<f32>, tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "op_all_to_all"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_all_to_all(%arg0: tensor<4x16xf32>) -> tensor<16x4xf32> {
+  //               CHECK: "vhlo.all_to_all_v2"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  //          CHECK-SAME:   concat_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>>,
+  //          CHECK-SAME:   split_count = #vhlo.integer_v1<4 : i64>
+  //          CHECK-SAME:   split_dimension = #vhlo.integer_v1<1 : i64>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<4x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x4x!vhlo.f32_v1>
+  %0 = "stablehlo.all_to_all"(%arg0) {
+    split_dimension = 1 : i64,
+    concat_dimension = 0 : i64,
+    split_count = 4 : i64,
+    replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>
+  } : (tensor<4x16xf32>) -> tensor<16x4xf32>
+  func.return %0 : tensor<16x4xf32>
+}
+
+// CHECK-LABEL: "op_and"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_and(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.and_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.and"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_atan2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_atan2(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.atan2_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.atan2"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_batch_norm_grad"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}}, %[[ARG4:.*]]: {{.*}})
+func.func @op_batch_norm_grad(%arg0: tensor<16x16x16x16xf32>, %arg1: tensor<16xf32>, %arg2: tensor<16xf32>, %arg3: tensor<16xf32>, %arg4: tensor<16x16x16x16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>) {
+  //      CHECK: "vhlo.batch_norm_grad_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]], %[[ARG4]]) <{
+  // CHECK-SAME:   epsilon = #vhlo.float_v1<1.000000e-03 : !vhlo.f32_v1>,
+  // CHECK-SAME:   feature_index = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>) -> (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>)
+  %0:3 = "stablehlo.batch_norm_grad"(%arg0, %arg1, %arg2, %arg3, %arg4) {
+    epsilon = 0.001 : f32,
+    feature_index = 0 : i64
+  } : (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16x16x16x16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>)
+  func.return %0#0, %0#1, %0#2 : tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_batch_norm_inference"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}}, %[[ARG4:.*]]: {{.*}})
+func.func @op_batch_norm_inference(%arg0: tensor<16x16x16x16xf32>, %arg1: tensor<16xf32>, %arg2: tensor<16xf32>, %arg3: tensor<16xf32>, %arg4: tensor<16xf32>) -> tensor<16x16x16x16xf32> {
+  //      CHECK: "vhlo.batch_norm_inference_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]], %[[ARG4]]) <{
+  // CHECK-SAME:   epsilon = #vhlo.float_v1<1.000000e-03 : !vhlo.f32_v1>,
+  // CHECK-SAME:   feature_index = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.batch_norm_inference"(%arg0, %arg1, %arg2, %arg3, %arg4) {
+    epsilon = 0.001 : f32,
+    feature_index = 0 : i64
+  } : (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16xf32>, tensor<16xf32>) -> tensor<16x16x16x16xf32>
+  func.return %0 : tensor<16x16x16x16xf32>
+}
+
+// CHECK-LABEL: "op_batch_norm_training"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_batch_norm_training(%arg0: tensor<16x16x16x16xf32>, %arg1: tensor<16xf32>, %arg2: tensor<16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>) {
+  //      CHECK: "vhlo.batch_norm_training_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   epsilon = #vhlo.float_v1<1.000000e-03 : !vhlo.f32_v1>,
+  // CHECK-SAME:   feature_index = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>) -> (!vhlo.tensor_v1<16x16x16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x!vhlo.f32_v1>)
+  %0:3 = "stablehlo.batch_norm_training"(%arg0, %arg1, %arg2) {
+    epsilon = 0.001 : f32,
+    feature_index = 0 : i64
+  } : (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>) -> (tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>)
+  func.return %0#0, %0#1, %0#2 : tensor<16x16x16x16xf32>, tensor<16xf32>, tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_bitcast_convert"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_bitcast_convert(%arg0: tensor<i32>) -> tensor<f32> {
+  // CHECK: "vhlo.bitcast_convert_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.bitcast_convert"(%arg0) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_broadcast_in_dim"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_broadcast_in_dim(%arg0: tensor<16xf32>) -> tensor<16x16xf32> {
+  //      CHECK: "vhlo.broadcast_in_dim_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.broadcast_in_dim"(%arg0) {
+    broadcast_dimensions = array<i64: 1>
+  } : (tensor<16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_broadcast"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_broadcast(%arg0: tensor<16xf32>) -> tensor<16x16xf32> {
+  //      CHECK: "vhlo.broadcast_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   broadcast_sizes = #vhlo.tensor_v1<dense<16> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.broadcast"(%arg0) {
+    broadcast_sizes = array<i64: 16>
+  } : (tensor<16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_case"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_case(%arg0: tensor<i32>, %arg1: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.case_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.case"(%arg0) ({
+    "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_cbrt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cbrt(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.cbrt_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.cbrt"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_ceil"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_ceil(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.ceil_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.ceil"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_cholesky"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cholesky(%arg0: tensor<1x16x16xf32>) -> tensor<1x16x16xf32> {
+  //      CHECK: "vhlo.cholesky_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   lower = #vhlo.bool_v1<true>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.cholesky"(%arg0) {
+    lower = true
+  } : (tensor<1x16x16xf32>) -> tensor<1x16x16xf32>
+  func.return %0 : tensor<1x16x16xf32>
+}
+
+// CHECK-LABEL: "op_clamp"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_clamp(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.clamp_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.clamp"(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_count_leading_zeros"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_count_leading_zeros(%arg0: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.count_leading_zeros_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.count_leading_zeros"(%arg0) : (tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_collective_permute"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_collective_permute(%arg0: tensor<16x8xf32>) -> tensor<16x8xf32> {
+  //               CHECK: "vhlo.collective_permute_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x8x!vhlo.f32_v1>
+  %0 = "stablehlo.collective_permute"(%arg0) {
+    source_target_pairs = dense<[[0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>
+  } : (tensor<16x8xf32>) -> tensor<16x8xf32>
+  func.return %0 : tensor<16x8xf32>
+}
+
+// CHECK-LABEL: "op_compare"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_compare(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<i1> {
+  //      CHECK: "vhlo.compare_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   compare_type = #vhlo<comparison_type_v1 TOTALORDER>,
+  // CHECK-SAME:   comparison_direction = #vhlo<comparison_direction_v1 EQ>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.compare"(%arg0, %arg1) {
+    comparison_direction = #stablehlo<comparison_direction EQ>,
+    compare_type = #stablehlo<comparison_type TOTALORDER>
+  } : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_complex"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_complex(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<complex<f32>> {
+  // CHECK: "vhlo.complex_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.complex"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<complex<f32>>
+  func.return %0 : tensor<complex<f32>>
+}
+
+// CHECK-LABEL: "op_composite"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_composite(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.composite_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"my_int"> = #vhlo.integer_v1<1 : i64>, #vhlo.string_v1<"my_string"> = #vhlo.string_v1<"foo">}>
+  //          CHECK-SAME:   decomposition = #vhlo.string_v1<"composite_target">
+  //          CHECK-SAME:   name = #vhlo.string_v1<"stablehlo.composite_target">
+  //          CHECK-SAME:   version = #vhlo.integer_v1<1 : i32>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.composite"(%arg0) {
+    name = "stablehlo.composite_target",
+    decomposition = @composite_target,
+    version = 1 : i32,
+    composite_attributes = {
+      my_string = "foo",
+      my_int = 1 : i64
+    }
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_concatenate"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_concatenate(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.concatenate_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x!vhlo.f32_v1>, !vhlo.tensor_v1<8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.concatenate"(%arg0, %arg1) {
+    dimension = 0 : i64
+  } : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_constant"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_constant(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.constant_v1"() <{
+  // CHECK-SAME:   value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>
+  // CHECK-SAME: }> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.constant"() {
+    value = dense<0.0> : tensor<f32>
+  } : () -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_convert"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_convert(%arg0: tensor<i32>) -> tensor<f32> {
+  // CHECK: "vhlo.convert_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.convert"(%arg0) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_convolution"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_convolution(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>) -> tensor<1x7x7x16xf32> {
+  //      CHECK: "vhlo.convolution_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<1> : tensor<2x2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<true> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
+  %0 = "stablehlo.convolution"(%arg0, %arg1) {
+    window_strides = array<i64: 2, 2>,
+    padding = dense<1> : tensor<2x2xi64>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64,
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>) -> tensor<1x7x7x16xf32>
+  func.return %0 : tensor<1x7x7x16xf32>
+}
+
+// CHECK-LABEL: "op_cosine"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cosine(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.cosine_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.cosine"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_create_token"
+func.func @op_create_token() -> !stablehlo.token {
+  // CHECK: "vhlo.create_token_v1"() : () -> !vhlo.token_v1
+  %0 = "stablehlo.create_token"() : () -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_cross_replica_sum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_cross_replica_sum(%arg0: tensor<f32>) -> tensor<f32> {
+  //               CHECK: "vhlo.cross-replica-sum_v1"(%[[ARG0]]) <{
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>
+  //          CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.cross-replica-sum"(%arg0) {
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_custom_call"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_custom_call(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.custom_call_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING>,
+  // CHECK-SAME:   backend_config = #vhlo.string_v1<"\08\03\1A\02">,
+  // CHECK-SAME:   call_target_name = #vhlo.string_v1<"foo">,
+  // CHECK-SAME:   called_computations = #vhlo.array_v1<[#vhlo.string_v1<"foo">]>,
+  // CHECK-SAME:   has_side_effect = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   operand_layouts = #vhlo.array_v1<[#vhlo.tensor_v1<dense<> : tensor<0xindex>>]>,
+  // CHECK-SAME:   output_operand_aliases = #vhlo.array_v1<[
+  // CHECK-SAME:     #vhlo.output_operand_alias_v1<
+  // CHECK-SAME:       outputTupleIndices = [],
+  // CHECK-SAME:       operandIndex = 0,
+  // CHECK-SAME:       operandTupleIndices = []>]>
+  // CHECK-SAME:   result_layouts = #vhlo.array_v1<[#vhlo.tensor_v1<dense<> : tensor<0xindex>>]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo",
+    has_side_effect = true,
+    backend_config = "\08\03\1A\02",
+    api_version = 2 : i32,
+    called_computations = [@foo],
+    operand_layouts = [dense<> : tensor<0xindex>],
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [],
+                                 operand_index = 0,
+                                 operand_tuple_indices = []>],
+    result_layouts = [dense<> : tensor<0xindex>]
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_custom_call_empty_result_layout"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func public @op_custom_call_empty_result_layout(%arg0: tensor<i64>) -> tensor<i64> {
+  // %0 = "vhlo.custom_call_v1"(%arg0) <{>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tuple_v1<>
+  //      CHECK: "vhlo.custom_call_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   api_version = #vhlo<api_version_v1 API_VERSION_STATUS_RETURNING>,
+  // CHECK-SAME:   backend_config = #vhlo.string_v1<"">,
+  // CHECK-SAME:   call_target_name = #vhlo.string_v1<"empty_output">,
+  // CHECK-SAME:   called_computations = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   has_side_effect = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   operand_layouts = #vhlo.array_v1<[#vhlo.tensor_v1<dense<> : tensor<0xindex>>]>,
+  // CHECK-SAME:   output_operand_aliases = #vhlo.array_v1<[]>,
+  // CHECK-SAME:   result_layouts = #vhlo.array_v1<[]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tuple_v1<>
+  %0 = "stablehlo.custom_call"(%arg0) <{
+    api_version = 2 : i32,
+    call_target_name = "empty_output",
+    has_side_effect = true,
+    operand_layouts = [dense<> : tensor<0xindex>],
+    result_layouts = []
+  }> : (tensor<i64>) -> tuple<>
+  return %arg0 : tensor<i64>
+}
+
+// CHECK-LABEL: "op_divide"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_divide(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.divide_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.divide"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_dot_general"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dot_general(%arg0: tensor<8x8x16xf32>, %arg1: tensor<8x16x8xf32>) -> tensor<8x8x8xf32> {
+  //      CHECK: "vhlo.dot_general_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   accumulation_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>,
+  // CHECK-SAME:   rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>,
+  // CHECK-SAME:   rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>,
+  // CHECK-SAME:   rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<8x16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot_general"(%arg0, %arg1) {
+    dot_dimension_numbers = #stablehlo.dot<
+      lhs_batching_dimensions = [0],
+      lhs_contracting_dimensions = [2],
+      rhs_batching_dimensions = [0],
+      rhs_contracting_dimensions = [1]
+    >,
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<8x8x16xf32>, tensor<8x16x8xf32>) -> tensor<8x8x8xf32>
+  func.return %0 : tensor<8x8x8xf32>
+}
+
+// CHECK-LABEL: "op_dot"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dot(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  //      CHECK: "vhlo.dot_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dot"(%arg0, %arg1) {
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_broadcast_in_dim"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dynamic_broadcast_in_dim(%arg0: tensor<?x?xf32>, %arg1: tensor<2xindex>) -> tensor<?x?xf32> {
+  //      CHECK: "vhlo.dynamic_broadcast_in_dim_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   known_expanding_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   known_nonexpanding_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x?x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_broadcast_in_dim"(%arg0, %arg1) {
+    broadcast_dimensions = array<i64: 0, 1>,
+    known_expanding_dimensions = array<i64: 0>,
+    known_nonexpanding_dimensions = array<i64: 1>
+  } : (tensor<?x?xf32>, tensor<2xindex>) -> tensor<?x?xf32>
+  func.return %0 : tensor<?x?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_conv"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_conv(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>, %arg2: tensor<2x2xi64>) -> tensor<1x?x?x16xf32> {
+  //      CHECK: "vhlo.dynamic_conv_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   batch_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   feature_group_count = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   input_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   input_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   input_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   kernel_input_feature_dimension = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   kernel_output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   lhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   output_batch_dimension = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   output_feature_dimension = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   output_spatial_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   precision_config = #vhlo.array_v1<[#vhlo<precision_v1 HIGHEST>, #vhlo<precision_v1 HIGHEST>]>,
+  // CHECK-SAME:   rhs_dilation = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>,
+  // CHECK-SAME:   window_reversal = #vhlo.tensor_v1<dense<true> : tensor<2xi1>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<2x2x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
+    window_strides = array<i64: 2, 2>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
+    dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
+    feature_group_count = 1 : i64,
+    batch_group_count = 1 : i64,
+    precision_config = [#stablehlo<precision HIGHEST>, #stablehlo<precision HIGHEST>]
+  } : (tensor<1x8x8x207xf32>, tensor<3x3x207x16xf32>, tensor<2x2xi64>) -> tensor<1x?x?x16xf32>
+  func.return %0 : tensor<1x?x?x16xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>, %arg2 : tensor<3xi32>) -> tensor<1x5x8xf32> {
+  //      CHECK: "vhlo.dynamic_gather_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>, !vhlo.tensor_v1<3x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >,
+    indices_are_sorted = true
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x8xf32>
+  func.return %0 : tensor<1x5x8xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_gather_with_batching_dims"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_gather_with_batching_dims(%arg0 : tensor<5x2x4x9xf32>, %arg1 : tensor<1x5x2xi32>, %arg2 : tensor<4xi32>) -> tensor<1x5x8xf32> {
+  //      CHECK: "vhlo.dynamic_gather_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<5x2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x8x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [1, 2],
+      operand_batching_dims = [0],
+      start_indices_batching_dims = [1],
+      start_index_map = [1, 2],
+      index_vector_dim = 2
+    >,
+    indices_are_sorted = true
+  } : (tensor<5x2x4x9xf32>, tensor<1x5x2xi32>, tensor<4xi32>) -> tensor<1x5x8xf32>
+  func.return %0 : tensor<1x5x8xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_iota"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_dynamic_iota(%arg0: tensor<1xindex>) -> tensor<?xf32> {
+  //      CHECK: "vhlo.dynamic_iota_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   iota_dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<1x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_iota"(%arg0) {
+    iota_dimension = 0 : i64
+  } : (tensor<1xindex>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_pad"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}}, %[[ARG4:.*]]: {{.*}})
+func.func @op_dynamic_pad(%arg0: tensor<?xf32>, %arg1: tensor<f32>, %arg2: tensor<1xindex>, %arg3: tensor<1xindex>, %arg4: tensor<1xindex>) -> tensor<?xf32> {
+  // CHECK: "vhlo.dynamic_pad_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<?x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_pad"(%arg0, %arg1, %arg2, %arg3, %arg4) : (tensor<?xf32>, tensor<f32>, tensor<1xindex>, tensor<1xindex>, tensor<1xindex>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_reshape"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dynamic_reshape(%arg0: tensor<16xf32>, %arg1: tensor<2xindex>) -> tensor<?x?xf32> {
+  // CHECK: "vhlo.dynamic_reshape_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x?x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_reshape"(%arg0, %arg1) : (tensor<16xf32>, tensor<2xindex>) -> tensor<?x?xf32>
+  func.return %0 : tensor<?x?xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_dynamic_slice(%arg0: tensor<16xf32>, %arg1: tensor<i64>) -> tensor<4xf32> {
+  //      CHECK: "vhlo.dynamic_slice_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<4x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_slice"(%arg0, %arg1) {
+    slice_sizes = array<i64: 4>
+  } : (tensor<16xf32>, tensor<i64>) -> tensor<4xf32>
+  func.return %0 : tensor<4xf32>
+}
+
+// CHECK-LABEL: "op_dynamic_update_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_dynamic_update_slice(%arg0: tensor<16xf32>, %arg1: tensor<4xf32>, %arg2: tensor<i64>) -> tensor<16xf32> {
+  // CHECK: "vhlo.dynamic_update_slice_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.dynamic_update_slice"(%arg0, %arg1, %arg2) : (tensor<16xf32>, tensor<4xf32>, tensor<i64>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_einsum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_einsum(%arg0: tensor<8x16xf32>, %arg1: tensor<16x8xf32>) -> tensor<8x8xf32> {
+  //      CHECK: "vhlo.einsum_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   einsum_config = #vhlo.string_v1<"ab,bc->ac">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x8x!vhlo.f32_v1>
+  %0 = "stablehlo.einsum"(%arg0, %arg1) {
+    einsum_config = "ab,bc->ac"
+  } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
+  func.return %0 : tensor<8x8xf32>
+}
+
+// CHECK-LABEL: "op_exponential_minus_one"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_exponential_minus_one(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.exponential_minus_one_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.exponential_minus_one"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_exponential"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_exponential(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.exponential_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.exponential"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_fft"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_fft(%arg0: tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>> {
+  //      CHECK: "vhlo.fft_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   fft_length = #vhlo.tensor_v1<dense<16> : tensor<1xi64>>,
+  // CHECK-SAME:   fft_type = #vhlo<fft_type_v1 FFT>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<16x!vhlo.complex_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.fft"(%arg0) {
+    fft_type = #stablehlo<fft_type FFT>,
+    fft_length = array<i64: 16>
+  } : (tensor<16xcomplex<f32>>) -> tensor<16xcomplex<f32>>
+  func.return %0 : tensor<16xcomplex<f32>>
+}
+
+// CHECK-LABEL: "op_floor"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_floor(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.floor_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.floor"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+func.func private @op_func(%arg0: tensor<f32> {stablehlo.arg = "0"}) -> (tensor<f32> {stablehlo.result = "0"}) {
+  // CHECK:      "vhlo.func_v1"() <{
+  // CHECK-SAME:   arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"stablehlo.arg"> = #vhlo.string_v1<"0">}>]>,
+  // CHECK-SAME:   function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>>>,
+  // CHECK-SAME:   res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"stablehlo.result"> = #vhlo.string_v1<"0">}>]>,
+  // CHECK-SAME:   sym_name = #vhlo.string_v1<"op_func">,
+  // CHECK-SAME:   sym_visibility = #vhlo.string_v1<"private">
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG0:.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : () -> ()
+
+  func.return %arg0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_gather"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_gather(%arg0 : tensor<2x4x9xf32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x1xf32> {
+  //      CHECK: "vhlo.gather_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x1x!vhlo.f32_v1>
+  %0 = "stablehlo.gather"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [0, 1],
+      start_index_map = [0, 1],
+      index_vector_dim = 2
+    >,
+    slice_sizes = array<i64: 1, 1, 1>,
+    indices_are_sorted = true
+  } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
+  func.return %0 : tensor<1x5x1xf32>
+}
+
+// CHECK-LABEL: "op_gather_with_batching_dims"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_gather_with_batching_dims(%arg0 : tensor<5x2x4x9xf32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x1xf32> {
+  //      CHECK: "vhlo.gather_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   collapsed_slice_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   offset_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>,
+  // CHECK-SAME:   operand_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   slice_sizes = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>,
+  // CHECK-SAME:   start_index_map = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   start_indices_batching_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<5x2x4x9x!vhlo.f32_v1>, !vhlo.tensor_v1<1x5x2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x5x1x!vhlo.f32_v1>
+  %0 = "stablehlo.gather"(%arg0, %arg1) {
+    dimension_numbers = #stablehlo.gather<
+      offset_dims = [2],
+      collapsed_slice_dims = [1, 2],
+      operand_batching_dims = [0],
+      start_indices_batching_dims = [1],
+      start_index_map = [1, 2],
+      index_vector_dim = 2
+    >,
+    slice_sizes = array<i64: 1, 1, 1, 1>,
+    indices_are_sorted = true
+  } : (tensor<5x2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
+  func.return %0 : tensor<1x5x1xf32>
+}
+
+// CHECK-LABEL: "op_get_dimension_size"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_get_dimension_size(%arg0: tensor<?xf32>) -> tensor<i32> {
+  //      CHECK: "vhlo.get_dimension_size_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.get_dimension_size"(%arg0) {
+    dimension = 0 : i64
+  } : (tensor<?xf32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_get_tuple_element"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_get_tuple_element(%arg0: tuple<tensor<f32>, tensor<i32>>) -> tensor<f32> {
+  //      CHECK: "vhlo.get_tuple_element_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   index = #vhlo.integer_v1<0 : i32>
+  // CHECK-SAME: }> : (!vhlo.tuple_v1<!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.get_tuple_element"(%arg0) {
+    index = 0 : i32
+  } : (tuple<tensor<f32>, tensor<i32>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_if"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_if(%arg0: tensor<i1>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.if_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }, {
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.if"(%arg0) ({
+    "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }, {
+    "stablehlo.return"(%arg2) : (tensor<f32>) -> ()
+  }) : (tensor<i1>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_imag"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_imag(%arg0: tensor<complex<f32>>) -> tensor<f32> {
+  // CHECK: "vhlo.imag_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.imag"(%arg0) : (tensor<complex<f32>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_infeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_infeed(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //               CHECK: "vhlo.infeed_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   infeed_config = #vhlo.string_v1<"foo">,
+  // CHECK-SAME{LITERAL}:   layout = #vhlo.array_v1<[#vhlo.array_v1<[]>]>
+  //          CHECK-SAME: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.infeed"(%arg0) {
+    infeed_config = "foo",
+    layout = [[]]
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "op_iota"
+func.func @op_iota() -> tensor<16xf32> {
+  //      CHECK: "vhlo.iota_v1"() <{
+  // CHECK-SAME:   iota_dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : () -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.iota"() {
+    iota_dimension = 0 : i64
+  } : () -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_is_finite"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_is_finite(%arg0: tensor<f32>) -> tensor<i1> {
+  // CHECK: "vhlo.is_finite_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.is_finite"(%arg0) : (tensor<f32>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_log"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_log(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.log_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.log"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_log_plus_one"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_log_plus_one(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.log_plus_one_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.log_plus_one"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_logistic"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_logistic(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.logistic_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.logistic"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_map"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_map(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.map_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.abs_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.map"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>):
+      %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    dimensions = array<i64: 0>
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_maximum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_maximum(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.maximum_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.maximum"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_minimum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_minimum(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.minimum_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.minimum"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_multiply"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_multiply(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.multiply_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.multiply"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_negate"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_negate(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.negate_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.negate"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_not"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_not(%arg0: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.not_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.not"(%arg0) : (tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_optimization_barrier"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_optimization_barrier(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.optimization_barrier_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.optimization_barrier"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_or"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_or(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.or_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.or"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "op_outfeed"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_outfeed(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.outfeed_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   outfeed_config = #vhlo.string_v1<"foo">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.outfeed"(%arg0, %arg1) {
+    outfeed_config = "foo"
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_pad"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_pad(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.pad_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   edge_padding_high = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>,
+  // CHECK-SAME:   edge_padding_low = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>,
+  // CHECK-SAME:   interior_padding = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.pad"(%arg0, %arg1) {
+    edge_padding_high = array<i64: 4>,
+    edge_padding_low = array<i64: 4>,
+    interior_padding = array<i64: 0>
+  } : (tensor<8xf32>, tensor<f32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_popcnt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_popcnt(%arg0: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.popcnt_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.popcnt"(%arg0) : (tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_power"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_power(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.power_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.power"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_real_dynamic_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}}, %[[ARG3:.*]]: {{.*}})
+func.func @op_real_dynamic_slice(%arg0: tensor<?xf32>, %arg1: tensor<1xindex>, %arg2: tensor<1xindex>, %arg3: tensor<1xindex>) -> tensor<?xf32> {
+  // CHECK: "vhlo.real_dynamic_slice_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]], %[[ARG3]]) : (!vhlo.tensor_v1<?x!vhlo.f32_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>, !vhlo.tensor_v1<1x!vhlo.index_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.real_dynamic_slice"(%arg0, %arg1, %arg2, %arg3) : (tensor<?xf32>, tensor<1xindex>, tensor<1xindex>, tensor<1xindex>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "op_real"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_real(%arg0: tensor<complex<f32>>) -> tensor<f32> {
+  // CHECK: "vhlo.real_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.real"(%arg0) : (tensor<complex<f32>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_recv_no_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_recv_no_source_target_pairs(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //      CHECK: "vhlo.recv_v2"(%[[ARG0]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<3 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<> : tensor<0xi64>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  %0:2 = "stablehlo.recv"(%arg0) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 3>,
+    is_host_transfer = true
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
+
+// CHECK-LABEL: "op_reduce"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_reduce(%arg0: tensor<16xf32>, %arg1: tensor<f32>) -> tensor<f32> {
+  //  CHECK: "vhlo.reduce_v1"(%[[ARG0]], %[[ARG1]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.reduce"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    dimensions = array<i64: 0>
+  } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_reduce_precision"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reduce_precision(%arg0: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.reduce_precision_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   exponent_bits = #vhlo.integer_v1<8 : i32>
+  // CHECK-SAME:   mantissa_bits = #vhlo.integer_v1<10 : i32>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_precision"(%arg0) {
+    exponent_bits = 8 : i32,
+    mantissa_bits = 10 : i32
+  } : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK_lABEL: "op_reduce_with_promotable_types"
+func.func @op_reduce_with_promotable_types(%arg0: tensor<4x4xf32>, %arg1 : tensor<f32>)
+    -> (tensor<4xf64>) {
+  //  CHECK: "vhlo.reduce_v1"(%[[ARG0:.*]], %[[ARG1:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<4x4x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x!vhlo.f64_v1>
+  %0 = "stablehlo.reduce"(%arg0, %arg1) ({
+  ^bb0(%arg2: tensor<f64>, %arg3: tensor<f64> ):
+    %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+    "stablehlo.return"(%1) : (tensor<f64>) -> ()
+
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+
+  func.return %0: tensor<4xf64>
+}
+
+// CHECK-LABEL: "op_reduce_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reduce_scatter(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //               CHECK: "vhlo.reduce_scatter_v1"(%[[ARG0]]) <{
+  //          CHECK-SAME:   channel_id = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME{LITERAL}:   replica_groups = #vhlo.tensor_v1<dense<[[0], [1]]> : tensor<2x1xi64>>,
+  //          CHECK-SAME:   scatter_dimension = #vhlo.integer_v1<0 : i64>
+  //          CHECK-SAME:   use_global_device_ids = #vhlo.bool_v1<true>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_scatter"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.add"(%arg1, %arg2) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension = 0 : i64,
+    replica_groups = dense<[[0], [1]]> : tensor<2x1xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+    use_global_device_ids
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK_lABEL: "op_reduce_scatter_with_promotable_types"
+func.func @op_reduce_scatter_with_promotable_types(%data: tensor<4x16xf32>) -> tensor<4x4xf64> {
+  //  CHECK: "vhlo.reduce_scatter_v1"(%[[ARG0:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<4x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x4x!vhlo.f64_v1>
+  %0 = "stablehlo.reduce_scatter"(%data) ({
+    ^bb0(%arg2: tensor<f64>, %arg3: tensor<f64>):
+    %1 = stablehlo.add %arg2, %arg3 : tensor<f64>
+    "stablehlo.return"(%1) : (tensor<f64>) -> ()
+  }) {replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>,
+      scatter_dimension = 1 : i64,
+      channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>,
+      use_global_device_ids} : (tensor<4x16xf32>) -> tensor<4x4xf64>
+  func.return %0 : tensor<4x4xf64>
+}
+
+
+// CHECK-LABEL: "op_reduce_window"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_reduce_window(%arg0: tensor<2x17x31x7xf32>, %arg1: tensor<f32>) -> tensor<2x9x16x7xf32> {
+  //               CHECK: "vhlo.reduce_window_v1"(%[[ARG0]], %[[ARG1]]) <{
+  //          CHECK-SAME:   base_dilations = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  // CHECK-SAME{LITERAL}:   padding = #vhlo.tensor_v1<dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>>,
+  //          CHECK-SAME:   window_dilations = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  //          CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<[1, 4, 4, 1]> : tensor<4xi64>>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.maximum_v1"(%[[ARG2]], %[[ARG3]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  //          CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //          CHECK-NEXT: }) : (!vhlo.tensor_v1<2x17x31x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x9x16x7x!vhlo.f32_v1>
+  %0 = "stablehlo.reduce_window"(%arg0, %arg1) ({
+    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
+      %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
+    padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
+  } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
+  func.return %0 : tensor<2x9x16x7xf32>
+}
+
+// CHECK-LABEL: "op_reduce_window_with_promotable_types"
+func.func @op_reduce_window_with_promotable_types(%arg0: tensor<4x2xf32>,
+    %arg1: tensor<4x2xf32>, %init0: tensor<f32>, %init1: tensor<f32>) ->
+    (tensor<2x2xf64>, tensor<2x2xf32>) {
+  //  CHECK: "vhlo.reduce_window_v1"(%[[ARG0:.*]], %[[ARG1:.*]], %[[ARG2:.*]], %[[ARG3:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]], %[[VAL2:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<4x2x!vhlo.f32_v1>, !vhlo.tensor_v1<4x2x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> (!vhlo.tensor_v1<2x2x!vhlo.f64_v1>, !vhlo.tensor_v1<2x2x!vhlo.f32_v1>)
+  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
+         ^bb0(%a0: tensor<f64>, %a1: tensor<f32>, %b0: tensor<f64>,
+                %b1: tensor<f32>):
+              %2 = stablehlo.add %a0, %b0 : tensor<f64>
+              %3 = stablehlo.add %a1, %b1 : tensor<f32>
+              "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
+            })
+         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
+         window_dimensions = array<i64: 5, 1>,
+         window_strides = array<i64: 3, 1> }
+         : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
+              (tensor<2x2xf64>, tensor<2x2xf32>)
+  func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
+}
+
+// CHECK-LABEL: "op_remainder"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_remainder(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.remainder_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.remainder"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_replica_id"
+func.func @op_replica_id() -> tensor<ui32> {
+  // CHECK: "vhlo.replica_id_v1"() : () -> !vhlo.tensor_v1<!vhlo.ui32_v1>
+  %0 = "stablehlo.replica_id"() : () -> tensor<ui32>
+  func.return %0 : tensor<ui32>
+}
+
+// CHECK-LABEL: "op_partition_id"
+func.func @op_partition_id() -> tensor<ui32> {
+  // CHECK: "vhlo.partition_id_v1"() : () -> !vhlo.tensor_v1<!vhlo.ui32_v1>
+  %0 = "stablehlo.partition_id"() : () -> tensor<ui32>
+  func.return %0 : tensor<ui32>
+}
+
+// CHECK-LABEL: "op_reshape"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reshape(%arg0: tensor<16xf32>) -> tensor<4x4xf32> {
+  // CHECK: "vhlo.reshape_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x4x!vhlo.f32_v1>
+  %0 = "stablehlo.reshape"(%arg0) : (tensor<16xf32>) -> tensor<4x4xf32>
+  func.return %0 : tensor<4x4xf32>
+}
+
+// CHECK-LABEL: "op_return"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_return(%arg0: tensor<i32>, %arg1: tensor<f32>) -> tensor<f32> {
+  //      CHECK: "vhlo.case_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.case"(%arg0) ({
+    "stablehlo.return"(%arg1) : (tensor<f32>) -> ()
+  }) : (tensor<i32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_reverse"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_reverse(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.reverse_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.reverse"(%arg0) {
+    dimensions = array<i64: 0>
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_rng_bit_generator"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_rng_bit_generator(%arg0: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
+  //      CHECK: "vhlo.rng_bit_generator_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   rng_algorithm = #vhlo<rng_algorithm_v1 PHILOX>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>)
+  %0:2 = "stablehlo.rng_bit_generator"(%arg0) {
+    rng_algorithm = #stablehlo<rng_algorithm PHILOX>
+  } : (tensor<f32>) -> (tensor<f32>, tensor<f32>)
+  func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
+}
+
+// CHECK-LABEL: "op_rng"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_rng(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<0xindex>) -> tensor<f32> {
+  //      CHECK: "vhlo.rng_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   rng_distribution = #vhlo<rng_distribution_v1 NORMAL>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<0x!vhlo.index_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.rng"(%arg0, %arg1, %arg2) {
+    rng_distribution = #stablehlo<rng_distribution NORMAL>
+  } : (tensor<f32>, tensor<f32>, tensor<0xindex>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_round_nearest_afz"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_round_nearest_afz(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.round_nearest_afz_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.round_nearest_afz"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_round_nearest_even"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_round_nearest_even(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.round_nearest_even_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.round_nearest_even"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_rsqrt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_rsqrt(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.rsqrt_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.rsqrt"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_scatter(%arg0: tensor<200x100x300xf32>, %arg1: tensor<10x2xi32>, %arg2: tensor<10x300xf32>) -> tensor<200x100x300xf32> {
+  //      CHECK: "vhlo.scatter_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>,
+  // CHECK-SAME:   unique_indices = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   update_window_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>
+  %0 = "stablehlo.scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [0, 1],
+      scatter_dims_to_operand_dims = [0, 1],
+      index_vector_dim = 1
+    >,
+    indices_are_sorted = true,
+    unique_indices = true
+  } : (tensor<200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) -> tensor<200x100x300xf32>
+  func.return %0 : tensor<200x100x300xf32>
+}
+
+// CHECK-LABEL: "op_scatter_with_batching_dims"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_scatter_with_batching_dims(%arg0: tensor<10x200x100x300xf32>, %arg1: tensor<10x2xi32>, %arg2: tensor<10x300xf32>) -> tensor<10x200x100x300xf32> {
+  //      CHECK: "vhlo.scatter_v2"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   index_vector_dim = #vhlo.integer_v1<1 : i64>,
+  // CHECK-SAME:   indices_are_sorted = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   input_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   inserted_window_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>,
+  // CHECK-SAME:   scatter_indices_batching_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   unique_indices = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   update_window_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG3:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG4:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.add_v1"(%[[ARG3]], %[[ARG4]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<10x200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x200x100x300x!vhlo.f32_v1>
+  %0 = "stablehlo.scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [1, 2],
+      input_batching_dims = [0],
+      scatter_dims_to_operand_dims = [1, 2],
+      scatter_indices_batching_dims = [0],
+      index_vector_dim = 1
+    >,
+    indices_are_sorted = true,
+    unique_indices = true
+  } : (tensor<10x200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) -> tensor<10x200x100x300xf32>
+  func.return %0 : tensor<10x200x100x300xf32>
+}
+
+// CHECK_lABEL: "op_scatter_with_promotable_types"
+func.func @op_scatter_with_promotable_types(%input_tensor: tensor<200x100x300xf32>,
+    %scatter_indices: tensor<10x2xi32>, %updates: tensor<10x300xf32>) ->
+      tensor<200x100x300xf64> {
+  //  CHECK: "vhlo.scatter_v2"(%[[ARG0:.*]], %[[ARG1:.*]], %[[ARG2:.*]])
+  //  CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  //  CHECK:     "vhlo.return_v1"(%[[VAL1:.*]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  //  CHECK: }) : (!vhlo.tensor_v1<200x100x300x!vhlo.f32_v1>, !vhlo.tensor_v1<10x2x!vhlo.i32_v1>, !vhlo.tensor_v1<10x300x!vhlo.f32_v1>) -> !vhlo.tensor_v1<200x100x300x!vhlo.f64_v1>
+  %0 = "stablehlo.scatter" (%input_tensor, %scatter_indices, %updates) ({
+  ^bb0(%lhs: tensor<f64>, %rhs: tensor<f64>):
+    %add = stablehlo.add %lhs, %rhs : tensor<f64>
+    "stablehlo.return"(%add) : (tensor<f64>) -> ()
+  }) {
+    scatter_dimension_numbers = #stablehlo.scatter<
+      update_window_dims = [1],
+      inserted_window_dims = [0, 1],
+      scatter_dims_to_operand_dims = [0, 1],
+      index_vector_dim = 1
+    >,
+    indices_are_sorted = true,
+    unique_indices = true
+  } : (tensor<200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) ->
+      tensor<200x100x300xf64>
+  func.return %0 : tensor<200x100x300xf64>
+}
+
+// CHECK-LABEL: "op_select_and_scatter"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_select_and_scatter(%arg0: tensor<10x24x24x64xf32>, %arg1: tensor<12x13x13x66xf32>, %arg2: tensor<f32>) -> tensor<10x24x24x64xf32> {
+  //      CHECK: "vhlo.select_and_scatter_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) <{
+  // CHECK-SAME:   padding = #vhlo.tensor_v1<dense<1> : tensor<4x2xi64>>,
+  // CHECK-SAME:   window_dimensions = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>,
+  // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<[1, 2, 2, 1]> : tensor<4xi64>>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG31:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG41:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL11:.*]] = "vhlo.compare_v1"(%[[ARG31]], %[[ARG41]]) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL11]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }, {
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG32:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG42:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL12:.*]] = "vhlo.add_v1"(%[[ARG32]], %[[ARG42]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL12]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>, !vhlo.tensor_v1<12x13x13x66x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>
+  %0 = "stablehlo.select_and_scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg3, %arg4) {compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GE>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }, {
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+      "stablehlo.return"(%1) : (tensor<f32>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
+    padding = dense<1> : tensor<4x2xi64>
+  } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
+  func.return %0 : tensor<10x24x24x64xf32>
+}
+
+// CHECK-LABEL: "op_select_and_scatter_with_promotable_types"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_select_and_scatter_with_promotable_types(%arg0: tensor<10x24x24x64xf32>, %arg1: tensor<12x13x13x66xf32>, %arg2: tensor<f32>) -> tensor<10x24x24x64xf64> {
+  // CHECK: "vhlo.select_and_scatter_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]])
+  // CHECK:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f64_v1>):
+  // CHECK:     %[[VAL:.*]] = "vhlo.add_v1"(%[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>, !vhlo.tensor_v1<!vhlo.f64_v1>) -> !vhlo.tensor_v1<!vhlo.f64_v1>
+  // CHECK:     "vhlo.return_v1"(%[[VAL]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>) -> ()
+  // CHECK: }) : (!vhlo.tensor_v1<10x24x24x64x!vhlo.f32_v1>, !vhlo.tensor_v1<12x13x13x66x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<10x24x24x64x!vhlo.f64_v1>
+  %0 = "stablehlo.select_and_scatter"(%arg0, %arg1, %arg2) ({
+    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg3, %arg4) {compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GE>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }, {
+    ^bb0(%arg3: tensor<f64>, %arg4: tensor<f64>):
+      %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+      "stablehlo.return"(%1) : (tensor<f64>) -> ()
+  }) {
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
+    padding = dense<1> : tensor<4x2xi64>
+  } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf64>
+  func.return %0 : tensor<10x24x24x64xf64>
+}
+
+// CHECK-LABEL: "op_select"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}}, %[[ARG2:.*]]: {{.*}})
+func.func @op_select(%arg0: tensor<i1>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.select_v1"(%[[ARG0]], %[[ARG1]], %[[ARG2]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.select"(%arg0, %arg1, %arg2) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_send_no_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_send_no_source_target_pairs(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.send_v2"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+  // CHECK-SAME:   channel_type = #vhlo.integer_v1<2 : i64>,
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  %0 = "stablehlo.send"(%arg0, %arg1) {
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 2>,
+    is_host_transfer = true
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "op_set_dimension_size"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_set_dimension_size(%arg0: tensor<?xf32>, %arg1: tensor<i32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.set_dimension_size_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<?x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.set_dimension_size"(%arg0, %arg1) {
+    dimension = 0 : i64
+  } : (tensor<?xf32>, tensor<i32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_shift_left"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_shift_left(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.shift_left_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.shift_left"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_shift_right_arithmetic"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_shift_right_arithmetic(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.shift_right_arithmetic_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.shift_right_arithmetic"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_shift_right_logical"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_shift_right_logical(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.shift_right_logical_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.shift_right_logical"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "op_sign"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sign(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.sign_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.sign"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_sine"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sine(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.sine_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.sine"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_slice"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_slice(%arg0: tensor<16xf32>) -> tensor<4xf32> {
+  //      CHECK: "vhlo.slice_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   limit_indices = #vhlo.tensor_v1<dense<4> : tensor<1xi64>>,
+  // CHECK-SAME:   start_indices = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>,
+  // CHECK-SAME:   strides = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<4x!vhlo.f32_v1>
+  %0 = "stablehlo.slice"(%arg0) {
+    start_indices = array<i64: 0>,
+    limit_indices = array<i64: 4>,
+    strides = array<i64: 1>
+  } : (tensor<16xf32>) -> tensor<4xf32>
+  func.return %0 : tensor<4xf32>
+}
+
+// CHECK-LABEL: "op_sort"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sort(%arg0: tensor<16xf32>) -> tensor<16xf32> {
+  //      CHECK: "vhlo.sort_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   dimension = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME:   is_stable = #vhlo.bool_v1<true>
+  // CHECK-SAME: }> ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>, %[[ARG2:arg.*]]: !vhlo.tensor_v1<!vhlo.f32_v1>):
+  // CHECK-NEXT:     %[[VAL1:.*]] = "vhlo.compare_v1"(%[[ARG1]], %[[ARG2]]) <{compare_type = #vhlo<comparison_type_v1 FLOAT>, comparison_direction = #vhlo<comparison_direction_v1 GT>}>
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[VAL1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x!vhlo.f32_v1>
+  %0 = "stablehlo.sort"(%arg0) ({
+    ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
+      %1 = "stablehlo.compare"(%arg1, %arg2) {compare_type = #stablehlo<comparison_type FLOAT>, comparison_direction = #stablehlo<comparison_direction GT>} : (tensor<f32>, tensor<f32>) -> tensor<i1>
+      "stablehlo.return"(%1) : (tensor<i1>) -> ()
+  }) {
+    dimension = 0 : i64,
+    is_stable = true
+  } : (tensor<16xf32>) -> tensor<16xf32>
+  func.return %0 : tensor<16xf32>
+}
+
+// CHECK-LABEL: "op_sqrt"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_sqrt(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.sqrt_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.sqrt"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_subtract"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_subtract(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.subtract_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.subtract"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_tan"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_tan(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.tan_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.tan"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_tanh"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_tanh(%arg0: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.tanh_v2"(%[[ARG0]]) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.tanh"(%arg0) : (tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_torch_index_select"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_torch_index_select(%arg0: tensor<5x1x5xf32>, %arg1: tensor<2xi32>) ->  tensor<2x1x5xf32> {
+  //      CHECK: "vhlo.torch_index_select_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   batch_dims = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME:   dim = #vhlo.integer_v1<0 : i64>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<5x1x5x!vhlo.f32_v1>, !vhlo.tensor_v1<2x!vhlo.i32_v1>) -> !vhlo.tensor_v1<2x1x5x!vhlo.f32_v1>
+  %0 = "stablehlo.torch_index_select"(%arg0, %arg1) {
+    dim = 0 : i64,
+    batch_dims = 0 : i64
+  } : (tensor<5x1x5xf32>, tensor<2xi32>) -> tensor<2x1x5xf32>
+  func.return %0 : tensor<2x1x5xf32>
+}
+
+// CHECK-LABEL: "op_transpose"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_transpose(%arg0: tensor<16x8xf32>) ->  tensor<8x16xf32> {
+  //      CHECK: "vhlo.transpose_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x16x!vhlo.f32_v1>
+  %0 = "stablehlo.transpose"(%arg0) {
+    permutation = array<i64: 1, 0>
+  } : (tensor<16x8xf32>) -> tensor<8x16xf32>
+  func.return %0 : tensor<8x16xf32>
+}
+
+// CHECK-LABEL: "op_triangular_solve"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_triangular_solve(%arg0: tensor<16x16xf32>, %arg1: tensor<16x16xf32>) ->  tensor<16x16xf32> {
+  //      CHECK: "vhlo.triangular_solve_v1"(%[[ARG0]], %[[ARG1]]) <{
+  // CHECK-SAME:   left_side = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   lower = #vhlo.bool_v1<true>,
+  // CHECK-SAME:   transpose_a = #vhlo<transpose_v1 NO_TRANSPOSE>,
+  // CHECK-SAME:   unit_diagonal = #vhlo.bool_v1<true>
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<16x16x!vhlo.f32_v1>, !vhlo.tensor_v1<16x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<16x16x!vhlo.f32_v1>
+  %0 = "stablehlo.triangular_solve"(%arg0, %arg1) {
+    left_side = true,
+    lower = true,
+    unit_diagonal = true,
+    transpose_a = #stablehlo<transpose NO_TRANSPOSE>
+  } : (tensor<16x16xf32>, tensor<16x16xf32>) -> tensor<16x16xf32>
+  func.return %0 : tensor<16x16xf32>
+}
+
+// CHECK-LABEL: "op_tuple"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_tuple(%arg0: tensor<f32>) -> tuple<tensor<f32>> {
+  // CHECK: "vhlo.tuple_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tuple_v1<!vhlo.tensor_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.tuple"(%arg0) : (tensor<f32>) -> tuple<tensor<f32>>
+  func.return %0 : tuple<tensor<f32>>
+}
+
+// CHECK-LABEL: "op_unary_einsum"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_unary_einsum(%arg0: tensor<8x16xf32>) -> tensor<8xf32> {
+  //      CHECK: "vhlo.unary_einsum_v1"(%[[ARG0]]) <{
+  // CHECK-SAME:   einsum_config = #vhlo.string_v1<"ab->a">
+  // CHECK-SAME: }> : (!vhlo.tensor_v1<8x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x!vhlo.f32_v1>
+  %0 = "stablehlo.unary_einsum"(%arg0) {
+    einsum_config = "ab->a"
+  } : (tensor<8x16xf32>) -> tensor<8xf32>
+  func.return %0 : tensor<8xf32>
+}
+
+// CHECK-LABEL: "op_uniform_dequantize"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_uniform_dequantize(%arg0: tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<f32> {
+  // CHECK: "vhlo.uniform_dequantize_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.uniform_dequantize"(%arg0) : (tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "op_uniform_quantize"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_uniform_quantize(%arg0: tensor<f32>) -> tensor<!quant.uniform<i8:f32, 34.0:16>> {
+  // CHECK: "vhlo.uniform_quantize_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>
+  %0 = "stablehlo.uniform_quantize"(%arg0) : (tensor<f32>) -> tensor<!quant.uniform<i8:f32, 34.0:16>>
+  func.return %0 : tensor<!quant.uniform<i8:f32, 34.0:16>>
+}
+
+// CHECK-LABEL: "op_while"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_while(%arg0: tensor<i1>) -> tensor<i1> {
+  //      CHECK: "vhlo.while_v1"(%[[ARG0]]) ({
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.bool_v1>):
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT:   }, {
+  // CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG1:arg.*]]: !vhlo.tensor_v1<!vhlo.bool_v1>)
+  // CHECK-NEXT:     "vhlo.return_v1"(%[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
+  // CHECK-NEXT: }) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.while"(%arg0) ({
+    ^bb0(%arg1: tensor<i1>):
+      "stablehlo.return"(%arg1) : (tensor<i1>) -> ()
+    }, {
+    ^bb0(%arg1: tensor<i1>):
+      "stablehlo.return"(%arg1) : (tensor<i1>) -> ()
+  }) : (tensor<i1>) -> tensor<i1>
+  func.return %0: tensor<i1>
+}
+
+// CHECK-LABEL: "op_xor"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_xor(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.xor_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.xor"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// ============ TYPES ============
+
+// CHECK-LABEL: "type_i1"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i1(%arg0: tensor<i1>, %arg1: tensor<i1>) -> tensor<i1> {
+  // CHECK: "vhlo.and_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
+  %0 = "stablehlo.and"(%arg0, %arg1) : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  func.return %0 : tensor<i1>
+}
+
+// CHECK-LABEL: "type_i2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i2(%arg0: tensor<i2>, %arg1: tensor<i2>) -> tensor<i2> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i2_v1>, !vhlo.tensor_v1<!vhlo.i2_v1>) -> !vhlo.tensor_v1<!vhlo.i2_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i2>, tensor<i2>) -> tensor<i2>
+  func.return %0 : tensor<i2>
+}
+
+// CHECK-LABEL: "type_i4"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i4(%arg0: tensor<i4>, %arg1: tensor<i4>) -> tensor<i4> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i4_v1>, !vhlo.tensor_v1<!vhlo.i4_v1>) -> !vhlo.tensor_v1<!vhlo.i4_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i4>, tensor<i4>) -> tensor<i4>
+  func.return %0 : tensor<i4>
+}
+
+// CHECK-LABEL: "type_i8"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i8(%arg0: tensor<i8>, %arg1: tensor<i8>) -> tensor<i8> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i8_v1>, !vhlo.tensor_v1<!vhlo.i8_v1>) -> !vhlo.tensor_v1<!vhlo.i8_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i8>, tensor<i8>) -> tensor<i8>
+  func.return %0 : tensor<i8>
+}
+
+// CHECK-LABEL: "type_i16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i16(%arg0: tensor<i16>, %arg1: tensor<i16>) -> tensor<i16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i16_v1>, !vhlo.tensor_v1<!vhlo.i16_v1>) -> !vhlo.tensor_v1<!vhlo.i16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i16>, tensor<i16>) -> tensor<i16>
+  func.return %0 : tensor<i16>
+}
+
+// CHECK-LABEL: "type_i32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i32(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
+  func.return %0 : tensor<i32>
+}
+
+// CHECK-LABEL: "type_i64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_i64(%arg0: tensor<i64>, %arg1: tensor<i64>) -> tensor<i64> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<!vhlo.i64_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>
+  func.return %0 : tensor<i64>
+}
+
+// CHECK-LABEL: "type_ui2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui2(%arg0: tensor<ui2>, %arg1: tensor<ui2>) -> tensor<ui2> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui2_v1>, !vhlo.tensor_v1<!vhlo.ui2_v1>) -> !vhlo.tensor_v1<!vhlo.ui2_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui2>, tensor<ui2>) -> tensor<ui2>
+  func.return %0 : tensor<ui2>
+}
+
+// CHECK-LABEL: "type_ui4"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui4(%arg0: tensor<ui4>, %arg1: tensor<ui4>) -> tensor<ui4> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui4_v1>, !vhlo.tensor_v1<!vhlo.ui4_v1>) -> !vhlo.tensor_v1<!vhlo.ui4_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui4>, tensor<ui4>) -> tensor<ui4>
+  func.return %0 : tensor<ui4>
+}
+
+// CHECK-LABEL: "type_ui8"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui8(%arg0: tensor<ui8>, %arg1: tensor<ui8>) -> tensor<ui8> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui8_v1>, !vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<!vhlo.ui8_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui8>, tensor<ui8>) -> tensor<ui8>
+  func.return %0 : tensor<ui8>
+}
+
+// CHECK-LABEL: "type_ui16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui16(%arg0: tensor<ui16>, %arg1: tensor<ui16>) -> tensor<ui16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui16_v1>, !vhlo.tensor_v1<!vhlo.ui16_v1>) -> !vhlo.tensor_v1<!vhlo.ui16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui16>, tensor<ui16>) -> tensor<ui16>
+  func.return %0 : tensor<ui16>
+}
+
+// CHECK-LABEL: "type_ui32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui32(%arg0: tensor<ui32>, %arg1: tensor<ui32>) -> tensor<ui32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui32_v1>, !vhlo.tensor_v1<!vhlo.ui32_v1>) -> !vhlo.tensor_v1<!vhlo.ui32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui32>, tensor<ui32>) -> tensor<ui32>
+  func.return %0 : tensor<ui32>
+}
+
+// CHECK-LABEL: "type_ui64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_ui64(%arg0: tensor<ui64>, %arg1: tensor<ui64>) -> tensor<ui64> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.ui64_v1>, !vhlo.tensor_v1<!vhlo.ui64_v1>) -> !vhlo.tensor_v1<!vhlo.ui64_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<ui64>, tensor<ui64>) -> tensor<ui64>
+  func.return %0 : tensor<ui64>
+}
+
+// CHECK-LABEL: "type_f4E2M1FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f4E2M1FN(%arg0: tensor<f4E2M1FN>, %arg1: tensor<f4E2M1FN>) -> tensor<f4E2M1FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f4E2M1FN_v1>, !vhlo.tensor_v1<!vhlo.f4E2M1FN_v1>) -> !vhlo.tensor_v1<!vhlo.f4E2M1FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f4E2M1FN>, tensor<f4E2M1FN>) -> tensor<f4E2M1FN>
+  func.return %0 : tensor<f4E2M1FN>
+}
+
+// CHECK-LABEL: "type_f6E2M3FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f6E2M3FN(%arg0: tensor<f6E2M3FN>, %arg1: tensor<f6E2M3FN>) -> tensor<f6E2M3FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f6E2M3FN_v1>, !vhlo.tensor_v1<!vhlo.f6E2M3FN_v1>) -> !vhlo.tensor_v1<!vhlo.f6E2M3FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f6E2M3FN>, tensor<f6E2M3FN>) -> tensor<f6E2M3FN>
+  func.return %0 : tensor<f6E2M3FN>
+}
+
+// CHECK-LABEL: "type_f6E3M2FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f6E3M2FN(%arg0: tensor<f6E3M2FN>, %arg1: tensor<f6E3M2FN>) -> tensor<f6E3M2FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f6E3M2FN_v1>, !vhlo.tensor_v1<!vhlo.f6E3M2FN_v1>) -> !vhlo.tensor_v1<!vhlo.f6E3M2FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f6E3M2FN>, tensor<f6E3M2FN>) -> tensor<f6E3M2FN>
+  func.return %0 : tensor<f6E3M2FN>
+}
+
+// CHECK-LABEL: "type_f8E3M4"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E3M4(%arg0: tensor<f8E3M4>, %arg1: tensor<f8E3M4>) -> tensor<f8E3M4> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E3M4_v1>, !vhlo.tensor_v1<!vhlo.f8E3M4_v1>) -> !vhlo.tensor_v1<!vhlo.f8E3M4_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E3M4>, tensor<f8E3M4>) -> tensor<f8E3M4>
+  func.return %0 : tensor<f8E3M4>
+}
+
+// CHECK-LABEL: "type_f8E4M3"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3(%arg0: tensor<f8E4M3>, %arg1: tensor<f8E4M3>) -> tensor<f8E4M3> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3>, tensor<f8E4M3>) -> tensor<f8E4M3>
+  func.return %0 : tensor<f8E4M3>
+}
+
+// CHECK-LABEL: "type_f8E4M3FN"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3FN(%arg0: tensor<f8E4M3FN>, %arg1: tensor<f8E4M3FN>) -> tensor<f8E4M3FN> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3FN_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3FN_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3FN_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3FN>, tensor<f8E4M3FN>) -> tensor<f8E4M3FN>
+  func.return %0 : tensor<f8E4M3FN>
+}
+
+// CHECK-LABEL: "type_f8E5M2"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E5M2(%arg0: tensor<f8E5M2>, %arg1: tensor<f8E5M2>) -> tensor<f8E5M2> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E5M2_v1>, !vhlo.tensor_v1<!vhlo.f8E5M2_v1>) -> !vhlo.tensor_v1<!vhlo.f8E5M2_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E5M2>, tensor<f8E5M2>) -> tensor<f8E5M2>
+  func.return %0 : tensor<f8E5M2>
+}
+
+// CHECK-LABEL: "type_f8E4M3FNUZ"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3FNUZ(%arg0: tensor<f8E4M3FNUZ>, %arg1: tensor<f8E4M3FNUZ>) -> tensor<f8E4M3FNUZ> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3FNUZ_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3FNUZ_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3FNUZ_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3FNUZ>, tensor<f8E4M3FNUZ>) -> tensor<f8E4M3FNUZ>
+  func.return %0 : tensor<f8E4M3FNUZ>
+}
+
+// CHECK-LABEL: "type_f8E4M3B11FNUZ"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E4M3B11FNUZ(%arg0: tensor<f8E4M3B11FNUZ>, %arg1: tensor<f8E4M3B11FNUZ>) -> tensor<f8E4M3B11FNUZ> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E4M3B11FNUZ_v1>, !vhlo.tensor_v1<!vhlo.f8E4M3B11FNUZ_v1>) -> !vhlo.tensor_v1<!vhlo.f8E4M3B11FNUZ_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E4M3B11FNUZ>, tensor<f8E4M3B11FNUZ>) -> tensor<f8E4M3B11FNUZ>
+  func.return %0 : tensor<f8E4M3B11FNUZ>
+}
+
+// CHECK-LABEL: "type_f8E5M2FNUZ"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E5M2FNUZ(%arg0: tensor<f8E5M2FNUZ>, %arg1: tensor<f8E5M2FNUZ>) -> tensor<f8E5M2FNUZ> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E5M2FNUZ_v1>, !vhlo.tensor_v1<!vhlo.f8E5M2FNUZ_v1>) -> !vhlo.tensor_v1<!vhlo.f8E5M2FNUZ_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E5M2FNUZ>, tensor<f8E5M2FNUZ>) -> tensor<f8E5M2FNUZ>
+  func.return %0 : tensor<f8E5M2FNUZ>
+}
+
+// CHECK-LABEL: "type_f8E8M0FNU"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f8E8M0FNU(%arg0: tensor<f8E8M0FNU>, %arg1: tensor<f8E8M0FNU>) -> tensor<f8E8M0FNU> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f8E8M0FNU_v1>, !vhlo.tensor_v1<!vhlo.f8E8M0FNU_v1>) -> !vhlo.tensor_v1<!vhlo.f8E8M0FNU_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E8M0FNU>, tensor<f8E8M0FNU>) -> tensor<f8E8M0FNU>
+  func.return %0 : tensor<f8E8M0FNU>
+}
+
+// CHECK-LABEL: "type_bf16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_bf16(%arg0: tensor<bf16>, %arg1: tensor<bf16>) -> tensor<bf16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
+  func.return %0 : tensor<bf16>
+}
+
+// CHECK-LABEL: "type_f16"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f16(%arg0: tensor<f16>, %arg1: tensor<f16>) -> tensor<f16> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f16_v1>, !vhlo.tensor_v1<!vhlo.f16_v1>) -> !vhlo.tensor_v1<!vhlo.f16_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f16>, tensor<f16>) -> tensor<f16>
+  func.return %0 : tensor<f16>
+}
+
+// CHECK-LABEL: "type_f32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f32(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: "type_f64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_f64(%arg0: tensor<f64>, %arg1: tensor<f64>) -> tensor<f64> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.f64_v1>, !vhlo.tensor_v1<!vhlo.f64_v1>) -> !vhlo.tensor_v1<!vhlo.f64_v1>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f64>, tensor<f64>) -> tensor<f64>
+  func.return %0 : tensor<f64>
+}
+
+// CHECK-LABEL: "type_complex_f32"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_complex_f32(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>) -> tensor<complex<f32>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>, !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>) -> !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f32_v1>>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<complex<f32>>, tensor<complex<f32>>) -> tensor<complex<f32>>
+  func.return %0 : tensor<complex<f32>>
+}
+
+// CHECK-LABEL: "type_complex_f64"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_complex_f64(%arg0: tensor<complex<f64>>, %arg1: tensor<complex<f64>>) -> tensor<complex<f64>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f64_v1>>, !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f64_v1>>) -> !vhlo.tensor_v1<!vhlo.complex_v1<!vhlo.f64_v1>>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<complex<f64>>, tensor<complex<f64>>) -> tensor<complex<f64>>
+  func.return %0 : tensor<complex<f64>>
+}
+
+// CHECK-LABEL: "type_tf32"
+// CHECK: #vhlo.type_v1<!vhlo.tf31_v1>
+func.func @type_tf32() attributes {stablehlo.attr = tf32 } {
+  return
+}
+
+// CHECK-LABEL: "type_none"
+// CHECK: #vhlo.type_v1<!vhlo.none_v1>
+func.func @type_none() attributes {stablehlo.attr = none } {
+  return
+}
+
+// CHECK-LABEL: "type_dynamism_ranked"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_dynamism_ranked(%arg0: tensor<?xf32>) -> tensor<?xf32> {
+  // CHECK: "vhlo.abs_v1"(%[[ARG0]]) : (!vhlo.tensor_v1<?x!vhlo.f32_v1>) -> !vhlo.tensor_v1<?x!vhlo.f32_v1>
+  %0 = "stablehlo.abs"(%arg0) : (tensor<?xf32>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// CHECK-LABEL: "type_per_tensor_quantization"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @type_per_tensor_quantization(%arg0: tensor<!quant.uniform<i8:f32, 34.0:16>>, %arg1: tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<!quant.uniform<i8:f32, 34.0:16>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG1]]) : (!vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>, !vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>) -> !vhlo.tensor_v1<!vhlo.quant_v1<!vhlo.i8_v1:!vhlo.f32_v1, 3.400000e+01:16, -128:127, 1>>
+  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<!quant.uniform<i8:f32, 34.0:16>>, tensor<!quant.uniform<i8:f32, 34.0:16>>) -> tensor<!quant.uniform<i8:f32, 34.0:16>>
+  func.return %0 : tensor<!quant.uniform<i8:f32, 34.0:16>>
+}
+
+// CHECK-LABEL: "type_per_axis_quantization"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_per_axis_quantization(%arg0: tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>>) -> tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>> {
+  // CHECK: "vhlo.add_v1"(%[[ARG0]], %[[ARG0]]) : (!vhlo.tensor_v1<2x!vhlo.quant_per_axis_v1<!vhlo.i8_v1:!vhlo.f32_v1, 0, [3.400000e+01, 3.400000e+01], [16, 16], -128:127, 1>>, !vhlo.tensor_v1<2x!vhlo.quant_per_axis_v1<!vhlo.i8_v1:!vhlo.f32_v1, 0, [3.400000e+01, 3.400000e+01], [16, 16], -128:127, 1>>) -> !vhlo.tensor_v1<2x!vhlo.quant_per_axis_v1<!vhlo.i8_v1:!vhlo.f32_v1, 0, [3.400000e+01, 3.400000e+01], [16, 16], -128:127, 1>>
+  %0 = stablehlo.add %arg0, %arg0 : tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>>
+  func.return %0 : tensor<2x!quant.uniform<i8:f32:0, {34.0:16, 34.0:16}>>
+}
+
+//       CHECK: function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.token_v1) -> !vhlo.token_v1>>
+// CHECK-LABEL: "type_token_callee"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_token_callee(%arg0: !stablehlo.token) -> !stablehlo.token {
+  // CHECK: "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.token_v1) -> ()
+  return %arg0 : !stablehlo.token
+}
+
+//       CHECK: function_type = #vhlo.type_v1<!vhlo.func_v1<(!vhlo.token_v1) -> !vhlo.token_v1>>
+// CHECK-LABEL: "type_token_caller"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_token_caller(%arg0: !stablehlo.token) -> !stablehlo.token {
+  // CHECK:      "vhlo.call_v1"(%[[ARG0]]) <{callee = #vhlo.string_v1<"type_token_callee">}
+  // CHECK-SAME: (!vhlo.token_v1) -> !vhlo.token_v1
+  %0 = func.call @type_token_callee(%arg0) : (!stablehlo.token) -> !stablehlo.token
+  return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: "type_tuple"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @type_tuple(%arg0: tuple<tensor<f32>>) -> tuple<!stablehlo.token> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "foo"
+  // CHECK: (!vhlo.tuple_v1<!vhlo.tensor_v1<!vhlo.f32_v1>>) -> !vhlo.tuple_v1<!vhlo.token_v1>
+  } : (tuple<tensor<f32>>) -> tuple<!stablehlo.token>
+  return %0 : tuple<!stablehlo.token>
+}
+
+// ============ DEPENDENCIES  ============
+
+func.func @composite_target(%arg0: tensor<f32>) -> tensor<f32> {
+  return %arg0: tensor<f32>
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
@@ -800,16 +800,18 @@
   func.return %0 : !stablehlo.token
 }
 
-// CHECK-LABEL: "default_recv"
-// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
-func.func @default_recv(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
-  //      CHECK: "vhlo.recv_v1"(%[[ARG0]]) <{
+// CHECK-LABEL: "op_recv_with_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_recv_with_source_target_pairs(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //      CHECK: "vhlo.recv_v2"(%[[ARG0]]) <{
   // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
   // CHECK-SAME:   channel_type = #vhlo.integer_v1<1 : i64>,
-  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>
-  // CHECK-SAME: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
   %0:2 = "stablehlo.recv"(%arg0) {
-    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+    source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
   } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
   func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
 }
@@ -817,13 +819,15 @@
 // CHECK-LABEL: "default_send"
 // CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
 func.func @default_send(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
-  //      CHECK: "vhlo.send_v1"(%[[ARG0]], %[[ARG1]]) <{
+  //      CHECK: "vhlo.send_v2"(%[[ARG0]], %[[ARG1]]) <{
   // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
   // CHECK-SAME:   channel_type = #vhlo.integer_v1<1 : i64>,
-  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>
-  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<false>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
   %0 = "stablehlo.send"(%arg0, %arg1) {
-    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,
+    source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>
   } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
   func.return %0 : !stablehlo.token
 }
@@ -1988,14 +1992,15 @@
   func.return %0 : tensor<f32>
 }
 
-// CHECK-LABEL: "op_recv"
-// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
-func.func @op_recv(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
-  //      CHECK: "vhlo.recv_v1"(%[[ARG0]]) <{
+// CHECK-LABEL: "op_recv_no_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}})
+func.func @op_recv_no_source_target_pairs(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  //      CHECK: "vhlo.recv_v2"(%[[ARG0]]) <{
   // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
   // CHECK-SAME:   channel_type = #vhlo.integer_v1<3 : i64>,
-  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>
-  // CHECK-SAME: }> : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<> : tensor<0xi64>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
   %0:2 = "stablehlo.recv"(%arg0) {
     channel_handle = #stablehlo.channel_handle<handle = 0, type = 3>,
     is_host_transfer = true
@@ -2408,14 +2413,15 @@
   func.return %0 : tensor<f32>
 }
 
-// CHECK-LABEL: "op_send"
-// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
-func.func @op_send(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
-  //      CHECK: "vhlo.send_v1"(%[[ARG0]], %[[ARG1]]) <{
+// CHECK-LABEL: "op_send_no_source_target_pairs"
+// CHECK-NEXT: (%[[ARG0:.*]]: {{.*}}, %[[ARG1:.*]]: {{.*}})
+func.func @op_send_no_source_target_pairs(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  //      CHECK: "vhlo.send_v2"(%[[ARG0]], %[[ARG1]]) <{
   // CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
   // CHECK-SAME:   channel_type = #vhlo.integer_v1<2 : i64>,
-  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>
-  // CHECK-SAME: }> : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+  // CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>,
+  // CHECK-SAME{LITERAL}:   source_target_pairs = #vhlo.tensor_v1<dense<> : tensor<0xi64>>
+  // CHECK-NEXT{LITERAL}: }) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
   %0 = "stablehlo.send"(%arg0, %arg1) {
     channel_handle = #stablehlo.channel_handle<handle = 0, type = 2>,
     is_host_transfer = true
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo_mixed.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo_mixed.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo_mixed.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo_mixed.mlir
@@ -5,15 +5,9 @@
 // about what constitutes a good test! The CHECK should be
 // minimized and named to reflect the test intent.
 
-// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py
-
-// The script is designed to make adding checks to
-// a test case fast, it is *not* designed to be authoritative
-// minimized and named to reflect the test intent.
-
-// RUN: stablehlo-opt %s --stablehlo-legalize-to-vhlo=allow-other-dialects | FileCheck %s
+// RUN: stablehlo-opt %s --stablehlo-legalize-to-vhlo=allow-other-dialects --mlir-print-local-scope | FileCheck %s
 // RUN: stablehlo-opt %s > %t.0
-// RUN: stablehlo-opt %s --stablehlo-legalize-to-vhlo=allow-other-dialects | stablehlo-opt --vhlo-legalize-to-stablehlo > %t.1
+// RUN: stablehlo-translate %s --serialize --target=1.0.0 --allow-other-dialects | stablehlo-translate --deserialize | stablehlo-opt > %t.1
 // RUN: diff %t.0 %t.1
 
 // CHECK-LABEL:   vhlo.func_v1 @op_other(
@@ -26,6 +20,34 @@
 func.func @op_other(%arg0: tensor<f32>) -> tensor<f32> {
   %0 = arith.addf %arg0, %arg0 : tensor<f32>
   return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL:   vhlo.func_v1 @func_attributes(
+// CHECK-SAME:                                  %[[VAL_0:[0-9]+|[a-zA-Z$._-][a-zA-Z0-9$._-]*]]: !vhlo.tensor_v1<!vhlo.f32_v1>) -> (!vhlo.tensor_v1<!vhlo.f32_v1>) {
+// CHECK:           "vhlo.return_v1"(%[[VAL_0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+// CHECK:          } {arg_attrs = #vhlo.array_v1<[]>, builtin.map = affine_map<(d0) -> (d0)>, mhlo.sharding = #vhlo.string_v1<"{replicated}">, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
+func.func @func_attributes(%arg0: tensor<f32>) -> tensor<f32> attributes {
+  mhlo.sharding = "{replicated}",
+  builtin.map = affine_map<(d0) -> (d0)>
+} {
+  return %arg0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL:   vhlo.func_v1 @func_mixed_attributes(
+// CHECK-SAME:                                        %[[VAL_0:[0-9]+|[a-zA-Z$._-][a-zA-Z0-9$._-]*]]: !vhlo.tensor_v1<!vhlo.f32_v1>) -> (!vhlo.tensor_v1<!vhlo.f32_v1>) {
+// CHECK:           "vhlo.return_v1"(%[[VAL_0]]) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
+// CHECK:         }
+// CHECK-SAME:    vhlo.mixed_array = #vhlo.array_v1<[affine_map<(d0) -> (d0)>, #vhlo.string_v1<"STR_ATTR">]>
+// CHECK-SAME:    vhlo.mixed_dict = #vhlo.dict_v1<{#vhlo.string_v1<"affine_map"> = affine_map<(d0) -> (d0)>, #vhlo.string_v1<"str_attr"> = #vhlo.string_v1<"STR_ATTR">}
+func.func @func_mixed_attributes(%arg0: tensor<f32>) -> tensor<f32> attributes {
+  vhlo.mixed_array = [affine_map<(d0) -> (d0)>, "STR_ATTR"],
+  vhlo.mixed_dict = {affine_map = affine_map<(d0) -> (d0)>, str_attr = "STR_ATTR"}
+} {
+  return %arg0 : tensor<f32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_attributes_invalid.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_attributes_invalid.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_attributes_invalid.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_attributes_invalid.mlir
@@ -1,17 +1,8 @@
 // RUN: stablehlo-opt --vhlo-to-version=target=1.9.0 -verify-diagnostics --split-input-file %s
 
-func.func @invalid_array_element() -> () attributes {
-  // expected-error @+1 {{expected array of VHLO attriutes}}
-  vhlo.attr = #vhlo.array_v1<[#stablehlo<precision DEFAULT>]>
-} {
-  return
-}
-
-// -----
-
 func.func @invalid_dict_element_value() -> () attributes {
-  // expected-error @+1 {{expected VHLO attribute}}
-  vhlo.attr = #vhlo.dict_v1<{#vhlo.string_v1<"attr1"> = 3 : i32}>
+  // expected-error @+1 {{expected VHLO key attribute}}
+  vhlo.attr = #vhlo.dict_v1<{"attr1" = #vhlo.array_v1<[]>}>
 } {
   return
 }
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_11_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_11_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_11_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade.1_11_0.mlir
@@ -0,0 +1,38 @@
+// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.11.0' %s | FileCheck %s
+
+// SendOp and RecvOp were changed in v1.11.0 to have
+// source_target_pair attribute. Ensure that serializing for 1.10.0 is valid and targets the
+// v1.10.0 opset.
+//
+// This will catch issues in op `isLegal` checks:
+//   op.minVersion() <= target <= op.maxVersion()
+
+// CHECK-LABEL: vhlo.func_v1 @send_op
+// CHECK-NEXT: "vhlo.send_v1"(%arg0, %arg1) <{
+// CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+// CHECK-SAME:   channel_type = #vhlo.integer_v1<2 : i64>,
+// CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>
+// CHECK-NEXT:   !vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1) -> !vhlo.token_v1
+func.func public @send_op(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  %0 = "stablehlo.send"(%arg0, %arg1) {
+    source_target_pairs = dense<[]> : tensor<0xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 2>,
+    is_host_transfer = true
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// CHECK-LABEL: vhlo.func_v1 @recv_op
+// CHECK-NEXT: "vhlo.recv_v1"(%arg0) <{
+// CHECK-SAME:   channel_id = #vhlo.integer_v1<0 : i64>,
+// CHECK-SAME:   channel_type = #vhlo.integer_v1<3 : i64>,
+// CHECK-SAME:   is_host_transfer = #vhlo.bool_v1<true>
+// CHECK-NEXT:   !vhlo.token_v1) -> (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.token_v1)
+func.func public @recv_op(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  %0:2 = "stablehlo.recv"(%arg0) {
+    source_target_pairs = dense<[]> : tensor<0xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 3>,
+    is_host_transfer = true
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir
@@ -0,0 +1,25 @@
+// RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.11.0' --verify-diagnostics --split-input-file %s
+
+// expected-error @-3 {{failed to convert VHLO to v1.11.0}}
+func.func public @send_op(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
+  // expected-error @+1 {{failed to legalize operation 'vhlo.send_v2' that was explicitly marked illegal}}
+  %0 = "stablehlo.send"(%arg0, %arg1) {
+    source_target_pairs = dense<[[0,1],[1,2]]> : tensor<2x2xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 2>,
+    is_host_transfer = true
+  } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
+  func.return %0 : !stablehlo.token
+}
+
+// -----
+
+// expected-error @-3 {{failed to convert VHLO to v1.11.0}}
+func.func public @recv_op(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
+  // expected-error @+1 {{failed to legalize operation 'vhlo.recv_v2' that was explicitly marked illegal}}
+  %0:2 = "stablehlo.recv"(%arg0) {
+    source_target_pairs = dense<[[0,1],[1,2]]> : tensor<2x2xi64>,
+    channel_handle = #stablehlo.channel_handle<handle = 0, type = 3>,
+    is_host_transfer = true
+  } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
+  func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
+}
diff --ruN a/stablehlo/stablehlo/tools/StablehloTranslateMain.cpp b/stablehlo/stablehlo/tools/StablehloTranslateMain.cpp
--- stablehlo/stablehlo/tools/StablehloTranslateMain.cpp
+++ stablehlo/stablehlo/tools/StablehloTranslateMain.cpp
@@ -76,6 +76,11 @@
     "target", llvm::cl::desc("Target version for serialization"),
     llvm::cl::init(""));
 
+llvm::cl::opt<bool> allowOtherDialectsOption(
+    "allow-other-dialects",
+    llvm::cl::desc("Allow mixed dialect in serialization"),
+    llvm::cl::init(false));
+
 llvm::cl::opt<std::string> argsOption(
     "args", llvm::cl::desc("Arguments to pass to the interpreter"),
     llvm::cl::init(""));
@@ -317,7 +322,8 @@
           return module.emitError("failed to strip debuginfo");
       }
 
-      return stablehlo::serializePortableArtifact(module, targetVersion, os);
+      return stablehlo::serializePortableArtifact(
+          module, targetVersion, os, allowOtherDialectsOption.getValue());
     },
     [](DialectRegistry &registry) {
       mlir::registerAllDialects(registry);
diff --ruN a/stablehlo/stablehlo/transforms/MapStablehloToVhlo.h b/stablehlo/stablehlo/transforms/MapStablehloToVhlo.h
--- stablehlo/stablehlo/transforms/MapStablehloToVhlo.h
+++ stablehlo/stablehlo/transforms/MapStablehloToVhlo.h
@@ -123,7 +123,7 @@
 MAP_STABLEHLO_TO_VHLO(PowOp, V1)
 MAP_STABLEHLO_TO_VHLO(RealDynamicSliceOp, V1)
 MAP_STABLEHLO_TO_VHLO(RealOp, V1)
-MAP_STABLEHLO_TO_VHLO(RecvOp, V1)
+MAP_STABLEHLO_TO_VHLO(RecvOp, V2)
 MAP_STABLEHLO_TO_VHLO(ReduceOp, V1)
 MAP_STABLEHLO_TO_VHLO(ReducePrecisionOp, V1)
 MAP_STABLEHLO_TO_VHLO(ReduceScatterOp, V1)
@@ -141,7 +141,7 @@
 MAP_STABLEHLO_TO_VHLO(ScatterOp, V2)
 MAP_STABLEHLO_TO_VHLO(SelectAndScatterOp, V1)
 MAP_STABLEHLO_TO_VHLO(SelectOp, V1)
-MAP_STABLEHLO_TO_VHLO(SendOp, V1)
+MAP_STABLEHLO_TO_VHLO(SendOp, V2)
 MAP_STABLEHLO_TO_VHLO(SetDimensionSizeOp, V1)
 MAP_STABLEHLO_TO_VHLO(ShiftLeftOp, V1)
 MAP_STABLEHLO_TO_VHLO(ShiftRightArithmeticOp, V1)
diff --ruN a/stablehlo/stablehlo/transforms/Passes.h b/stablehlo/stablehlo/transforms/Passes.h
--- stablehlo/stablehlo/transforms/Passes.h
+++ stablehlo/stablehlo/transforms/Passes.h
@@ -35,6 +35,7 @@
 #include "mlir/Transforms/DialectConversion.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/dialect/Version.h"
+#include "stablehlo/dialect/VhloTypes.h"
 
 namespace mlir {
 namespace stablehlo {
@@ -54,17 +55,17 @@
 // Populates StableHLO ops to VHLO ops rewriting patterns.
 void populateStablehloToVhloPatterns(MLIRContext *context,
                                      RewritePatternSet *patterns,
-                                     TypeConverter *converter);
+                                     vhlo::VhloTypeConverter *converter);
 
 // Populates VHLO ops to StableHLO ops rewriting patterns.
 void populateVhloToStablehloPatterns(MLIRContext *context,
                                      RewritePatternSet *patterns,
-                                     TypeConverter *converter);
+                                     vhlo::VhloTypeConverter *converter);
 
 // Populates VHLO downgrade rewriting patterns.
 void populateVhloToVersionPatterns(MLIRContext *context,
                                    RewritePatternSet *patterns,
-                                   TypeConverter *converter);
+                                   vhlo::VhloTypeConverter *converter);
 
 /// Collection of rewrite patterns for lowering of CHLO ops to StableHLO and
 /// Shape ops.
diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp b/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
--- stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
+++ stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
@@ -57,7 +57,7 @@
 class StablehloToVhloTypeConverter : public vhlo::VhloTypeConverter {
  public:
   StablehloToVhloTypeConverter(bool allowOtherDialects)
-      : vhlo::VhloTypeConverter() {
+      : vhlo::VhloTypeConverter(allowOtherDialects) {
     LLVM_DEBUG(
         llvm::dbgs()
         << "[StablehloToVhloTypeConverter] Creating with allowOtherDialects: "
@@ -82,7 +82,15 @@
       return vhlo::TokenV1Type::get(token.getContext());
     });
     addBuiltinToVhloConversions();
-    if (allowOtherDialects) addUnrealizedMaterializations();
+    if (allowOtherDialects) {
+      addUnrealizedMaterializations();
+
+      // Attribute conversion fallback uses the type converter.
+      addTypeAttributeConversion(
+          [](Type, Attribute attr) -> TypeConverter::AttributeConversionResult {
+            return attr;
+          });
+    }
   }
 
   Attribute convertEncoding(Attribute attr) const final {
@@ -114,7 +122,7 @@
   return vhlo::Name##Version##Attr::get(attr.getContext(), vhloValue.value())
 
 Attribute convertGeneric(Attribute stablehloAttr,
-                         const TypeConverter* typeConverter) {
+                         const vhlo::VhloTypeConverter* typeConverter) {
   LLVM_DEBUG(llvm::dbgs() << "Convert generic: " << stablehloAttr << '\n');
 
   // Handle StableHLO attributes.
@@ -241,6 +249,10 @@
     return vhlo::TypeV1Attr::get(attr.getContext(), vhloType);
   }
 
+  // Fall back to type converter for unknown attributes.
+  auto unknownAttr = typeConverter->convertUnknownAttribute(stablehloAttr);
+  if (unknownAttr) return unknownAttr;
+
   LLVM_DEBUG(llvm::dbgs() << "Failed to convert: " << stablehloAttr << '\n');
   return {};  // Failed to convert attribute.
 }
@@ -268,13 +280,15 @@
 Attribute convertBool(const ConversionPattern& pattern, int64_t stablehloDim) {
   auto stablehloType = IntegerType::get(pattern.getContext(), 1);
   auto stablehloAttr = IntegerAttr::get(stablehloType, stablehloDim);
-  return convertGeneric(stablehloAttr, pattern.getTypeConverter());
+  return convertGeneric(stablehloAttr,
+                        pattern.getTypeConverter<vhlo::VhloTypeConverter>());
 }
 
 Attribute convertInt(const ConversionPattern& pattern, int64_t stablehloDim) {
   auto stablehloType = IntegerType::get(pattern.getContext(), 64);
   auto stablehloAttr = IntegerAttr::get(stablehloType, stablehloDim);
-  return convertGeneric(stablehloAttr, pattern.getTypeConverter());
+  return convertGeneric(stablehloAttr,
+                        pattern.getTypeConverter<vhlo::VhloTypeConverter>());
 }
 
 Attribute convertInts(const ConversionPattern& pattern,
@@ -282,7 +296,8 @@
   auto stablehloType = RankedTensorType::get(
       stablehloDims.size(), IntegerType::get(pattern.getContext(), 64));
   auto stablehloAttr = DenseIntElementsAttr::get(stablehloType, stablehloDims);
-  return convertGeneric(stablehloAttr, pattern.getTypeConverter());
+  return convertGeneric(stablehloAttr,
+                        pattern.getTypeConverter<vhlo::VhloTypeConverter>());
 }
 
 Attribute convertSymbol(const ConversionPattern& pattern,
@@ -290,7 +305,7 @@
   auto stablehloSymbolAttr = dyn_cast<FlatSymbolRefAttr>(stablehloAttr);
   if (!stablehloSymbolAttr) return {};
   return convertGeneric(stablehloSymbolAttr.getAttr(),
-                        pattern.getTypeConverter());
+                        pattern.getTypeConverter<vhlo::VhloTypeConverter>());
 }
 
 SpecialResult convertChannelHandle(const ConversionPattern& pattern,
@@ -445,15 +460,15 @@
   vhloAttrs.emplace_back(
       StringAttr::get(pattern.getContext(), "lhs_precision_type"),
       convertGeneric(TypeAttr::get(attr.getLhsPrecisionType()),
-                     pattern.getTypeConverter()));
+                     pattern.getTypeConverter<vhlo::VhloTypeConverter>()));
   vhloAttrs.emplace_back(
       StringAttr::get(pattern.getContext(), "rhs_precision_type"),
       convertGeneric(TypeAttr::get(attr.getRhsPrecisionType()),
-                     pattern.getTypeConverter()));
+                     pattern.getTypeConverter<vhlo::VhloTypeConverter>()));
   vhloAttrs.emplace_back(
       StringAttr::get(pattern.getContext(), "accumulation_type"),
       convertGeneric(TypeAttr::get(attr.getAccumulationType()),
-                     pattern.getTypeConverter()));
+                     pattern.getTypeConverter<vhlo::VhloTypeConverter>()));
 
   // Components
   auto vhloLhsComponentCount = convertInt(pattern, attr.getLhsComponentCount());
@@ -712,7 +727,9 @@
   auto addDefaultAttr = [&](StringRef vhloName, Attribute stablehloAttr) {
     vhloAttrs.emplace_back(
         StringAttr::get(pattern.getContext(), vhloName),
-        convertGeneric(stablehloAttr, pattern.getTypeConverter()));
+        convertGeneric(
+            stablehloAttr,
+            pattern.template getTypeConverter<vhlo::VhloTypeConverter>()));
   };
   if constexpr (std::is_same<StablehloOpTy, func::FuncOp>::value) {
     if (!stablehloOp.getSymVisibilityAttr())
@@ -893,6 +910,8 @@
                 std::is_same<StablehloOpTy, stablehlo::SendOp>::value) {
     if (!stablehloOp.getIsHostTransferAttr())
       addDefaultAttr("is_host_transfer", builder.getBoolAttr(false));
+    if (!stablehloOp.getSourceTargetPairsAttr())
+      addDefaultAttr("source_target_pairs", builder.getDenseI64ArrayAttr({}));
   }
   if constexpr (std::is_same<StablehloOpTy, stablehlo::ReduceWindowOp>::value) {
     auto numWindowDimensions =
@@ -987,8 +1006,9 @@
         case SpecialResult::SPECIAL_FAILURE:
           return failure();
         case SpecialResult::NOT_SPECIAL:
-          auto vhloAttr = convertGeneric(stablehloAttr.getValue(),
-                                         this->getTypeConverter());
+          auto vhloAttr = convertGeneric(
+              stablehloAttr.getValue(),
+              this->template getTypeConverter<vhlo::VhloTypeConverter>());
           if (!vhloAttr) return failure();
           vhloAttrs.push_back({stablehloAttr.getName(), vhloAttr});
           break;
@@ -1075,14 +1095,14 @@
   }
 
  private:
-  std::shared_ptr<StablehloToVhloTypeConverter> converter;
+  std::shared_ptr<vhlo::VhloTypeConverter> converter;
   FrozenRewritePatternSet patterns;
   std::shared_ptr<ConversionTarget> target;
 };
 
 void populateStablehloToVhloPatterns(MLIRContext* context,
                                      RewritePatternSet* patterns,
-                                     TypeConverter* converter) {
+                                     vhlo::VhloTypeConverter* converter) {
   populateStablehloToVhloPatterns<
 #define GET_OP_LIST
 #include "stablehlo/dialect/StablehloOps.cpp.inc"
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -16,6 +16,7 @@
 
 #include <cstddef>
 #include <cstdint>
+#include <limits>
 #include <tuple>
 #include <utility>
 
@@ -1038,8 +1039,11 @@
   // Populate additional patterns for StableHLO extensions.
   state.addAdditionalPatterns(patterns);
 
+  // No float folding and fold as much as possible. Shape refinement will fail
+  // if int shape computations are unable to be folded.
   StablehloAggressiveFolderPassOptions folderOptions;
   folderOptions.optimizeFloat = false;
+  folderOptions.foldOpElementLimit = std::numeric_limits<int64_t>::max();
 
   // The folding patterns implement partial evaluation of shape computations
   // which is a critical part of implementing type refinement for ops like
diff --ruN a/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
@@ -57,7 +57,10 @@
 
 class VhloToStablehloTypeConverter : public vhlo::VhloTypeConverter {
  public:
-  VhloToStablehloTypeConverter() : vhlo::VhloTypeConverter() {
+  // Safe to allow other dialects since the StableHLO->VHLO conversion guards
+  // the legality of types / attributes.
+  VhloToStablehloTypeConverter()
+      : vhlo::VhloTypeConverter(/*allowOtherDialects=*/true) {
     addConversion([](Type type) -> Type { return type; });
     addConversion([](vhlo::TokenV1Type token) -> Type {
       LLVM_DEBUG(llvm::dbgs() << "Converting TokenType\n");
@@ -90,7 +93,7 @@
   return stablehlo::Name##Attr::get(attr.getContext(), stablehloValue.value())
 
 Attribute convertGeneric(Attribute vhloAttr,
-                         const TypeConverter* typeConverter) {
+                         const vhlo::VhloTypeConverter* typeConverter) {
   LLVM_DEBUG(llvm::dbgs() << "Converting attr " << vhloAttr);
   if (auto vhloAttrs = dyn_cast<vhlo::ArrayV1Attr>(vhloAttr)) {
     SmallVector<Attribute> stablehloAttrs;
@@ -189,6 +192,10 @@
     // All VHLO attributes must have counterparts in StableHLO.
     return {};
   }
+
+  // Fall back to type converter for unknown attributes.
+  auto unknownAttr = typeConverter->convertUnknownAttribute(vhloAttr);
+  if (unknownAttr) return unknownAttr;
 
   // This should be unreachable unless program is a mix of VHLO and other
   // dialects, e.g. due to user edits to textual assembly format.
@@ -229,7 +236,7 @@
 }
 
 LogicalResult convertTypeAttr(Attribute vhloAttr, Type& result,
-                              const TypeConverter* typeConverter) {
+                              const vhlo::VhloTypeConverter* typeConverter) {
   auto stablehloAttr = convertGeneric(vhloAttr, typeConverter);
   if (!stablehloAttr || !isa<TypeAttr>(stablehloAttr)) return failure();
   result = cast<TypeAttr>(stablehloAttr).getValue();
@@ -244,7 +251,7 @@
 }
 
 LogicalResult convertInts(Attribute vhloAttr,
-                          const TypeConverter* typeConverter,
+                          const vhlo::VhloTypeConverter* typeConverter,
                           SmallVector<int64_t>& result) {
   auto vhloTensorAttr = dyn_cast<vhlo::TensorV1Attr>(vhloAttr);
   if (!vhloTensorAttr) return failure();
@@ -256,7 +263,7 @@
 }
 
 Attribute convertSymbol(Attribute vhloAttr,
-                        const TypeConverter* typeConverter) {
+                        const vhlo::VhloTypeConverter* typeConverter) {
   auto vhloStringAttr = dyn_cast<vhlo::StringV1Attr>(vhloAttr);
   if (!vhloStringAttr) return {};
   auto stablehloStringAttr = dyn_cast_or_null<StringAttr>(
@@ -267,7 +274,7 @@
 
 template <typename OpType>
 Attribute convertChannelHandle(OpType vhloOp,
-                               const TypeConverter* typeConverter) {
+                               const vhlo::VhloTypeConverter* typeConverter) {
   int64_t channelId, channelType;
   if (failed(convertInt(vhloOp.getChannelId(), channelId)) ||
       failed(convertInt(vhloOp.getChannelType(), channelType)))
@@ -277,7 +284,7 @@
 }
 
 Attribute convertChannelId(Attribute vhloAttr,
-                           const TypeConverter* typeConverter) {
+                           const vhlo::VhloTypeConverter* typeConverter) {
   int64_t channelId;
   if (failed(convertInt(vhloAttr, channelId))) return {};
   return stablehlo::ChannelHandleAttr::get(vhloAttr.getContext(), channelId,
@@ -285,8 +292,8 @@
 }
 
 template <typename OpType>
-Attribute convertConvDimensionNumbers(OpType vhloOp,
-                                      const TypeConverter* typeConverter) {
+Attribute convertConvDimensionNumbers(
+    OpType vhloOp, const vhlo::VhloTypeConverter* typeConverter) {
   int64_t stablehloInputBatchDimension, stablehloInputFeatureDimension;
   SmallVector<int64_t> stablehloInputSpatialDimensions;
   int64_t stablehloKernelInputFeatureDimension,
@@ -323,7 +330,7 @@
 }
 
 Attribute convertCustomCallCalledComputations(
-    Attribute vhloAttr, const TypeConverter* typeConverter) {
+    Attribute vhloAttr, const vhlo::VhloTypeConverter* typeConverter) {
   if (auto vhloArrayAttr = dyn_cast<vhlo::ArrayV1Attr>(vhloAttr)) {
     SmallVector<Attribute> stablehloAttrs;
     for (auto vhloAttr : vhloArrayAttr.getValue()) {
@@ -336,8 +343,8 @@
   return {};
 }
 
-FailureOr<Attribute> convertDotAlgorithm(vhlo::DotGeneralOpV2 vhloOp,
-                                         const TypeConverter* typeConverter) {
+FailureOr<Attribute> convertDotAlgorithm(
+    vhlo::DotGeneralOpV2 vhloOp, const vhlo::VhloTypeConverter* typeConverter) {
   Type lhsPrecisionType, rhsPrecisionType, accumulationType;
   if (isNoneType(vhloOp.getLhsComponentCount())) {
     // All must be nonetype
@@ -373,8 +380,8 @@
       numPrimitiveOperations, allowImpreciseAccumulation);
 }
 
-Attribute convertDotDimensionNumbers(vhlo::DotGeneralOpV2 vhloOp,
-                                     const TypeConverter* typeConverter) {
+Attribute convertDotDimensionNumbers(
+    vhlo::DotGeneralOpV2 vhloOp, const vhlo::VhloTypeConverter* typeConverter) {
   SmallVector<int64_t> stablehloLhsBatchingDimensions,
       stablehloRhsBatchingDimensions, stablehloLhsContractingDimensions,
       stablehloRhsContractingDimensions;
@@ -394,13 +401,13 @@
 }
 
 Attribute convertFuncCallee(Attribute vhloAttr,
-                            const TypeConverter* typeConverter) {
+                            const vhlo::VhloTypeConverter* typeConverter) {
   return convertSymbol(vhloAttr, typeConverter);
 }
 
 template <typename OpType>
-Attribute convertGatherDimensionNumbers(OpType vhloOp,
-                                        const TypeConverter* typeConverter) {
+Attribute convertGatherDimensionNumbers(
+    OpType vhloOp, const vhlo::VhloTypeConverter* typeConverter) {
   SmallVector<int64_t> stablehloOffsetDims, stablehloCollapsedSliceDims,
       stablehloOperandBatchingDims, stablehloStartIndicesBatchingDims,
       stablehloStartIndexMap;
@@ -423,8 +430,8 @@
       stablehloStartIndexMap, stablehloIndexVectorDim);
 }
 
-Attribute convertScatterDimensionNumbers(vhlo::ScatterOpV2 vhloOp,
-                                         const TypeConverter* typeConverter) {
+Attribute convertScatterDimensionNumbers(
+    vhlo::ScatterOpV2 vhloOp, const vhlo::VhloTypeConverter* typeConverter) {
   SmallVector<int64_t> stablehloUpdateWindowDims, stablehloInsertedWindowDims,
       stablehloInputBatchingDims, stablehloScatterIndicesBatchingDims,
       stablehloScatterDimsToOperandDims;
@@ -463,10 +470,11 @@
                              VhloOpTy vhloOp,
                              SmallVector<NamedAttribute>& vhloAttrs,
                              SmallVector<NamedAttribute>& stablehloAttrs) {
+  auto typeConverter =
+      pattern.template getTypeConverter<vhlo::VhloTypeConverter>();
   if constexpr (std::is_same<VhloOpTy, vhlo::ConvolutionOpV1>::value ||
                 std::is_same<VhloOpTy, vhlo::DynamicConvOpV2>::value) {
-    auto stablehloAttr =
-        convertConvDimensionNumbers(vhloOp, pattern.getTypeConverter());
+    auto stablehloAttr = convertConvDimensionNumbers(vhloOp, typeConverter);
     if (!stablehloAttr) return failure();
     stablehloAttrs.emplace_back(
         StringAttr::get(pattern.getContext(), "dimension_numbers"),
@@ -480,7 +488,7 @@
   if constexpr (std::is_same<VhloOpTy, vhlo::DotGeneralOpV2>::value) {
     // Dot Dimension Numbers
     auto stablehloDotDimAttr =
-        convertDotDimensionNumbers(vhloOp, pattern.getTypeConverter());
+        convertDotDimensionNumbers(vhloOp, typeConverter);
     if (!stablehloDotDimAttr) return failure();
     stablehloAttrs.emplace_back(
         StringAttr::get(pattern.getContext(), "dot_dimension_numbers"),
@@ -489,8 +497,7 @@
                "lhs_contracting_dimensions", "rhs_contracting_dimensions");
 
     // Dot Algorithm
-    auto stablehloDotAlgorithmAttr =
-        convertDotAlgorithm(vhloOp, pattern.getTypeConverter());
+    auto stablehloDotAlgorithmAttr = convertDotAlgorithm(vhloOp, typeConverter);
     if (failed(stablehloDotAlgorithmAttr)) return failure();
     if (stablehloDotAlgorithmAttr.value())
       stablehloAttrs.emplace_back(
@@ -503,8 +510,7 @@
   }
   if constexpr (std::is_same<VhloOpTy, vhlo::DynamicGatherOpV2>::value ||
                 std::is_same<VhloOpTy, vhlo::GatherOpV2>::value) {
-    auto stablehloAttr =
-        convertGatherDimensionNumbers(vhloOp, pattern.getTypeConverter());
+    auto stablehloAttr = convertGatherDimensionNumbers(vhloOp, typeConverter);
     if (!stablehloAttr) return failure();
     stablehloAttrs.emplace_back(
         StringAttr::get(pattern.getContext(), "dimension_numbers"),
@@ -514,8 +520,7 @@
                "start_index_map", "index_vector_dim");
   }
   if constexpr (std::is_same<VhloOpTy, vhlo::ScatterOpV2>::value) {
-    auto stablehloAttr =
-        convertScatterDimensionNumbers(vhloOp, pattern.getTypeConverter());
+    auto stablehloAttr = convertScatterDimensionNumbers(vhloOp, typeConverter);
     if (!stablehloAttr) return failure();
     stablehloAttrs.emplace_back(
         StringAttr::get(pattern.getContext(), "scatter_dimension_numbers"),
@@ -524,10 +529,9 @@
                "input_batching_dims", "scatter_indices_batching_dims",
                "scatter_dims_to_operand_dims", "index_vector_dim");
   }
-  if constexpr (std::is_same<VhloOpTy, vhlo::RecvOpV1>::value ||
-                std::is_same<VhloOpTy, vhlo::SendOpV1>::value) {
-    auto stablehloAttr =
-        convertChannelHandle(vhloOp, pattern.getTypeConverter());
+  if constexpr (std::is_same<VhloOpTy, vhlo::RecvOpV2>::value ||
+                std::is_same<VhloOpTy, vhlo::SendOpV2>::value) {
+    auto stablehloAttr = convertChannelHandle(vhloOp, typeConverter);
     if (!stablehloAttr) return failure();
     stablehloAttrs.emplace_back(
         StringAttr::get(pattern.getContext(), "channel_handle"), stablehloAttr);
@@ -537,7 +541,7 @@
 }
 
 template <typename T, typename DenseArrayAttr>
-SpecialResult convertDenseArray(const TypeConverter* typeConverter,
+SpecialResult convertDenseArray(const vhlo::VhloTypeConverter* typeConverter,
                                 StringAttr vhloName, Attribute vhloAttr,
                                 SmallVector<NamedAttribute>& stablehloAttrs) {
   auto tensorAttr = dyn_cast<vhlo::TensorV1Attr>(vhloAttr);
@@ -556,15 +560,15 @@
 }
 
 SpecialResult convertDenseBoolArray(
-    const TypeConverter* typeConverter, StringAttr vhloName, Attribute vhloAttr,
-    SmallVector<NamedAttribute>& stablehloAttrs) {
+    const vhlo::VhloTypeConverter* typeConverter, StringAttr vhloName,
+    Attribute vhloAttr, SmallVector<NamedAttribute>& stablehloAttrs) {
   return convertDenseArray<bool, DenseBoolArrayAttr>(typeConverter, vhloName,
                                                      vhloAttr, stablehloAttrs);
 }
 
 SpecialResult convertDenseI64Array(
-    const TypeConverter* typeConverter, StringAttr vhloName, Attribute vhloAttr,
-    SmallVector<NamedAttribute>& stablehloAttrs) {
+    const vhlo::VhloTypeConverter* typeConverter, StringAttr vhloName,
+    Attribute vhloAttr, SmallVector<NamedAttribute>& stablehloAttrs) {
   return convertDenseArray<int64_t, DenseI64ArrayAttr>(
       typeConverter, vhloName, vhloAttr, stablehloAttrs);
 }
@@ -575,7 +579,8 @@
                              SmallVector<NamedAttribute>& stablehloAttrs) {
   StringAttr stablehloName = vhloName;
   Attribute stablehloAttr;
-  auto typeConverter = pattern.getTypeConverter();
+  auto typeConverter =
+      pattern.template getTypeConverter<vhlo::VhloTypeConverter>();
 
   if constexpr (std::is_same<VhloOpTy, vhlo::AllGatherOpV2>::value ||
                 std::is_same<VhloOpTy, vhlo::AllReduceOpV2>::value ||
@@ -585,7 +590,7 @@
                 std::is_same<VhloOpTy, vhlo::CollectiveBroadcastOpV1>::value) {
     if (vhloName == "channel_id") {
       stablehloName = StringAttr::get(pattern.getContext(), "channel_handle");
-      stablehloAttr = convertChannelId(vhloAttr, pattern.getTypeConverter());
+      stablehloAttr = convertChannelId(vhloAttr, typeConverter);
       if (!stablehloAttr) return specialFailure();
     }
     if (vhloName == "use_global_device_ids") {
@@ -597,20 +602,20 @@
   }
   if constexpr (std::is_same<VhloOpTy, vhlo::CustomCallOpV1>::value) {
     if (vhloName == "called_computations") {
-      stablehloAttr = convertCustomCallCalledComputations(
-          vhloAttr, pattern.getTypeConverter());
+      stablehloAttr =
+          convertCustomCallCalledComputations(vhloAttr, typeConverter);
       if (!stablehloAttr) return specialFailure();
     }
   }
   if constexpr (std::is_same<VhloOpTy, vhlo::CompositeOpV1>::value) {
     if (vhloName == "decomposition") {
-      stablehloAttr = convertSymbol(vhloAttr, pattern.getTypeConverter());
+      stablehloAttr = convertSymbol(vhloAttr, typeConverter);
       if (!stablehloAttr) return specialFailure();
     }
   }
   if constexpr (std::is_same<VhloOpTy, vhlo::CallOpV1>::value) {
     if (vhloName == "callee") {
-      stablehloAttr = convertFuncCallee(vhloAttr, pattern.getTypeConverter());
+      stablehloAttr = convertFuncCallee(vhloAttr, typeConverter);
       if (!stablehloAttr) return specialFailure();
     }
   }
@@ -760,8 +765,8 @@
 template <typename T>
 bool isSplatTensor(const ConversionPattern& pattern, Attribute vhloAttr,
                    T splatValue) {
-  auto attr = dyn_cast_or_null<DenseElementsAttr>(
-      convertGeneric(vhloAttr, pattern.getTypeConverter()));
+  auto attr = dyn_cast_or_null<DenseElementsAttr>(convertGeneric(
+      vhloAttr, pattern.getTypeConverter<vhlo::VhloTypeConverter>()));
   return attr && attr.isSplat() &&
          attr.template getSplatValue<T>() == splatValue;
 }
@@ -885,10 +890,12 @@
     if (isEmptyString(vhloOp.getOutfeedConfig()))
       eraseAttrs(vhloAttrs, "outfeed_config");
   }
-  if constexpr (std::is_same<VhloOpTy, vhlo::RecvOpV1>::value ||
-                std::is_same<VhloOpTy, vhlo::SendOpV1>::value) {
+  if constexpr (std::is_same<VhloOpTy, vhlo::RecvOpV2>::value ||
+                std::is_same<VhloOpTy, vhlo::SendOpV2>::value) {
     if (isBoolean(vhloOp.getIsHostTransferAttr(), false))
       eraseAttrs(vhloAttrs, "is_host_transfer");
+    if (isEmptyTensor(vhloOp.getSourceTargetPairs()))
+      eraseAttrs(vhloAttrs, "source_target_pairs");
   }
   if constexpr (std::is_same<VhloOpTy, vhlo::ReduceWindowOpV1>::value) {
     if (isSplatTensor(pattern, vhloOp.getWindowStridesAttr(), 1ll))
@@ -977,8 +984,9 @@
         case SpecialResult::SPECIAL_FAILURE:
           return failure();
         case SpecialResult::NOT_SPECIAL:
-          auto stablehloAttr =
-              convertGeneric(vhloAttr.getValue(), this->getTypeConverter());
+          auto stablehloAttr = convertGeneric(
+              vhloAttr.getValue(),
+              this->template getTypeConverter<vhlo::VhloTypeConverter>());
           if (!stablehloAttr) return failure();
           stablehloAttrs.push_back({vhloAttr.getName(), stablehloAttr});
           break;
@@ -1056,7 +1064,7 @@
 template <typename... StablehloOpTypes>
 void populateVhloToStablehloPatterns(MLIRContext* context,
                                      RewritePatternSet* patterns,
-                                     TypeConverter* converter) {
+                                     vhlo::VhloTypeConverter* converter) {
   patterns
       ->add<VhloToStablehloOpConverter<StablehloToVhloOp<StablehloOpTypes>>...>(
           *converter, context);
@@ -1104,7 +1112,7 @@
 
 void populateVhloToStablehloPatterns(MLIRContext* context,
                                      RewritePatternSet* patterns,
-                                     TypeConverter* converter) {
+                                     vhlo::VhloTypeConverter* converter) {
   populateVhloToStablehloPatterns<
 #define GET_OP_LIST
 #include "stablehlo/dialect/StablehloOps.cpp.inc"
diff --ruN a/stablehlo/stablehlo/transforms/VhloToVersion.cpp b/stablehlo/stablehlo/transforms/VhloToVersion.cpp
--- stablehlo/stablehlo/transforms/VhloToVersion.cpp
+++ stablehlo/stablehlo/transforms/VhloToVersion.cpp
@@ -56,15 +56,23 @@
 
 // Currently there are no type-to-version conversions so this class
 // simply validates that all types are from the VHLO dialect.
-class VhloToVersionConverter : public TypeConverter {
+class VhloToVersionConverter : public VhloTypeConverter {
  public:
-  VhloToVersionConverter() : TypeConverter() {
-    addConversion([](Type type) -> Type {
-      if (llvm::isa<vhlo::VhloDialect>(type.getDialect())) return type;
-      LLVM_DEBUG(llvm::dbgs() << "Invalid type: " << type << '\n');
-      return {};
+  // Safe to allow other dialects since the StableHLO->VHLO conversion guards
+  // the legality of types / attributes.
+  VhloToVersionConverter() : VhloTypeConverter(/*allowOtherDialects=*/true) {
+    addConversion([&](Type type) -> Type {
+      if (llvm::isa<vhlo::VhloDialect>(type.getDialect())) {
+        // Once a type becomes versioned we will check the version here.
+        return type;
+      }
+
+      LLVM_DEBUG(llvm::dbgs() << "Not VHLO type, allowed: " << type << '\n');
+      return type;
     });
   }
+
+  Attribute convertEncoding(Attribute attr) const override { return attr; };
 };
 
 // Check user-specified target version. Emit error if invalid.
@@ -111,6 +119,10 @@
 LogicalResult isLegalType(Type type, const Version& targetVersion);
 
 LogicalResult isLegalAttribute(const Attribute& attr, Version targetVersion) {
+  // StableHLO->VHLO conversion guards the legality of types / attributes.
+  // Other dialect types that make it past that step are valid here.
+  if (!mlir::isa<vhlo::VhloDialect>(attr.getDialect())) return success();
+
   auto attrInterface = dyn_cast<VersionedAttrInterface>(attr);
   if (!attrInterface || !isLegalVersion(attrInterface, targetVersion)) {
     LLVM_DEBUG(llvm::dbgs() << "failed to legalize attribute " << attr
@@ -119,10 +131,11 @@
   }
 
   // Recursively check attrs if VHLO attr is a container
-  if (auto arrAttr = dyn_cast<ArrayV1Attr>(attr))
+  if (auto arrAttr = dyn_cast<ArrayV1Attr>(attr)) {
     return success(llvm::all_of(arrAttr.getValue(), [&](Attribute ele) {
       return succeeded(isLegalAttribute(ele, targetVersion));
     }));
+  }
   if (auto arrAttr = dyn_cast<DictionaryV1Attr>(attr)) {
     return success(llvm::all_of(
         arrAttr.getValue(), [&](std::pair<Attribute, Attribute> entry) {
@@ -146,6 +159,10 @@
 }
 
 LogicalResult isLegalType(Type type, const Version& targetVersion) {
+  // StableHLO->VHLO conversion guards the legality of types / attributes.
+  // Other dialect types that make it past that step are valid here.
+  if (!mlir::isa<vhlo::VhloDialect>(type.getDialect())) return success();
+
   // All valid VHLO types must have versioned type interface.
   auto typeInterface = dyn_cast<VersionedTypeInterface>(type);
   if (!typeInterface || !isLegalVersion(typeInterface, targetVersion)) {
@@ -170,10 +187,11 @@
       return failure();
     return isLegalType(ranked.getElementType(), targetVersion);
   }
-  if (auto tuple = dyn_cast<TupleV1Type>(type))
+  if (auto tuple = dyn_cast<TupleV1Type>(type)) {
     return success(llvm::all_of(tuple.getTypes(), [&](Type ele) {
       return succeeded(isLegalType(ele, targetVersion));
     }));
+  }
   if (auto quant = dyn_cast<UniformQuantizedV1Type>(type))
     return success(
         succeeded(isLegalType(quant.getStorageType(), targetVersion)) &&
@@ -213,7 +231,7 @@
 bool isLegalOperation(Operation* op, const Version& targetVersion) {
   // Validate op
   auto opInterface = dyn_cast<VersionedOpInterface>(op);
-  if (!opInterface) return false;
+  if (!opInterface) return true;
   if (!isLegalVersion(opInterface, targetVersion)) return false;
   LLVM_DEBUG(llvm::dbgs() << "Legal op version for target. " << op << '\n');
 
@@ -454,7 +472,7 @@
 namespace stablehlo {
 void populateVhloToVersionPatterns(MLIRContext* context,
                                    RewritePatternSet* patterns,
-                                   TypeConverter* converter) {
+                                   vhlo::VhloTypeConverter* converter) {
   vhlo::populateWithGenerated(*patterns);
   patterns->add<vhlo::ScatterOpV1ToV2, vhlo::ScatterOpV2ToV1>(context);
   patterns->add<vhlo::AllReduceOpV1ToV2, vhlo::AllReduceOpV2ToV1>(context);
diff --ruN a/stablehlo/stablehlo/transforms/VhloToVersionPatterns.td b/stablehlo/stablehlo/transforms/VhloToVersionPatterns.td
--- stablehlo/stablehlo/transforms/VhloToVersionPatterns.td
+++ stablehlo/stablehlo/transforms/VhloToVersionPatterns.td
@@ -21,7 +21,7 @@
 
 def VHLO_GetEmptyDims : NativeCodeCall<"getEmptyI64Tensor($_builder)">;
 
-def VHLO_EmptyDims : AttrConstraint<CPred<"isEmptyTensor($_self)">, "Empty dims">;
+def VHLO_EmptyTensor : AttrConstraint<CPred<"isEmptyTensor($_self)">, "Empty dims">;
 
 def VHLO_NoneType : AttrConstraint<CPred<"isNoneType($_self)">, "None type">;
 
@@ -53,7 +53,7 @@
       (VHLO_GatherOpV2 $operand, $start_indices, $offset_dims, $collapsed_slice_dims, (VHLO_GetEmptyDims), (VHLO_GetEmptyDims), $start_index_map, $index_vector_dim, $slice_sizes, $indices_are_sorted)>;
 
 def GatherOpDowngradeV2ToV1 :
-  Pat<(VHLO_GatherOpV2 $operand, $start_indices, $offset_dims, $collapsed_slice_dims, VHLO_EmptyDims:$operand_batching_dims, VHLO_EmptyDims:$start_indices_batching_dims, $start_index_map, $index_vector_dim, $slice_sizes, $indices_are_sorted),
+  Pat<(VHLO_GatherOpV2 $operand, $start_indices, $offset_dims, $collapsed_slice_dims, VHLO_EmptyTensor:$operand_batching_dims, VHLO_EmptyTensor:$start_indices_batching_dims, $start_index_map, $index_vector_dim, $slice_sizes, $indices_are_sorted),
       (VHLO_GatherOpV1 $operand, $start_indices, $offset_dims, $collapsed_slice_dims, $start_index_map, $index_vector_dim, $slice_sizes, $indices_are_sorted)>;
 
 def DynamicGatherOpUpgradeV1ToV2:
@@ -61,7 +61,7 @@
       (VHLO_DynamicGatherOpV2 $operand, $start_indices, $slice_sizes, $offset_dims, $collapsed_slice_dims, (VHLO_GetEmptyDims), (VHLO_GetEmptyDims), $start_index_map, $index_vector_dim, $indices_are_sorted)>;
 
 def DynamicGatherOpDowngradeV2ToV1 :
-  Pat<(VHLO_DynamicGatherOpV2 $operand, $start_indices, $slice_sizes, $offset_dims, $collapsed_slice_dims, VHLO_EmptyDims:$operand_batching_dims, VHLO_EmptyDims:$start_indices_batching_dims, $start_index_map, $index_vector_dim, $indices_are_sorted),
+  Pat<(VHLO_DynamicGatherOpV2 $operand, $start_indices, $slice_sizes, $offset_dims, $collapsed_slice_dims, VHLO_EmptyTensor:$operand_batching_dims, VHLO_EmptyTensor:$start_indices_batching_dims, $start_index_map, $index_vector_dim, $indices_are_sorted),
       (VHLO_DynamicGatherOpV1 $operand, $start_indices, $slice_sizes, $offset_dims, $collapsed_slice_dims, $start_index_map, $index_vector_dim, $indices_are_sorted)>;
 
 def AllGatherOpUpgradeV1ToV2 :
@@ -92,6 +92,23 @@
       (VHLO_DotGeneralOpV2 $lhs, $rhs, $lhs_batching_dimensions, $rhs_batching_dimensions, $lhs_contracting_dimensions, $rhs_contracting_dimensions, $precision_config,
          (VHLO_GetNoneType), (VHLO_GetNoneType), (VHLO_GetNoneType), (VHLO_GetNoneType), (VHLO_GetNoneType), (VHLO_GetNoneType), (VHLO_GetNoneType))>;
 
+def SendOpDowngradeV2ToV1 :
+  Pat<(VHLO_SendOpV2 $operand, $token, $channel_id, $channel_type, $is_host_transfer, VHLO_EmptyTensor:$source_target_pairs),
+      (VHLO_SendOpV1 $operand, $token, $channel_id, $channel_type, $is_host_transfer)>;
+
+
+def SendOpUpgradeV1ToV2 :
+  Pat<(VHLO_SendOpV1 $operand, $token, $channel_id, $channel_type, $is_host_transfer),
+      (VHLO_SendOpV2 $operand, $token, $channel_id, $channel_type, $is_host_transfer, (VHLO_GetEmptyDims))>;
+
+def RecvOpDowngradeV2ToV1 :
+  Pat<(VHLO_RecvOpV2 $token, $channel_id, $channel_type, $is_host_transfer, VHLO_EmptyTensor:$source_target_pairs),
+      (VHLO_RecvOpV1 $token, $channel_id, $channel_type, $is_host_transfer)>;
+
+def RecvOpUpgradeV1ToV2 :
+  Pat<(VHLO_RecvOpV1 $token, $channel_id, $channel_type, $is_host_transfer),
+      (VHLO_RecvOpV2 $token, $channel_id, $channel_type, $is_host_transfer, (VHLO_GetEmptyDims))>;
+
 foreach resultAccuracyOpV1V2Pair = [
   [VHLO_CbrtOpV1, VHLO_CbrtOpV2],
   [VHLO_CosineOpV1, VHLO_CosineOpV2],
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -19,6 +19,7 @@
 #include <memory>
 #include <numeric>
 #include <optional>
+#include <string>
 #include <utility>
 
 #include "llvm/ADT/APInt.h"
@@ -47,6 +48,7 @@
 #include "mlir/IR/TypeUtilities.h"
 #include "mlir/IR/Types.h"
 #include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
 #include "mlir/Interfaces/SideEffectInterfaces.h"
 #include "mlir/Pass/Pass.h"
 #include "mlir/Rewrite/FrozenRewritePatternSet.h"
@@ -98,29 +100,88 @@
   return success();
 }
 
+template <typename Fn>
+static TypedAttr foldUnaryOpIntOrFloat(Type resultType, TypedAttr operand,
+                                       Fn&& folder) {
+  Type elemTy = getElementTypeOrSelf(operand);
+
+  Attribute res;
+  if (isa<IntegerType>(elemTy))
+    res = constFoldUnaryOp<IntegerAttr, IntegerAttr::ValueType, void>(operand,
+                                                                      folder);
+  if (isa<FloatType>(elemTy))
+    res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void>(operand,
+                                                                  folder);
+  if (res) return cast<TypedAttr>(res);
+
+  return nullptr;
+}
+
 /// Binary constant folder that used a generic folder function to handle both
 /// ints and floats.
 template <typename Fn>
-static TypedAttr foldBinaryOpIntOrFloat(TypedAttr lhs, TypedAttr rhs,
-                                        Fn&& folder) {
+FailureOr<TypedAttr> foldUnaryOpIntOrFloat(PatternRewriter& rewriter,
+                                            Operation* op, Fn&& folder) {
+  if (op->getNumOperands() != 1 || op->getNumResults() != 1)
+    return rewriter.notifyMatchFailure(op, "expected unary op");
+
+  TypedAttr attr;
+  matchPattern(op->getOperand(0), m_Constant(&attr));
+
+  if (!attr) return rewriter.notifyMatchFailure(op, "operand not constants");
+
+  TypedAttr res = foldUnaryOpIntOrFloat(op->getResultTypes()[0], attr, folder);
+  if (!res) return rewriter.notifyMatchFailure(op, "folding failed");
+
+  return res;
+}
+
+/// Binary constant folder that used a generic folder function to handle both
+/// ints and floats.
+template <typename Fn>
+static TypedAttr foldBinaryOpIntOrFloat(Type resultType, TypedAttr lhs,
+                                        TypedAttr rhs, Fn&& folder) {
   Attribute operands[2] = {lhs, rhs};
   Type elemTy = getElementTypeOrSelf(lhs);
 
   Attribute res;
   if (isa<IntegerType>(elemTy))
-    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void>(operands,
-                                                                       folder);
+    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void>(
+        operands, resultType, folder);
   if (isa<FloatType>(elemTy))
-    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void>(operands,
-                                                                   folder);
+    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void>(
+        operands, resultType, folder);
   if (res) return cast<TypedAttr>(res);
 
   return nullptr;
+}
+
+
+/// Binary constant folder that used a generic folder function to handle both
+/// ints and floats.
+template <typename Fn>
+FailureOr<TypedAttr> foldBinaryOpIntOrFloat(PatternRewriter& rewriter,
+                                            Operation* op, Fn&& folder) {
+  if (op->getNumOperands() != 2 || op->getNumResults() != 1)
+    return rewriter.notifyMatchFailure(op, "expected binary op");
+
+  TypedAttr lhsAttr, rhsAttr;
+  matchPattern(op->getOperand(0), m_Constant(&lhsAttr));
+  matchPattern(op->getOperand(1), m_Constant(&rhsAttr));
+
+  if (!lhsAttr || !rhsAttr)
+    return rewriter.notifyMatchFailure(op, "lhs & rhs operands not constants");
+
+  TypedAttr res =
+      foldBinaryOpIntOrFloat(op->getResultTypes()[0], lhsAttr, rhsAttr, folder);
+  if (!res) return rewriter.notifyMatchFailure(op, "folding failed");
+
+  return res;
 }
 
 template <class AttrElementT, class TargetAttrElementT, class CalculationT,
           typename OpType>
-LogicalResult evalConvertHelper(PatternRewriter& rewriter, OpType op,
+LogicalResult foldConvertHelper(PatternRewriter& rewriter, OpType op,
                                 DenseIntOrFPElementsAttr elements, Type resType,
                                 CalculationT&& calculate) {
   auto result = constFoldCastOp<AttrElementT, TargetAttrElementT,
@@ -128,18 +189,19 @@
                                 typename TargetAttrElementT::ValueType, void>(
       elements, resType, calculate);
 
-  if (!result)
+  if (!result) {
     return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
       diag << "cast of " << elements.getElementType() << " to " << resType
            << " failed";
     });
+  }
 
   rewriter.replaceOpWithNewOp<ConstantOp>(op, result);
   return success();
 }
 
 template <typename OpType>
-LogicalResult evalConvert(PatternRewriter& rewriter, OpType op,
+LogicalResult foldConvert(PatternRewriter& rewriter, OpType op,
                           DenseIntOrFPElementsAttr elements,
                           RankedTensorType resultType) {
   auto oldType = getElementTypeOrSelf(elements);
@@ -153,7 +215,7 @@
     if (auto newFloatType = dyn_cast<FloatType>(newType)) {
       // Float -> Float
       const auto& targetSemantics = newFloatType.getFloatSemantics();
-      return evalConvertHelper<FloatAttr, FloatAttr>(
+      return foldConvertHelper<FloatAttr, FloatAttr>(
           rewriter, op, elements, resultType,
           [&targetSemantics](const APFloat& operand, bool& castStatus) {
             bool losesInfo;
@@ -167,7 +229,7 @@
     }
 
     // Float -> Int
-    return evalConvertHelper<FloatAttr, IntegerAttr>(
+    return foldConvertHelper<FloatAttr, IntegerAttr>(
         rewriter, op, elements, resultType,
         [&newBitWidth, &isNewTypeUnsigned](const APFloat& operand,
                                            bool& castStatus) {
@@ -186,7 +248,7 @@
 
   if (auto newFloatType = dyn_cast<FloatType>(newType)) {
     // Int -> Float
-    return evalConvertHelper<IntegerAttr, FloatAttr>(
+    return foldConvertHelper<IntegerAttr, FloatAttr>(
         rewriter, op, elements, resultType,
         [&newFloatType, &isOldTypeUnsigned](const APInt& operand,
                                             bool& /*castStatus*/) {
@@ -199,64 +261,12 @@
   }
 
   // Int -> Int
-  return evalConvertHelper<IntegerAttr, IntegerAttr>(
+  return foldConvertHelper<IntegerAttr, IntegerAttr>(
       rewriter, op, elements, resultType,
       [&newBitWidth, &isOldTypeUnsigned](const APInt& operand,
                                          bool& /*castStatus*/) {
         return APSInt(operand, isOldTypeUnsigned).extOrTrunc(newBitWidth);
       });
-}
-
-// The patterns below implement partial evaluation of shape computations which
-// is a critical part of implementing type refinement for ops like
-// dynamic_broadcast_in_dim, dynamic_iota and dynamic_reshape whose shape
-// depends on the value of their shape operands.
-
-template <typename OpType, typename FuncType>
-LogicalResult evalElementwise(PatternRewriter& rewriter, OpType op,
-                              FuncType fn) {
-  auto resultType = op.getType();
-  if (failed(validateStaticShapeResult(rewriter, op, resultType)))
-    return failure();
-
-  if (!isa<IntegerType>(resultType.getElementType()))
-    return rewriter.notifyMatchFailure(op,
-                                       "expected integer result tensor type");
-
-  SmallVector<APSInt> result;
-  if constexpr (OpType::template hasTrait<OpTrait::OneOperand>()) {
-    SmallVector<APSInt> operand;
-    if (failed(hlo::matchInts(op.getOperand(), operand)))
-      return rewriter.notifyMatchFailure(op, "expected constant operand");
-    for (const auto& operandEl : operand) {
-      result.push_back(fn(operandEl));
-    }
-  } else if constexpr (OpType::template hasTrait<
-                           OpTrait::NOperands<2>::Impl>()) {
-    SmallVector<APSInt> lhs, rhs;
-    if (failed(hlo::matchInts(op.getLhs(), lhs)) ||
-        failed(hlo::matchInts(op.getRhs(), rhs)))
-      return rewriter.notifyMatchFailure(op, "expected constant operands");
-    for (auto [lhsEl, rhsEl] : llvm::zip(lhs, rhs)) {
-      result.push_back(fn(lhsEl, rhsEl));
-    }
-  } else if constexpr (OpType::template hasTrait<
-                           OpTrait::NOperands<3>::Impl>()) {
-    SmallVector<APSInt> x, y, z;
-    if (failed(hlo::matchInts(op->getOperand(0), x)) ||
-        failed(hlo::matchInts(op->getOperand(1), y)) ||
-        failed(hlo::matchInts(op->getOperand(2), z)))
-      return rewriter.notifyMatchFailure(op, "expected constant operands");
-    for (auto [xEl, yEl, zEl] : llvm::zip(x, y, z)) {
-      result.push_back(fn(xEl, yEl, zEl));
-    }
-  } else {
-    llvm::report_fatal_error("unsupported number of operands");
-  }
-
-  rewriter.replaceOpWithNewOp<ConstantOp>(op,
-                                          getTensorAttr(resultType, result));
-  return success();
 }
 
 template <typename OpType>
@@ -275,29 +285,18 @@
                        ArrayRef<StringRef> generatedNames = {}) = delete;
 
   const StablehloAggressiveFolderPassOptions& options;
-};
-
-struct FoldAddOpPattern final : FoldOpRewritePattern<mlir::stablehlo::AddOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(mlir::stablehlo::AddOp op,
-                                PatternRewriter& rewriter) const override {
-    Value lhs = op.getLhs();
-    Value rhs = op.getRhs();
-
-    // Pattern: add(cst,cst) -> cst
-    TypedAttr lhsAttr, rhsAttr;
-    matchPattern(lhs, m_Constant(&lhsAttr));
-    matchPattern(rhs, m_Constant(&rhsAttr));
-
-    if (TypedAttr res;
-        lhsAttr && rhsAttr &&
-        (res = foldBinaryOpIntOrFloat(lhsAttr, rhsAttr, std::plus<>{}))) {
-      rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res);
-      return success();
-    }
-
-    return failure();
+
+  LogicalResult validateElementCountForFold(PatternRewriter& rewriter,
+                                            Operation* op,
+                                            ShapedType resultType) const {
+    size_t numElems = resultType.getNumElements();
+    if (numElems > static_cast<size_t>(options.foldOpElementLimit))
+      return rewriter.notifyMatchFailure(
+          op,
+          "too many elements, fold "
+          "limit is " +
+              std::to_string(options.foldOpElementLimit));
+    return success();
   }
 };
 
@@ -318,100 +317,102 @@
   }
 };
 
-struct EvalAddOpShapePattern : public FoldOpRewritePattern<AddOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(AddOp op,
-                                PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op,
-                           [&](APSInt lhs, APSInt rhs) { return lhs + rhs; });
-  }
-};
-
-struct EvalAndOpPattern : public FoldOpRewritePattern<AndOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(AndOp op,
-                                PatternRewriter& rewriter) const override {
+struct FoldAddOpPattern final
+    : public ShapeOpRewritePattern<mlir::stablehlo::AddOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
+
+  LogicalResult matchAndRewrite(mlir::stablehlo::AddOp op,
+                                PatternRewriter& rewriter) const override {
+    if (failed(validateShapeFoldDtype(rewriter, op, op.getType())))
+      return failure();
+
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, std::plus<>{});
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+};
+
+struct FoldAndOpPattern : public ShapeOpRewritePattern<AndOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
+
+  LogicalResult matchAndRewrite(mlir::stablehlo::AndOp op,
+                                PatternRewriter& rewriter) const override {
+    // TODO: Support more int types
     auto resultType = op.getType();
     if (!resultType.getElementType().isInteger(1))
       return rewriter.notifyMatchFailure(op, "expected boolean element type");
 
-    return evalElementwise(rewriter, op, [&](APSInt lhsInt, APSInt rhsInt) {
-      return getAPSInt(resultType.getElementType(), lhsInt != 0 && rhsInt != 0);
-    });
-  }
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldAnd{});
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+
+  struct FoldAnd {
+    APInt operator()(APInt lhs, APInt rhs) const {
+      return APInt(lhs.getBitWidth(), !lhs.isZero() && !rhs.isZero());
+    }
+    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {
+      return std::nullopt;
+    }
+  };
 };
 
 // Pattern: broadcast_in_dim(splat, _) -> constant(splat)
-struct FoldBroadcastInDimSplatPattern final
-    : FoldOpRewritePattern<mlir::stablehlo::BroadcastInDimOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(mlir::stablehlo::BroadcastInDimOp op,
-                                PatternRewriter& rewriter) const override {
-    TypedValue<RankedTensorType> operand = op.getOperand();
-
-    if (SplatElementsAttr cstAttr;
-        matchPattern(operand, m_Constant(&cstAttr))) {
-      rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(
-          op, SplatElementsAttr::get(op.getType(),
-                                     cstAttr.getSplatValue<Attribute>()));
-      return success();
-    }
-    return failure();
-  }
-};
-
-struct EvalBroadcastInDimOpPattern
-    : public FoldOpRewritePattern<BroadcastInDimOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+struct FoldBroadcastInDimOpSplatPattern
+    : public ShapeOpRewritePattern<BroadcastInDimOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(BroadcastInDimOp op,
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
-    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
-      return failure();
-
-    auto operandType = op.getOperand().getType();
-    if (operandType.getRank() != 0)
-      return rewriter.notifyMatchFailure(op, "expected 0-dimensional type");
-
-    SmallVector<APSInt> operand;
-    if (failed(hlo::matchInts(op.getOperand(), operand)))
-      return rewriter.notifyMatchFailure(op, "expected constant operands");
-    auto scalar = operand[0];
-
-    rewriter.replaceOpWithNewOp<ConstantOp>(
-        op, getTensorAttr(op.getType(), scalar));
-    return success();
-  }
-};
-
-struct EvalClampOpPattern : public FoldOpRewritePattern<ClampOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(ClampOp op,
-                                PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op,
-                           [&](APSInt min, APSInt operand, APSInt max) {
-                             if (operand < min) return min;
-                             if (max < operand) return max;
-                             return operand;
-                           });
-  }
-};
-
-struct EvalCompareOpPattern : public FoldOpRewritePattern<CompareOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
+        failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    SplatElementsAttr cstAttr;
+    matchPattern(op.getOperand(), m_Constant(&cstAttr));
+    if (!cstAttr) return rewriter.notifyMatchFailure(op, "operand not splat");
+
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(
+        op, SplatElementsAttr::get(op.getType(),
+                                   cstAttr.getSplatValue<Attribute>()));
+    return success();
+  }
+};
+
+struct FoldCompareOpPattern : public ShapeOpRewritePattern<CompareOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(CompareOp op,
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
-    auto kind = op.getCompareType();
-    return evalElementwise(rewriter, op, [&](APInt lhs, APInt rhs) {
+    if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    auto res = foldBinaryOpIntOrFloat(
+        rewriter, op,
+        FoldCompare(op.getComparisonDirection(), op.getCompareType()));
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+
+  struct FoldCompare {
+    FoldCompare(ComparisonDirection direction,
+                std::optional<ComparisonType> kind)
+        : direction(direction), kind(kind) {}
+    ComparisonDirection direction;
+    std::optional<ComparisonType> kind;
+
+    // TODO: Enable float folding.
+    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
+      return std::nullopt;
+    }
+    APInt operator()(APInt lhs, APInt rhs) {
       bool result = false;
-      switch (op.getComparisonDirection()) {
+      switch (direction) {
         case ComparisonDirection::EQ:
           result = lhs == rhs;
           break;
@@ -431,9 +432,9 @@
           result = kind == ComparisonType::SIGNED ? lhs.slt(rhs) : lhs.ult(rhs);
           break;
       }
-      return getAPSInt(resultType.getElementType(), result);
-    });
-  }
+      return APInt(/*bitwidth=*/1, result);
+    }
+  };
 };
 
 //////////////////////////////////
@@ -441,16 +442,15 @@
 /////////////////////////////////
 
 struct FoldConcatenateOpPattern final
-    : FoldOpRewritePattern<mlir::stablehlo::ConcatenateOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    : ShapeOpRewritePattern<mlir::stablehlo::ConcatenateOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(mlir::stablehlo::ConcatenateOp op,
                                 PatternRewriter& rewriter) const override {
     RankedTensorType type = op.getType();
-    if (!type.hasStaticShape()) return failure();
-
-    size_t numElems = type.getNumElements();
-    if (numElems > static_cast<size_t>(options.foldOpElementLimit))
+    if (failed(validateStaticShapeResult(rewriter, op, type)) ||
+        failed(validateShapeFoldDtype(rewriter, op, type)) ||
+        failed(validateElementCountForFold(rewriter, op, type)))
       return failure();
 
     // Fold concatenate when all inputs are constants.
@@ -466,6 +466,7 @@
                                       int64_t{1}, std::multiplies<>{});
 
     SmallVector<Attribute> newElems;
+    size_t numElems = type.getNumElements();
     newElems.reserve(numElems);
 
     for (int64_t i = 0; i != topSize; ++i) {
@@ -485,31 +486,7 @@
   int64_t foldOpElementLimit;
 };
 
-struct EvalConcatenateOpPattern : public FoldOpRewritePattern<ConcatenateOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(ConcatenateOp op,
-                                PatternRewriter& rewriter) const override {
-    auto resultType = op.getType();
-    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
-      return failure();
-
-    if (op.getDimension() != 0)
-      return rewriter.notifyMatchFailure(op, "expected dimension = 0");
-
-    SmallVector<APSInt> result;
-    for (Value operand : op->getOperands()) {
-      if (failed(hlo::matchInts(operand, result)))
-        return rewriter.notifyMatchFailure(op, "expected constant operands");
-    }
-
-    rewriter.replaceOpWithNewOp<ConstantOp>(op,
-                                            getTensorAttr(resultType, result));
-    return success();
-  }
-};
-
-struct EvalConvertOpPattern : public ShapeOpRewritePattern<ConvertOp> {
+struct FoldConvertOpPattern : public ShapeOpRewritePattern<ConvertOp> {
   using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(ConvertOp op,
@@ -532,28 +509,50 @@
       return rewriter.notifyMatchFailure(
           op, "expected constant integer or float operand");
 
-    return evalConvert(rewriter, op, elements, resultType);
-  }
-};
-
-struct EvalDivOpPattern : public FoldOpRewritePattern<DivOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    return foldConvert(rewriter, op, elements, resultType);
+  }
+};
+
+struct FoldDivOpPattern : public ShapeOpRewritePattern<DivOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(DivOp op,
                                 PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op,
-                           [&](APSInt lhs, APSInt rhs) { return lhs / rhs; });
-  }
-};
-
-struct EvalGetDimensionSizeOpPattern
-    : public FoldOpRewritePattern<GetDimensionSizeOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    auto resultType = op.getType();
+    if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldDivide(isUnsignedInt));
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+
+  struct FoldDivide {
+    FoldDivide(bool isUnsignedInt)
+        : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
+    std::function<APInt(APInt, APInt)> foldIntFn;
+
+    // TODO: Enable float folding.
+    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
+      return std::nullopt;  // return lhs / rhs;
+    }
+    APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
+    static APInt foldUint(APInt lhs, APInt rhs) { return lhs.udiv(rhs); }
+    static APInt foldSint(APInt lhs, APInt rhs) { return lhs.sdiv(rhs); }
+  };
+};
+
+struct FoldGetDimensionSizeOpPattern
+    : public ShapeOpRewritePattern<GetDimensionSizeOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(GetDimensionSizeOp op,
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
-    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
+    if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
+        failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
     auto operandType = op.getOperand().getType();
@@ -567,86 +566,187 @@
   }
 };
 
-struct EvalMaxOpPattern : public FoldOpRewritePattern<MaxOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+/////
+// Max/Min/Clamp
+/////
+
+struct FoldMax {
+  FoldMax(bool isUnsignedInt)
+      : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
+  std::function<APInt(APInt, APInt)> foldIntFn;
+
+  // TODO: Enable float folding.
+  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
+    return std::nullopt;  // return lhs >= rhs ? lhs : rhs;
+  }
+  APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
+  static APInt foldUint(APInt lhs, APInt rhs) {
+    return lhs.uge(rhs) ? lhs : rhs;
+  }
+  static APInt foldSint(APInt lhs, APInt rhs) {
+    return lhs.sge(rhs) ? lhs : rhs;
+  }
+};
+
+struct FoldMin {
+  FoldMin(bool isUnsignedInt)
+      : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
+  std::function<APInt(APInt, APInt)> foldIntFn;
+
+  // TODO: Enable float folding.
+  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
+    return std::nullopt;  // return lhs <= rhs ? lhs : rhs;
+  }
+  APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
+  static APInt foldUint(APInt lhs, APInt rhs) {
+    return lhs.ule(rhs) ? lhs : rhs;
+  }
+  static APInt foldSint(APInt lhs, APInt rhs) {
+    return lhs.sle(rhs) ? lhs : rhs;
+  }
+};
+
+
+struct FoldMaxOpPattern : public ShapeOpRewritePattern<MaxOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(MaxOp op,
                                 PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op, [&](APSInt lhs, APSInt rhs) {
-      return lhs >= rhs ? lhs : rhs;
-    });
-  }
-};
-
-struct EvalMinOpPattern : public FoldOpRewritePattern<MinOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    auto resultType = op.getType();
+    if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMax(isUnsignedInt));
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+};
+
+struct FoldMinOpPattern : public ShapeOpRewritePattern<MinOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(MinOp op,
                                 PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op, [&](APSInt lhs, APSInt rhs) {
-      return lhs <= rhs ? lhs : rhs;
-    });
-  }
-};
-
-struct FoldMulOpPattern final : FoldOpRewritePattern<mlir::stablehlo::MulOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    auto resultType = op.getType();
+    if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMin(isUnsignedInt));
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+};
+
+// Clamp is folded using Min and Max folders.
+struct FoldClampOpPattern : public ShapeOpRewritePattern<ClampOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
+
+  LogicalResult matchAndRewrite(ClampOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    TypedAttr minAttr, operandAttr, maxAttr;
+    matchPattern(op.getMin(), m_Constant(&minAttr));
+    matchPattern(op.getOperand(), m_Constant(&operandAttr));
+    matchPattern(op.getMax(), m_Constant(&maxAttr));
+
+    if (!minAttr || !operandAttr || !maxAttr)
+      return rewriter.notifyMatchFailure(op, "operands not constant");
+
+    // Fold clamp using:
+    //   res = max(min, operand)
+    //   res = min(max, res)
+    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();
+    auto res = foldBinaryOpIntOrFloat(resultType, minAttr, operandAttr,
+                                      FoldMax(isUnsignedInt));
+    res = foldBinaryOpIntOrFloat(resultType, maxAttr, res,
+                                 FoldMin(isUnsignedInt));
+    if (!res) return rewriter.notifyMatchFailure(op, "failed to fold clamp");
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res);
+    return success();
+  }
+};
+
+struct FoldMulOpPattern final : ShapeOpRewritePattern<mlir::stablehlo::MulOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(mlir::stablehlo::MulOp op,
                                 PatternRewriter& rewriter) const override {
-    TypedAttr lhsAttr;
-    matchPattern(op.getLhs(), m_Constant(&lhsAttr));
-
-    TypedAttr rhsAttr;
-    matchPattern(op.getRhs(), m_Constant(&rhsAttr));
-
-    if (TypedAttr res;
-        lhsAttr && rhsAttr &&
-        (res = foldBinaryOpIntOrFloat(lhsAttr, rhsAttr, std::multiplies<>{}))) {
-      rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res);
-      return success();
-    }
-
-    return failure();
-  }
-};
-
-struct EvalMulOpPattern : public FoldOpRewritePattern<MulOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(MulOp op,
-                                PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op,
-                           [&](APSInt lhs, APSInt rhs) { return lhs * rhs; });
-  }
-};
-
-struct EvalOrOpPattern : public FoldOpRewritePattern<OrOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    if (failed(validateShapeFoldDtype(rewriter, op, op.getType())))
+      return failure();
+
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, std::multiplies<>{});
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+};
+
+struct FoldOrOpPattern : public ShapeOpRewritePattern<OrOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(OrOp op,
                                 PatternRewriter& rewriter) const override {
+    // TODO: Support more int types
     auto resultType = op.getType();
     if (!resultType.getElementType().isInteger(1))
       return rewriter.notifyMatchFailure(op, "expected boolean element type");
 
-    return evalElementwise(rewriter, op, [&](APSInt lhsInt, APSInt rhsInt) {
-      return getAPSInt(resultType.getElementType(), lhsInt != 0 || rhsInt != 0);
-    });
-  }
-};
-
-struct EvalRemOpPattern : public FoldOpRewritePattern<RemOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldOr{});
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+
+  struct FoldOr {
+    APInt operator()(APInt lhs, APInt rhs) const {
+      return APInt(lhs.getBitWidth(), !lhs.isZero() || !rhs.isZero());
+    }
+    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {
+      return std::nullopt;
+    }
+  };
+};
+
+struct FoldRemOpPattern : public ShapeOpRewritePattern<RemOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(RemOp op,
                                 PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op,
-                           [&](APSInt lhs, APSInt rhs) { return lhs % rhs; });
-  }
-};
-
-struct EvalReshapeOpPattern : public ShapeOpRewritePattern<ReshapeOp> {
+    auto resultType = op.getType();
+    if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldRem(isUnsignedInt));
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+
+  struct FoldRem {
+    FoldRem(bool isUnsignedInt)
+        : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
+    std::function<APInt(APInt, APInt)> foldIntFn;
+
+    // TODO: Enable float folding.
+    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
+      return std::nullopt;  // return lhs.remainder(rhs);
+    }
+    APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
+    static APInt foldUint(APInt lhs, APInt rhs) { return lhs.urem(rhs); }
+    static APInt foldSint(APInt lhs, APInt rhs) { return lhs.srem(rhs); }
+  };
+};
+
+// Pattern: reshape(cst, shape) -> cst
+struct FoldReshapeOpPattern : public ShapeOpRewritePattern<ReshapeOp> {
   using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(ReshapeOp op,
@@ -656,7 +756,6 @@
         failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
-    // Pattern: reshape(cst, shape) -> cst
     DenseIntOrFPElementsAttr attr;
     if (!matchPattern(op.getOperand(), m_Constant(&attr)))
       return rewriter.notifyMatchFailure(op, "expected constant operand");
@@ -665,53 +764,98 @@
   }
 };
 
-struct EvalSelectOpPattern : public FoldOpRewritePattern<SelectOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+struct FoldSelectOpPattern : public ShapeOpRewritePattern<SelectOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(SelectOp op,
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
-    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
-      return failure();
-
-    SmallVector<APSInt> pred, onTrue, onFalse;
-    if (failed(hlo::matchInts(op.getPred(), pred)) ||
-        failed(hlo::matchInts(op.getOnTrue(), onTrue)) ||
-        failed(hlo::matchInts(op.getOnFalse(), onFalse)))
+    if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
+        failed(validateShapeFoldDtype(rewriter, op, resultType)))
+      return failure();
+
+    DenseIntElementsAttr predAttr;
+    DenseElementsAttr onTrueAttr, onFalseAttr;
+    matchPattern(op.getPred(), m_Constant(&predAttr));
+    matchPattern(op.getOnTrue(), m_Constant(&onTrueAttr));
+    matchPattern(op.getOnFalse(), m_Constant(&onFalseAttr));
+    if (!predAttr || !onTrueAttr || !onFalseAttr)
       return rewriter.notifyMatchFailure(op, "expected constant operands");
 
-    SmallVector<APSInt> result;
+    // Optimization, handle splat predicate
+    if (isa<SplatElementsAttr>(predAttr)) {
+      auto pred = predAttr.getSplatValue<APInt>();
+      rewriter.replaceOpWithNewOp<ConstantOp>(
+          op, pred.isZero() ? onFalseAttr : onTrueAttr);
+      return success();
+    }
+
+    // TODO: Enable float folding.
+    if (op.getType().getElementType().isFloat())
+      return rewriter.notifyMatchFailure(op, "float select not supported yet");
+
+    // Fall back to verbose folding
+    if (failed(validateElementCountForFold(rewriter, op, resultType)))
+      return failure();
+
+    SmallVector<APInt> result;
     for (auto [predEl, onTrueEl, onFalseEl] :
-         llvm::zip(pred, onTrue, onFalse)) {
-      result.push_back(predEl != 0 ? onTrueEl : onFalseEl);
-    }
-
+         llvm::zip(predAttr.getValues<APInt>(), onTrueAttr.getValues<APInt>(),
+                   onFalseAttr.getValues<APInt>())) {
+      result.push_back(!predEl.isZero() ? onTrueEl : onFalseEl);
+    }
     rewriter.replaceOpWithNewOp<ConstantOp>(
-        op, getTensorAttr(op.getType(), result));
-    return success();
-  }
-};
-
-struct EvalSignOpPattern : public FoldOpRewritePattern<SignOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+        op, DenseIntElementsAttr::get(resultType, result));
+
+    return success();
+  }
+
+  struct FoldSelect {
+    std::optional<APFloat> operator()(APFloat pred, APFloat onTrue,
+                                      APFloat onFalse) {
+      return std::nullopt;
+    }
+
+    APInt operator()(APInt pred, APInt onTrue, APInt onFalse) {
+      return pred != 0 ? onTrue : onFalse;
+    }
+  };
+};
+
+struct FoldSignOpPattern : public ShapeOpRewritePattern<SignOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(SignOp op,
                                 PatternRewriter& rewriter) const override {
-    auto resultType = op.getType();
-    if (!isa<IntegerType>(resultType.getElementType()))
-      return rewriter.notifyMatchFailure(op,
-                                         "expected integer result tensor type");
-    return evalElementwise(rewriter, op, [&](APSInt operand) {
+    if (failed(validateShapeFoldDtype(rewriter, op, op.getType())))
+      return failure();
+
+    auto elementType = op.getType().getElementType();
+    auto res = foldUnaryOpIntOrFloat(rewriter, op, FoldSign(elementType));
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+
+  struct FoldSign {
+    FoldSign(Type elementType) : elementType(elementType) {}
+    Type elementType;
+    // TODO: Enable float folding.
+    std::optional<APFloat> operator()(APFloat operand) { return std::nullopt; }
+
+    APInt operator()(APInt operand) {
+      // SignOp only supports signed integers.
+      APSInt signedInt = getAPSInt(elementType, operand.getSExtValue());
       int64_t result;
-      if (operand.isNegative())
+      if (signedInt.isNegative())
         result = -1;
-      else if (operand.isZero())
+      else if (signedInt.isZero())
         result = 0;
       else
         result = 1;
-      return getAPSInt(resultType.getElementType(), result);
-    });
-  }
+      return getAPSInt(elementType, result);
+    }
+  };
 };
 
 template <typename RangeType>
@@ -749,13 +893,14 @@
                                 ArrayRef<ElementType>(result));
 }
 
-struct EvalSliceOpPattern : public FoldOpRewritePattern<SliceOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+struct FoldSliceOpPattern : public ShapeOpRewritePattern<SliceOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(SliceOp op,
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
-    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
+    if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
+        failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
     auto operand = op.getOperand();
@@ -784,36 +929,18 @@
 };
 
 struct FoldSubtractOpPattern final
-    : FoldOpRewritePattern<mlir::stablehlo::SubtractOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
+    : ShapeOpRewritePattern<mlir::stablehlo::SubtractOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
   LogicalResult matchAndRewrite(mlir::stablehlo::SubtractOp op,
                                 PatternRewriter& rewriter) const override {
-    Value lhs = op.getLhs();
-    Value rhs = op.getRhs();
-
-    TypedAttr lhsAttr, rhsAttr;
-    matchPattern(lhs, m_Constant(&lhsAttr));
-    matchPattern(rhs, m_Constant(&rhsAttr));
-
-    if (TypedAttr res;
-        lhsAttr && rhsAttr &&
-        (res = foldBinaryOpIntOrFloat(lhsAttr, rhsAttr, std::minus<>{}))) {
-      rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res);
-      return success();
-    }
-
-    return failure();
-  }
-};
-
-struct EvalSubtractOpPattern : public FoldOpRewritePattern<SubtractOp> {
-  using FoldOpRewritePattern::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(SubtractOp op,
-                                PatternRewriter& rewriter) const override {
-    return evalElementwise(rewriter, op,
-                           [&](APSInt lhs, APSInt rhs) { return lhs - rhs; });
+    if (failed(validateShapeFoldDtype(rewriter, op, op.getType())))
+      return failure();
+
+    auto res = foldBinaryOpIntOrFloat(rewriter, op, std::minus<>{});
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
   }
 };
 
@@ -823,42 +950,38 @@
 
   LogicalResult matchAndRewrite(mlir::stablehlo::SqrtOp op,
                                 PatternRewriter& rewriter) const final {
-    TypedAttr lhsAttr;
-    matchPattern(op.getOperand(), m_Constant(&lhsAttr));
-
-    if (!lhsAttr)
-      return rewriter.notifyMatchFailure(op, "operand not constant");
-
-    if (auto res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void>(
-            lhsAttr, foldSqrt)) {
-      rewriter.replaceOpWithNewOp<stablehlo::ConstantOp>(
-          op, op.getType(), llvm::cast<ElementsAttr>(res));
-      return success();
-    }
-
-    return rewriter.notifyMatchFailure(op, "unable to fold sqrt");
-  }
-
-  static std::optional<APFloat> foldSqrt(const APFloat& a) {
-    if (a.getSizeInBits(a.getSemantics()) == 64)
-      return APFloat(std::sqrt(a.convertToDouble()));
-
-    if (a.getSizeInBits(a.getSemantics()) == 32)
-      return APFloat(sqrtf(a.convertToFloat()));
-    return {};
-  }
-};
-
-struct EvalIotaOpPattern : public FoldOpRewritePattern<IotaOp> {
+    auto res = foldUnaryOpIntOrFloat(rewriter, op, FoldSqrt());
+    if (failed(res)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
+    return success();
+  }
+
+  struct FoldSqrt {
+    std::optional<APFloat> operator()(APFloat operand) {
+      if (operand.getSizeInBits(operand.getSemantics()) == 64)
+        return APFloat(std::sqrt(operand.convertToDouble()));
+
+      if (operand.getSizeInBits(operand.getSemantics()) == 32)
+        return APFloat(sqrtf(operand.convertToFloat()));
+      return std::nullopt;
+    }
+
+    // TODO: Enable int folding.
+    std::optional<APInt> operator()(APInt operand) {
+      return std::nullopt;
+    }
+  };
+};
+
+struct FoldIotaOpPattern : public FoldOpRewritePattern<IotaOp> {
   using FoldOpRewritePattern::FoldOpRewritePattern;
 
   LogicalResult matchAndRewrite(IotaOp op,
                                 PatternRewriter& rewriter) const override {
-    LLVM_DEBUG(llvm::dbgs() << "EvalIotaOpPattern folding: " << op << '\n');
+    LLVM_DEBUG(llvm::dbgs() << "FoldIotaOpPattern folding: " << op << '\n');
     auto resultType = cast<RankedTensorType>(op.getType());
-    size_t numElems = resultType.getNumElements();
-    if (numElems > static_cast<size_t>(options.foldOpElementLimit))
-      return rewriter.notifyMatchFailure(op, "too many elements to fold");
+    if (failed(validateElementCountForFold(rewriter, op, resultType)))
+      return failure();
 
     auto elementType = resultType.getElementType();
 
@@ -929,7 +1052,7 @@
 // transpose(constant) => constant with permuted dimensions
 // This covers ranked tensor types with 0 dimensions(zero elements) and 0
 // rank(scalar), as well as splat values.
-struct EvalTransposeOpPattern : public FoldOpRewritePattern<TransposeOp> {
+struct FoldTransposeOpPattern : public FoldOpRewritePattern<TransposeOp> {
   using FoldOpRewritePattern::FoldOpRewritePattern;
 
   LogicalResult matchAndRewrite(TransposeOp op,
@@ -943,6 +1066,7 @@
       return rewriter.notifyMatchFailure(
           op, "expected constant integer or float operand");
 
+    // TODO: Does this expand splat values? Should we special case splats?
     DenseElementsAttr resAttr;
     if (auto data = els.tryGetValues<APInt>())
       resAttr = transposeType(op, *data);
@@ -957,6 +1081,7 @@
   }
 };
 
+// TODO: Consider moving this into aggressive simplifications.
 struct LowerBoolSplatConstantsIntoReduceOpRegion
     : public FoldOpRewritePattern<ReduceOp> {
   using FoldOpRewritePattern::FoldOpRewritePattern;
@@ -1160,7 +1285,7 @@
   return true;
 }
 
-struct RemoveDeadWhileOpWithNoSideEffects
+struct FoldWhileOpDeadWithNoSideEffects
     : public FoldOpRewritePattern<WhileOp> {
   using FoldOpRewritePattern::FoldOpRewritePattern;
 
@@ -1233,23 +1358,16 @@
     PatternBenefit benefit) {
   populateStablehloShapeFolderPatterns(context, patterns, options, benefit);
 
-  patterns->add<EvalIotaOpPattern,                          //
-                EvalTransposeOpPattern,                     //
-                FoldReduceOpReducingZeroDims,               //
-                FoldReduceOpToConstantInitializer,          //
-                FoldReduceOpWithRedundantResults,           //
-                FoldWhileOpPattern,                         //
-                LowerBoolSplatConstantsIntoReduceOpRegion,  //
-                RemoveDeadWhileOpWithNoSideEffects>(context, options, benefit);
-
-  // TODO: Consolidate FoldOp patterns
-  // One is used by Shape Refinement, the other is a generic folder.
-  patterns->add<FoldAddOpPattern,                //
-                FoldBroadcastInDimSplatPattern,  //
-                FoldConcatenateOpPattern,        //
-                FoldMulOpPattern,                //
-                FoldSqrtOpPattern,               //
-                FoldSubtractOpPattern>(context, options);
+  patterns->add<FoldIotaOpPattern,                  //
+                FoldReduceOpReducingZeroDims,       //
+                FoldReduceOpToConstantInitializer,  //
+                FoldReduceOpWithRedundantResults,   //
+                FoldSqrtOpPattern,                  //
+                FoldTransposeOpPattern,             //
+                FoldWhileOpPattern,                 //
+                FoldWhileOpDeadWithNoSideEffects,   //
+                LowerBoolSplatConstantsIntoReduceOpRegion>(context, options,
+                                                           benefit);
 }
 
 class StablehloTargetIndependentOptimizationPass {
@@ -1266,25 +1384,25 @@
     MLIRContext* context, RewritePatternSet* patterns,
     const StablehloAggressiveFolderPassOptions& options,
     PatternBenefit benefit) {
-  patterns->add<EvalAddOpShapePattern>(context, options, benefit);
-  patterns->add<EvalAndOpPattern>(context, options, benefit);
-  patterns->add<EvalBroadcastInDimOpPattern>(context, options, benefit);
-  patterns->add<EvalClampOpPattern>(context, options, benefit);
-  patterns->add<EvalCompareOpPattern>(context, options, benefit);
-  patterns->add<EvalConcatenateOpPattern>(context, options, benefit);
-  patterns->add<EvalConvertOpPattern>(context, options, benefit);
-  patterns->add<EvalDivOpPattern>(context, options, benefit);
-  patterns->add<EvalGetDimensionSizeOpPattern>(context, options, benefit);
-  patterns->add<EvalMaxOpPattern>(context, options, benefit);
-  patterns->add<EvalMinOpPattern>(context, options, benefit);
-  patterns->add<EvalMulOpPattern>(context, options, benefit);
-  patterns->add<EvalOrOpPattern>(context, options, benefit);
-  patterns->add<EvalRemOpPattern>(context, options, benefit);
-  patterns->add<EvalReshapeOpPattern>(context, options, benefit);
-  patterns->add<EvalSelectOpPattern>(context, options, benefit);
-  patterns->add<EvalSignOpPattern>(context, options, benefit);
-  patterns->add<EvalSliceOpPattern>(context, options, benefit);
-  patterns->add<EvalSubtractOpPattern>(context, options, benefit);
+  patterns->add<FoldAddOpPattern>(context, options, benefit);
+  patterns->add<FoldAndOpPattern>(context, options, benefit);
+  patterns->add<FoldBroadcastInDimOpSplatPattern>(context, options, benefit);
+  patterns->add<FoldClampOpPattern>(context, options, benefit);
+  patterns->add<FoldCompareOpPattern>(context, options, benefit);
+  patterns->add<FoldConcatenateOpPattern>(context, options, benefit);
+  patterns->add<FoldConvertOpPattern>(context, options, benefit);
+  patterns->add<FoldDivOpPattern>(context, options, benefit);
+  patterns->add<FoldGetDimensionSizeOpPattern>(context, options, benefit);
+  patterns->add<FoldMaxOpPattern>(context, options, benefit);
+  patterns->add<FoldMinOpPattern>(context, options, benefit);
+  patterns->add<FoldMulOpPattern>(context, options, benefit);
+  patterns->add<FoldOrOpPattern>(context, options, benefit);
+  patterns->add<FoldRemOpPattern>(context, options, benefit);
+  patterns->add<FoldReshapeOpPattern>(context, options, benefit);
+  patterns->add<FoldSelectOpPattern>(context, options, benefit);
+  patterns->add<FoldSignOpPattern>(context, options, benefit);
+  patterns->add<FoldSliceOpPattern>(context, options, benefit);
+  patterns->add<FoldSubtractOpPattern>(context, options, benefit);
 }
 
 void populateStablehloShapeFolderPatterns(MLIRContext* context,
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
@@ -48,6 +48,8 @@
 def RankEqual : Constraint<
     CPred<"llvm::cast<ShapedType>($0.getType()).getRank() == llvm::cast<ShapedType>($1.getType()).getRank()">,
     "same rank">;
+
+def TensorDimsAllOne : Constraint<CPred<"tensorDimsAllOne($0, $1)">, "all tensor dims are 1">;
 
 def TypesEqual : Constraint<CPred<"$0.getType() == $1.getType()">, "operands are equal">;
 
@@ -100,6 +102,8 @@
 def ZeroExtent : AttrConstraint<
     CPred<"cast<DenseElementsAttr>($_self).getNumElements() == 0">,
     "is zero extent">;
+
+def AnyStaticShapeIntTensor : StaticShapeTensorOf<[HLO_Int]>;
 
 ///////////
 //// Native Code Call Utilities
@@ -503,7 +507,7 @@
 // Must be static shape, otherwise would require broadcasting via
 // CHLO_ConstantLike.
 def SubtractOp_FoldToZero
-  : Pat<(StableHLO_SubtractOp AnyStaticShapeTensor:$operand, $operand),
+  : Pat<(StableHLO_SubtractOp AnyStaticShapeIntTensor:$operand, $operand),
         (StableHLO_ConstantLike<"0"> $operand)>;
 
 // Pattern: subtract(X, 0) -> X

